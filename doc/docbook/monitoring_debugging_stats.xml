<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="monitoring_debugging_stats">
  <title>Monitoring, Debugging and Statistics</title>

  <para>Pegasus comes bundled with useful tools that help users debug
  workflows and generate useful statistics and plots about their workflow
  runs. Most of the tools query a runtime workflow database ( usually a
  sqllite in the workflow submit directory ) populated at runtime by
  pegasus-monitord. With the exception of pegasus-monitord (see below), all
  tools take in the submit directory as an argument. Users can invoke the
  tools listed in this chapter as follows:</para>

  <programlisting>$ pegasus-[toolname]   &lt;path to the submit directory&gt;</programlisting>

  <section id="workflow_status">
    <title>Workflow Status</title>

    <para>As the number of jobs and tasks in workflows increase, the ability
    to track the progress and quickly debug a workflow becomes more and more
    important. Pegasus comes with a series of utilities that can be used to
    monitor and debug workflows both in real-time as well as after execution
    is already completed.</para>

    <section id="monitoring_pegasus-status">
      <title>pegasus-status</title>

      <para>To monitor the execution of the workflow run the
      <command>pegasus-status</command> command as suggested by the output of
      the <command>pegasus-run</command> command.
      <command>pegasus-status</command> shows the current status of the Condor
      Q as pertaining to the master workflow from the workflow directory you
      are pointing it to. In a second section, it will show a summary of the
      state of all jobs in the workflow and all of its sub-workflows.</para>

      <para>The details of <command>pegasus-status</command> are described in
      its respective <link linkend="cli-pegasus-status">manual page</link>.
      There are many options to help you gather the most out of this tool,
      including a watch-mode to repeatedly draw information, various modes to
      add more information, and legends if you are new to it, or need to
      present it.</para>

      <programlisting><command>$ pegasus-status /Workflow/dags/directory</command>
STAT  IN_STATE  JOB
Run      05:08  level-3-0
Run      04:32   |-sleep_ID000005
Run      04:27   \_subdax_level-2_ID000004
Run      03:51      |-sleep_ID000003
Run      03:46      \_subdax_level-1_ID000002
Run      03:10         \_sleep_ID000001
Summary: 6 Condor jobs total (R:6)

UNREADY   READY     PRE  QUEUED    POST SUCCESS FAILURE %DONE
      0       0       0       6       0       3       0  33.3
Summary: 3 DAGs total (Running:3)</programlisting>

      <para>Without the <parameter>-l</parameter> option, the only a summary
      of the workflow statistics is shown under the current queue status.
      However, with the <parameter>-l</parameter> option, it will show each
      sub-workflow separately:</para>

      <programlisting><command>$ pegasus-status -l /Workflow/dags/directory</command>
STAT  IN_STATE  JOB
Run      07:01  level-3-0
Run      06:25   |-sleep_ID000005
Run      06:20   \_subdax_level-2_ID000004
Run      05:44      |-sleep_ID000003
Run      05:39      \_subdax_level-1_ID000002
Run      05:03         \_sleep_ID000001
Summary: 6 Condor jobs total (R:6)

UNRDY READY   PRE  IN_Q  POST  DONE  FAIL %DONE STATE   DAGNAME
    0     0     0     1     0     1     0  50.0 Running level-2_ID000004/level-1_ID000002/level-1-0.dag
    0     0     0     2     0     1     0  33.3 Running level-2_ID000004/level-2-0.dag
    0     0     0     3     0     1     0  25.0 Running *level-3-0.dag
    0     0     0     6     0     3     0  33.3         TOTALS (9 jobs)
Summary: 3 DAGs total (Running:3)</programlisting>

      <para>The following output shows a successful workflow of workflow
      summary after it has finished.</para>

      <programlisting><command>$ pegasus-status work/2011080514</command>
(no matching jobs found in Condor Q)
UNREADY   READY     PRE  QUEUED    POST SUCCESS FAILURE %DONE
      0       0       0       0       0   7,137       0 100.0
Summary: 44 DAGs total (Success:44)</programlisting>

      <para><warning>
          <para>For large workflows with many jobs, please note that
          <command>pegasus-status</command> will take time to compile state
          from all workflow files. This typically affects the initial run, and
          sub-sequent runs are faster due to the file system's buffer cache.
          However, on a low-RAM machine, thrashing is a possibility.</para>
        </warning>The following output show a failed workflow after no more
      jobs from it exist. Please note how no active jobs are shown, and the
      failure status of the total workflow.</para>

      <programlisting><command>$ pegasus-status work/submit</command>
(no matching jobs found in Condor Q)
UNREADY   READY     PRE  QUEUED    POST SUCCESS FAILURE %DONE
     20       0       0       0       0       0       2   0.0
Summary: 1 DAG total (Failure:1)</programlisting>
    </section>

    <section id="monitoring_pegasus-analyzer">
      <title>pegasus-analyzer</title>

      <para>Pegasus-analyzer is a command-line utility for parsing several
      files in the workflow directory and summarizing useful information to
      the user. It should be used after the workflow has already finished
      execution. pegasus-analyzer quickly goes through the jobstate.log file,
      and isolates jobs that did not complete successfully. It then parses
      their submit, and kickstart output files, printing to the user detailed
      information for helping the user debug what happened to his/her
      workflow.</para>

      <para>The simplest way to invoke pegasus-analyzer is to simply give it a
      workflow run directory, like in the example below:</para>

      <para><programlisting>$ pegasus-analyzer  /home/user/run0004
pegasus-analyzer: initializing...

************************************Summary*************************************

 Total jobs         :     26 (100.00%)
 # jobs succeeded   :     25 (96.15%)
 # jobs failed      :      1 (3.84%)
 # jobs held        :      1 (3.84%)
 # jobs unsubmitted :      0 (0.00%)

*******************************Held jobs' details*******************************
      
================================sleep_ID0000001=================================
       
       submit file            : sleep_ID0000001.sub
       last_job_instance_id   : 7
       reason                 :  Error from slot1@corbusier.isi.edu:
                                 STARTER at 128.9.64.188 failed to
                                 send file(s) to 
                                 &lt;128.9.64.188:62639&gt;: error reading from
                                 /opt/condor/8.4.8/local.corbusier/execute/dir_76205/f.out:
                                 (errno 2) No such file or directory;
                                SHADOW failed to receive file(s) from &lt;128.9.64.188:62653&gt; 

******************************Failed jobs' details******************************

============================register_viz_glidein_7_0============================

 last state: POST_SCRIPT_FAILURE
       site: local
submit file: /home/user/run0004/register_viz_glidein_7_0.sub
output file: /home/user/run0004/register_viz_glidein_7_0.out.002
 error file: /home/user/run0004/register_viz_glidein_7_0.err.002

-------------------------------Task #1 - Summary--------------------------------

site        : local
executable  : /lfs1/software/install/pegasus/default/bin/rc-client
arguments   : -Dpegasus.user.properties=/lfs1/work/pegasus/run0004/pegasus.15181.properties \
-Dpegasus.catalog.replica.url=rlsn://smarty.isi.edu --insert register_viz_glidein_7_0.in
exitcode    : 1
working dir : /lfs1/work/pegasus/run0004

---------Task #1 - pegasus::rc-client - pegasus::rc-client:1.0 - stdout---------

2009-02-20 16:25:13.467 ERROR [root] You need to specify the pegasus.catalog.replica property
2009-02-20 16:25:13.468 WARN  [root] non-zero exit-code 1</programlisting>In
      the case above, pegasus-analyzer's output contains a brief summary
      section, showing how many jobs have succeeded and how many have failed.
      If there are any held jobs, pegasus-analyzer will report the name of the
      job that was held, and the reason why , as determined from the
      dagman.out file for the workflow. The last_job_instance_id is the
      database id for the job in the job instance table of the monitoring
      database. After that, pegasus-analyzer will print information about each
      job that failed, showing its last known state, along with the location
      of its submit, output, and error files. pegasus-analyzer will also
      display any stdout and stderr from the job, as recorded in its kickstart
      record. Please consult pegasus-analyzer's man page for more examples and
      a detailed description of its various command-line options.</para>

      <note>
        <para>Starting with 4.0 release, by default pegasus analyzer queries
        the database to debug the workflow. If you want it to use files in the
        submit directory , use the <emphasis role="bold">--files</emphasis>
        option.</para>
      </note>
    </section>

    <section id="monitoring_pegasus-remove">
      <title>pegasus-remove</title>

      <para>If you want to abort your workflow for any reason you can use the
      pegasus-remove command listed in the output of pegasus-run invocation or
      by specifying the Dag directory for the workflow you want to
      terminate.</para>

      <programlisting><emphasis role="bold">$ pegasus-remove /PATH/To/WORKFLOW DIRECTORY</emphasis></programlisting>
    </section>

    <section>
      <title>Resubmitting failed workflows</title>

      <para>Pegasus will remove the DAGMan and all the jobs related to the
      DAGMan from the condor queue. A rescue DAG will be generated in case you
      want to resubmit the same workflow and continue execution from where it
      last stopped. A rescue DAG only skips jobs that have completely
      finished. It does not continue a partially running job unless the
      executable supports checkpointing.</para>

      <para>To resubmit an aborted or failed workflow with the same submit
      files and rescue Dag just rerun the pegasus-run command</para>

      <programlisting><emphasis role="bold">$ pegasus-run /Path/To/Workflow/Directory</emphasis></programlisting>
    </section>
  </section>

  <section id="plotting_statistics">
    <title>Plotting and Statistics</title>

    <para>Pegasus plotting and statistics tools queries the Stampede database
    created by pegasus-monitord for generating the output.The stampede scheme
    can be found <link linkend="stampede_schema_overview">here</link>.</para>

    <para>The statistics and plotting tools use the following terminology for
    defining tasks, jobs etc. Pegasus takes in a DAX which is composed of
    tasks. Pegasus plans it into a Condor DAG / Executable workflow that
    consists of Jobs. In case of Clustering, multiple tasks in the DAX can be
    captured into a single job in the Executable workflow. When DAGMan
    executes a job, a job instance is populated . Job instances capture
    information as seen by DAGMan. In case DAGMan retires a job on detecting a
    failure , a new job instance is populated. When DAGMan finds a job
    instance has finished , an invocation is associated with job instance. In
    case of clustered job, multiple invocations will be associated with a
    single job instance. If a Pre script or Post Script is associated with a
    job instance, then invocations are populated in the database for the
    corresponding job instance.</para>

    <section>
      <title>pegasus-statistics</title>

      <para>Pegasus statistics can compute statistics over one or more than
      one workflow run.</para>

      <para>Command to generate statistics over a single run is as shown
      below.</para>

      <programlisting>$ <emphasis><emphasis role="bold">pegasus-statistics /scratch/grid-setup/run0001/ -s all</emphasis> </emphasis>

#
# Pegasus Workflow Management System - http://pegasus.isi.edu
#
# Workflow summary:
#   Summary of the workflow execution. It shows total
#   tasks/jobs/sub workflows run, how many succeeded/failed etc.
#   In case of hierarchical workflow the calculation shows the
#   statistics across all the sub workflows.It shows the following
#   statistics about tasks, jobs and sub workflows.
#     * Succeeded - total count of succeeded tasks/jobs/sub workflows.
#     * Failed - total count of failed tasks/jobs/sub workflows.
#     * Incomplete - total count of tasks/jobs/sub workflows that are
#       not in succeeded or failed state. This includes all the jobs
#       that are not submitted, submitted but not completed etc. This
#       is calculated as  difference between 'total' count and sum of
#       'succeeded' and 'failed' count.
#     * Total - total count of tasks/jobs/sub workflows.
#     * Retries - total retry count of tasks/jobs/sub workflows.
#     * Total+Retries - total count of tasks/jobs/sub workflows executed
#       during workflow run. This is the cumulative of retries,
#       succeeded and failed count.
# Workflow wall time:
#   The wall time from the start of the workflow execution to the end as
#   reported by the DAGMAN.In case of rescue dag the value is the
#   cumulative of all retries.
# Cumulative job wall time:
#   The sum of the wall time of all jobs as reported by kickstart.
#   In case of job retries the value is the cumulative of all retries.
#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),
#   the wall time value includes jobs from the sub workflows as well.
# Cumulative job wall time as seen from submit side:
#   The sum of the wall time of all jobs as reported by DAGMan.
#   This is similar to the regular cumulative job wall time, but includes
#   job management overhead and delays. In case of job retries the value
#   is the cumulative of all retries. For workflows having sub workflow
#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs
#   from the sub workflows as well.
# Cumulative job badput wall time:
#   The sum of the wall time of all failed jobs as reported by kickstart.
#   In case of job retries the value is the cumulative of all retries.
#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),
#   the wall time value includes jobs from the sub workflows as well.
# Cumulative job badput wall time as seen from submit side:
#   The sum of the wall time of all failed jobs as reported by DAGMan.
#   This is similar to the regular cumulative job badput wall time, but includes
#   job management overhead and delays. In case of job retries the value
#   is the cumulative of all retries. For workflows having sub workflow
#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs
#   from the sub workflows as well.
------------------------------------------------------------------------------
Type           Succeeded Failed  Incomplete  Total     Retries   Total+Retries
Tasks          4         0       0           4         0         4            
Jobs           20        0       0           20        0         20           
Sub-Workflows  0         0       0           0         0         0            
------------------------------------------------------------------------------

Workflow wall time                                       : 6 mins, 55 secs
Cumulative job wall time                                 : 4 mins, 58 secs
Cumulative job wall time as seen from submit side        : 5 mins, 11 secs
Cumulative job badput wall time                          : 0.0 secs
Cumulative job badput wall time as seen from submit side : 0.0 secs

Integrity Metrics
5 files checksums compared with total duration of 0.439 secs
8 files checksums generated with total duration of 1.031 secs

Summary                       : ./statistics/summary.txt
Workflow execution statistics : ./statistics/workflow.txt
Job instance statistics       : ./statistics/jobs.txt
Transformation statistics     : ./statistics/breakdown.txt
Integrity statistics          : ./statistics/integrity.txt
Time statistics               : ./statistics/time.txt


</programlisting>

      <para>By default the output gets generated to a statistics folder inside
      the submit directory. The output that is generated by pegasus-statistics
      is based on the value set for command line option 's'(statistics_level).
      In the sample run the command line option 's' is set to 'all' to
      generate all the statistics information for the workflow run. Please
      consult the pegasus-statistics man page to find a detailed description
      of various command line options.</para>

      <note>
        <para>In case of hierarchal workflows, the metrics that are displayed
        on stdout take into account all the jobs/tasks/sub workflows that make
        up the workflow by recursively iterating through each sub
        workflow.</para>
      </note>

      <para/>

      <para>Command to generate statistics over all workflow runs populated in
      a single database is as shown below.</para>

      <para><programlisting>$ <emphasis><emphasis role="bold">pegasus-statistics -Dpegasus.monitord.output='mysql://s_user:s_user123@127.0.0.1:3306/stampede' -o /scratch/workflow_1_2/statistics -s all --multiple-wf</emphasis> </emphasis>


#
# Pegasus Workflow Management System - http://pegasus.isi.edu
#
# Workflow summary:
#   Summary of the workflow execution. It shows total
#   tasks/jobs/sub workflows run, how many succeeded/failed etc.
#   In case of hierarchical workflow the calculation shows the
#   statistics across all the sub workflows.It shows the following
#   statistics about tasks, jobs and sub workflows.
#     * Succeeded - total count of succeeded tasks/jobs/sub workflows.
#     * Failed - total count of failed tasks/jobs/sub workflows.
#     * Incomplete - total count of tasks/jobs/sub workflows that are
#       not in succeeded or failed state. This includes all the jobs
#       that are not submitted, submitted but not completed etc. This
#       is calculated as  difference between 'total' count and sum of
#       'succeeded' and 'failed' count.
#     * Total - total count of tasks/jobs/sub workflows.
#     * Retries - total retry count of tasks/jobs/sub workflows.
#     * Total+Retries - total count of tasks/jobs/sub workflows executed
#       during workflow run. This is the cumulative of retries,
#       succeeded and failed count.
# Workflow wall time:
#   The wall time from the start of the workflow execution to the end as
#   reported by the DAGMAN.In case of rescue dag the value is the
#   cumulative of all retries.
# Workflow cumulative job wall time:
#   The sum of the wall time of all jobs as reported by kickstart.
#   In case of job retries the value is the cumulative of all retries.
#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),
#   the wall time value includes jobs from the sub workflows as well.
# Cumulative job wall time as seen from submit side:
#   The sum of the wall time of all jobs as reported by DAGMan.
#   This is similar to the regular cumulative job wall time, but includes
#   job management overhead and delays. In case of job retries the value
#   is the cumulative of all retries. For workflows having sub workflow
#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs
#   from the sub workflows as well.
# Workflow cumulative job badput wall time:
#   The sum of the wall time of all failed jobs as reported by kickstart.
#   In case of job retries the value is the cumulative of all retries.
#   For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX jobs),
#   the wall time value includes jobs from the sub workflows as well.
# Cumulative job badput wall time as seen from submit side:
#   The sum of the wall time of all failed jobs as reported by DAGMan.
#   This is similar to the regular cumulative job badput wall time, but includes
#   job management overhead and delays. In case of job retries the value
#   is the cumulative of all retries. For workflows having sub workflow
#   jobs (i.e SUBDAG and SUBDAX jobs), the wall time value includes jobs
#   from the sub workflows as well.

------------------------------------------------------------------------------
Type           Succeeded Failed  Incomplete  Total     Retries   Total+Retries
Tasks          8         0       0           8         0         8
Jobs           34        0       0           34        0         34
Sub-Workflows  0         0       0           0         0         0
------------------------------------------------------------------------------

Workflow cumulative job wall time                        : 8 mins, 5 secs
Cumulative job wall time as seen from submit side        : 8 mins, 35 secs
Workflow cumulative job badput wall time                 : 0
Cumulative job badput wall time as seen from submit side : 0

</programlisting><note>
          <para>When computing statistics over multiple workflows, please
          note,</para>

          <orderedlist>
            <listitem>
              <para>All workflow run information should be populated in a
              single STAMPEDE database.</para>
            </listitem>

            <listitem>
              <para>The --output argument must be specified.</para>
            </listitem>

            <listitem>
              <para>Job statistics information is not computed.</para>
            </listitem>

            <listitem>
              <para>Workflow wall time information is not computed.</para>
            </listitem>
          </orderedlist>
        </note></para>

      <para>Pegasus statistics can also compute statistics over a few
      specified workflow runs, by specifying the either the submit
      directories, or the workflow UUIDs.</para>

      <programlisting>pegasus-statistics -Dpegasus.monitord.output='&lt;DB_URL&gt;' -o &lt;OUTPUT_DIR&gt; &lt;SUBMIT_DIR_1&gt; &lt;SUBMIT_DIR_2&gt; .. &lt;SUBMIT_DIR_n&gt;

OR

pegasus-statistics -Dpegasus.monitord.output='&lt;DB_URL&gt;' -o &lt;OUTPUT_DIR&gt; <emphasis
          role="bold">--isuuid</emphasis> &lt;UUID_1&gt; &lt;UUID_2&gt; .. &lt;UUID_n&gt;

</programlisting>

      <para>pegasus-statistics generates the following statistics files based
      on the command line options set.</para>

      <section id="peg_stats_summary">
        <title id="peg_stats_jobs">Summary Statistics File
        [summary.txt]</title>

        <para>The summary statistics are listed on the stdout by default, and
        can be written out to a file by providing the -s summary
        option.</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Workflow summary</emphasis> - Summary
            of the workflow execution. In case of hierarchical workflow the
            calculation shows the statistics across all the sub workflows.It
            shows the following statistics about tasks, jobs and sub
            workflows.</para>

            <itemizedlist>
              <listitem>
                <para><emphasis role="bold">Succeeded</emphasis> - total count
                of succeeded tasks/jobs/sub workflows.</para>
              </listitem>

              <listitem>
                <para><emphasis role="bold">Failed</emphasis> - total count of
                failed tasks/jobs/sub workflows.</para>
              </listitem>

              <listitem>
                <para><emphasis role="bold">Incomplete</emphasis> - total
                count of tasks/jobs/sub workflows that are not in succeeded or
                failed state. This includes all the jobs that are not
                submitted, submitted but not completed etc. This is calculated
                as difference between 'total' count and sum of 'succeeded' and
                'failed' count.</para>
              </listitem>

              <listitem>
                <para><emphasis role="bold">Total</emphasis> - total count of
                tasks/jobs/sub workflows.</para>
              </listitem>

              <listitem>
                <para><emphasis role="bold">Retries</emphasis> - total retry
                count of tasks/jobs/sub workflows.</para>
              </listitem>

              <listitem>
                <para><emphasis role="bold">Total Run</emphasis> - total count
                of tasks/jobs/sub workflows executed during workflow run. This
                is the cumulative of total retries, succeeded and failed
                count.</para>
              </listitem>
            </itemizedlist>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Workflow wall time</emphasis> - The
            wall time from the start of the workflow execution to the end as
            reported by the DAGMAN.In case of rescue dag the value is the
            cumulative of all retries.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Workflow cummulate job wall
            time</emphasis> - The sum of the wall time of all jobs as reported
            by kickstart. In case of job retries the value is the cumulative
            of all retries. For workflows having sub workflow jobs (i.e SUBDAG
            and SUBDAX jobs), the wall time value includes jobs from the sub
            workflows as well. This value is multiplied by the
            multiplier_factor in the job instance table.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Cumulative job wall time as seen from
            submit side</emphasis> - The sum of the wall time of all jobs as
            reported by DAGMan. This is similar to the regular cumulative job
            wall time, but includes job management overhead and delays. In
            case of job retries the value is the cumulative of all retries.
            For workflows having sub workflow jobs (i.e SUBDAG and SUBDAX
            jobs), the wall time value includes jobs from the sub workflows.
            This value is multiplied by the multiplier_factor in the job
            instance table.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Integrity Metrics </emphasis></para>

            <itemizedlist>
              <listitem>
                <para>Number of files for which the checksum was compared
                against a previously computed or provided checksum and total
                duration in seconds spent in doing it.</para>
              </listitem>

              <listitem>
                <para>Number of files for which the checksum was generated
                during workflow execution and total duration in seconds spent
                in doing it.</para>
              </listitem>
            </itemizedlist>
          </listitem>
        </itemizedlist>
      </section>

      <section id="peg_stats_workflow">
        <title>Workflow statistics file per workflow [workflow.txt]</title>

        <para>Workflow statistics file per workflow contains the following
        information about each workflow run. In case of hierarchal workflows,
        the file contains a table for each sub workflow. The file also
        contains a 'Total' table at the bottom which is the cumulative of all
        the individual statistics details.</para>

        <para>A sample table is shown below. It shows the following statistics
        about tasks, jobs and sub workflows.</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Workflow retries</emphasis> - number
            of times a workflow was retried.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Succeeded</emphasis> - total count of
            succeeded tasks/jobs/sub workflows.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Failed</emphasis> - total count of
            failed tasks/jobs/sub workflows.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Incomplete</emphasis> - total count of
            tasks/jobs/sub workflows that are not in succeeded or failed
            state. This includes all the jobs that are not submitted,
            submitted but not completed etc. This is calculated as difference
            between 'total' count and sum of 'succeeded' and 'failed'
            count.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Total</emphasis> - total count of
            tasks/jobs/sub workflows.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Retries</emphasis> - total retry count
            of tasks/jobs/sub workflows.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Total Run</emphasis> - total count of
            tasks/jobs/sub workflows executed during workflow run. This is the
            cumulative of total retries, succeeded and failed count.</para>
          </listitem>
        </itemizedlist>

        <table>
          <title>Workflow Statistics</title>

          <tgroup align="center" cols="9">
            <thead>
              <row>
                <entry align="center">#</entry>

                <entry align="center">Type</entry>

                <entry align="center">Succeeded</entry>

                <entry align="center">Failed</entry>

                <entry align="center">Incomplete</entry>

                <entry align="center">Total</entry>

                <entry align="center">Retries</entry>

                <entry align="center">Total Run</entry>

                <entry align="center">Workflow Retries</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>2a6df11b-9972-4ba0-b4ba-4fd39c357af4</entry>

                <entry/>

                <entry/>

                <entry/>

                <entry/>

                <entry/>

                <entry/>

                <entry/>

                <entry>0</entry>
              </row>

              <row>
                <entry/>

                <entry>Tasks</entry>

                <entry>4</entry>

                <entry>0</entry>

                <entry>0</entry>

                <entry>4</entry>

                <entry>0</entry>

                <entry>4</entry>

                <entry/>
              </row>

              <row>
                <entry/>

                <entry>Jobs</entry>

                <entry>13</entry>

                <entry>0</entry>

                <entry>0</entry>

                <entry>13</entry>

                <entry>0</entry>

                <entry>13</entry>

                <entry/>
              </row>

              <row>
                <entry/>

                <entry>Sub Workflows</entry>

                <entry>0</entry>

                <entry>0</entry>

                <entry>0</entry>

                <entry>0</entry>

                <entry>0</entry>

                <entry>0</entry>

                <entry/>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>

      <section>
        <title>Job statistics file per workflow [jobs.txt]</title>

        <para>Job statistics file per workflow contains the following details
        about the job instances in each workflow. A sample file is shown
        below.</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Job</emphasis> - the name of the job
            instance</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Try</emphasis> - the number
            representing the job instance run count.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Site</emphasis> - the site where the
            job instance ran.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Kickstart(sec.)</emphasis> - the
            actual duration of the job instance in seconds on the remote
            compute node.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Mult</emphasis> - multiplier factor
            from the job instance table for the job.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Kickstart_Mult</emphasis> - value of
            the Kickstart column multiplied by Mult.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">CPU-Time</emphasis> - remote CPU time
            computed as the stime + utime (when Kickstart is not used, this is
            empty).</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Post(sec.)</emphasis> - the postscript
            time as reported by DAGMan.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">CondorQTime(sec.)</emphasis> - the
            time between submission by DAGMan and the remote Grid submission.
            It is an estimate of the time spent in the condor q on the submit
            node .</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Resource(sec.)</emphasis> - the time
            between the remote Grid submission and start of remote execution .
            It is an estimate of the time job instance spent in the remote
            queue .</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Runtime(sec.)</emphasis> - the time
            spent on the resource as seen by Condor DAGMan . Is always
            &gt;=kickstart .</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Seqexec(sec.)</emphasis> - the time
            taken for the completion of a clustered job instance .</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Seqexec-Delay(sec.)</emphasis> - the
            time difference between the time for the completion of a clustered
            job instance and sum of all the individual tasks kickstart time
            .</para>
          </listitem>
        </itemizedlist>

        <table>
          <title>Job statistics</title>

          <tgroup align="center" cols="13">
            <thead>
              <row>
                <entry align="center">Job</entry>

                <entry align="center">Try</entry>

                <entry align="center">Site</entry>

                <entry align="center">Kickstart</entry>

                <entry align="center">Mult</entry>

                <entry align="center">Kickstart_Mult</entry>

                <entry align="center">CPU-Time</entry>

                <entry align="center">Post</entry>

                <entry align="center">CondorQTime</entry>

                <entry align="center">Resource</entry>

                <entry align="center">Runtime</entry>

                <entry align="center">Seqexec</entry>

                <entry align="center">Seqexec-Delay</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>analyze_ID0000004</entry>

                <entry>1</entry>

                <entry>local</entry>

                <entry>60.002</entry>

                <entry>1</entry>

                <entry>60.002</entry>

                <entry>59.843</entry>

                <entry>5.0</entry>

                <entry>0.0</entry>

                <entry>-</entry>

                <entry>62.0</entry>

                <entry>-</entry>

                <entry>-</entry>
              </row>

              <row>
                <entry>create_dir_diamond_0_local</entry>

                <entry>1</entry>

                <entry>local</entry>

                <entry>0.027</entry>

                <entry>1</entry>

                <entry>0.027</entry>

                <entry>0.003</entry>

                <entry>5.0</entry>

                <entry>5.0</entry>

                <entry>-</entry>

                <entry>0.0</entry>

                <entry>-</entry>

                <entry>-</entry>
              </row>

              <row>
                <entry>findrange_ID0000002</entry>

                <entry>1</entry>

                <entry>local</entry>

                <entry>60.001</entry>

                <entry>10</entry>

                <entry>600.01</entry>

                <entry>59.921</entry>

                <entry>5.0</entry>

                <entry>0.0</entry>

                <entry>-</entry>

                <entry>60.0</entry>

                <entry>-</entry>

                <entry>-</entry>
              </row>

              <row>
                <entry>findrange_ID0000003</entry>

                <entry>1</entry>

                <entry>local</entry>

                <entry>60.002</entry>

                <entry>10</entry>

                <entry>600.02</entry>

                <entry>59.912</entry>

                <entry>5.0</entry>

                <entry>10.0</entry>

                <entry>-</entry>

                <entry>61.0</entry>

                <entry>-</entry>

                <entry>-</entry>
              </row>

              <row>
                <entry>preprocess_ID0000001</entry>

                <entry>1</entry>

                <entry>local</entry>

                <entry>60.002</entry>

                <entry>1</entry>

                <entry>60.002</entry>

                <entry>59.898</entry>

                <entry>5.0</entry>

                <entry>5.0</entry>

                <entry>-</entry>

                <entry>60.0</entry>

                <entry>-</entry>

                <entry>-</entry>
              </row>

              <row>
                <entry>register_local_1_0</entry>

                <entry>1</entry>

                <entry>local</entry>

                <entry>0.459</entry>

                <entry>1</entry>

                <entry>0.459</entry>

                <entry>0.432</entry>

                <entry>6.0</entry>

                <entry>5.0</entry>

                <entry>-</entry>

                <entry>0.0</entry>

                <entry>-</entry>

                <entry>-</entry>
              </row>

              <row>
                <entry>register_local_1_1</entry>

                <entry>1</entry>

                <entry>local</entry>

                <entry>0.338</entry>

                <entry>1</entry>

                <entry>0.338</entry>

                <entry>0.331</entry>

                <entry>5.0</entry>

                <entry>5.0</entry>

                <entry>-</entry>

                <entry>0.0</entry>

                <entry>-</entry>

                <entry>-</entry>
              </row>

              <row>
                <entry>register_local_2_0</entry>

                <entry>1</entry>

                <entry>local</entry>

                <entry>0.348</entry>

                <entry>1</entry>

                <entry>0.348</entry>

                <entry>0.342</entry>

                <entry>5.0</entry>

                <entry>5.0</entry>

                <entry>-</entry>

                <entry>0.0</entry>

                <entry>-</entry>

                <entry>-</entry>
              </row>

              <row>
                <entry>stage_in_local_local_0</entry>

                <entry>1</entry>

                <entry>local</entry>

                <entry>0.39</entry>

                <entry>1</entry>

                <entry>0.39</entry>

                <entry>0.032</entry>

                <entry>5.0</entry>

                <entry>5.0</entry>

                <entry>-</entry>

                <entry>0.0</entry>

                <entry>-</entry>

                <entry>-</entry>
              </row>

              <row>
                <entry>stage_out_local_local_0_0</entry>

                <entry>1</entry>

                <entry>local</entry>

                <entry>0.165</entry>

                <entry>1</entry>

                <entry>0.165</entry>

                <entry>0.108</entry>

                <entry>5.0</entry>

                <entry>10.0</entry>

                <entry>-</entry>

                <entry>0.0</entry>

                <entry>-</entry>

                <entry>-</entry>
              </row>

              <row>
                <entry>stage_out_local_local_1_0</entry>

                <entry>1</entry>

                <entry>local</entry>

                <entry>0.147</entry>

                <entry>1</entry>

                <entry>0.147</entry>

                <entry>0.098</entry>

                <entry>7.0</entry>

                <entry>5.0</entry>

                <entry>-</entry>

                <entry>0.0</entry>

                <entry>-</entry>

                <entry>-</entry>
              </row>

              <row>
                <entry>stage_out_local_local_1_1</entry>

                <entry>1</entry>

                <entry>local</entry>

                <entry>0.139</entry>

                <entry>1</entry>

                <entry>0.139</entry>

                <entry>0.089</entry>

                <entry>5.0</entry>

                <entry>6.0</entry>

                <entry>-</entry>

                <entry>0.0</entry>

                <entry>-</entry>

                <entry>-</entry>
              </row>

              <row>
                <entry>stage_out_local_local_2_0</entry>

                <entry>1</entry>

                <entry>local</entry>

                <entry>0.145</entry>

                <entry>1</entry>

                <entry>0.145</entry>

                <entry>0.101</entry>

                <entry>5.0</entry>

                <entry>5.0</entry>

                <entry>-</entry>

                <entry>0.0</entry>

                <entry>-</entry>

                <entry>-</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>

      <section id="peg_stats_transformation">
        <title>Transformation statistics file per workflow
        [breakdown.txt]</title>

        <para>Transformation statistics file per workflow contains information
        about the invocations in each workflow grouped by transformation name.
        A sample file is shown below.</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Transformation</emphasis> - name of
            the transformation.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Count</emphasis> - the number of times
            invocations with a given transformation name was executed.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Succeeded</emphasis> - the count of
            succeeded invocations with a given logical transformation name
            .</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Failed</emphasis> - the count of
            failed invocations with a given logical transformation name
            .</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Min (sec.)</emphasis> - the minimum
            runtime value of invocations with a given logical transformation
            name times the multipler_factor.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Max (sec.)</emphasis> - the minimum
            runtime value of invocations with a given logical transformation
            name times the multiplier_factor.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Mean (sec.)</emphasis> - the mean of
            the invocation runtimes with a given logical transformation name
            times the multiplier_factor.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Total (sec.)</emphasis> - the
            cumulative of runtime value of invocations with a given logical
            transformation name times the multiplier_factor.</para>
          </listitem>
        </itemizedlist>

        <table>
          <title>Transformation Statistics</title>

          <tgroup align="center" cols="8">
            <thead>
              <row>
                <entry align="center">Transformation</entry>

                <entry align="center">Count</entry>

                <entry align="center">Succeeded</entry>

                <entry align="center">Failed</entry>

                <entry align="center">Min</entry>

                <entry align="center">Max</entry>

                <entry align="center">Mean</entry>

                <entry align="center">Total</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>dagman::post</entry>

                <entry>13</entry>

                <entry>13</entry>

                <entry>0</entry>

                <entry>5.0</entry>

                <entry>7.0</entry>

                <entry>5.231</entry>

                <entry>68.0</entry>
              </row>

              <row>
                <entry>diamond::analyze</entry>

                <entry>1</entry>

                <entry>1</entry>

                <entry>0</entry>

                <entry>60.002</entry>

                <entry>60.002</entry>

                <entry>60.002</entry>

                <entry>60.002</entry>
              </row>

              <row>
                <entry>diamond::findrange</entry>

                <entry>2</entry>

                <entry>2</entry>

                <entry>0</entry>

                <entry>600.01</entry>

                <entry>600.02</entry>

                <entry>600.02</entry>

                <entry>1200.03</entry>
              </row>

              <row>
                <entry>diamond::preprocess</entry>

                <entry>1</entry>

                <entry>1</entry>

                <entry>0</entry>

                <entry>60.002</entry>

                <entry>60.002</entry>

                <entry>60.002</entry>

                <entry>60.002</entry>
              </row>

              <row>
                <entry>pegasus::dirmanager</entry>

                <entry>1</entry>

                <entry>1</entry>

                <entry>0</entry>

                <entry>0.027</entry>

                <entry>0.027</entry>

                <entry>0.027</entry>

                <entry>0.027</entry>
              </row>

              <row>
                <entry>pegasus::pegasus-transfer</entry>

                <entry>5</entry>

                <entry>5</entry>

                <entry>0</entry>

                <entry>0.139</entry>

                <entry>0.39</entry>

                <entry>0.197</entry>

                <entry>0.986</entry>
              </row>

              <row>
                <entry>pegasus::rc-client</entry>

                <entry>3</entry>

                <entry>3</entry>

                <entry>0</entry>

                <entry>0.338</entry>

                <entry>0.459</entry>

                <entry>0.382</entry>

                <entry>1.145</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>

      <section id="peg_stats_time">
        <title>Time statistics file [time.txt]</title>

        <para>Time statistics file contains job instance and invocation
        statistics information grouped by time and host. The time grouping can
        be on day/hour. The file contains the following tables Job instance
        statistics per day/hour, Invocation statistics per day/hour, Job
        instance statistics by host per day/hour and Invocation by host per
        day/hour. A sample Invocation statistics by host per day table is
        shown below.</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Job instance statistics per
            day/hour</emphasis> - the number of job instances run, total
            runtime sorted by day/hour.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Invocation statistics per
            day/hour</emphasis> - the number of invocations , total runtime
            sorted by day/hour.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Job instance statistics by host per
            day/hour</emphasis> - the number of job instances run, total
            runtime on each host sorted by day/hour.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Invocation statistics by host per
            day/hour</emphasis> - the number of invocations , total runtime on
            each host sorted by day/hour.</para>
          </listitem>
        </itemizedlist>

        <table>
          <title>Invocation statistics by host per day</title>

          <tgroup align="center" cols="4">
            <thead>
              <row>
                <entry align="center">Date [YYYY-MM-DD]</entry>

                <entry align="center">Host</entry>

                <entry align="center">Count</entry>

                <entry align="center">Runtime (Sec.)</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>2011-07-15</entry>

                <entry>butterfly.isi.edu</entry>

                <entry>54</entry>

                <entry>625.094</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>

      <section>
        <title id="peg_stats_integrity">Integrity statistics file per workflow
        [integrity.txt]</title>

        <para>Integrity statistics file contains integrity metrics grouped by
        file type (input or output) and integrity type (check or compute). A
        sample table is shown below. It shows the following statistics about
        integrity checks.</para>

        <para><itemizedlist>
            <listitem>
              <para><emphasis role="bold">Type</emphasis> - the type of
              integrity metric. Check means checksum was compared for a file,
              and compute means a checksum was generated for a file.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">File type</emphasis> - the type of
              file: input or output from a job perspective.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">Count</emphasis> - the number of
              times type, file type integrity check was performed.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">Total duration</emphasis> - sum of
              duration in seconds for the 'count' number of records matching
              the particular type, file-type combo.</para>
            </listitem>
          </itemizedlist><table>
            <title>Integrity Statistics</title>

            <tgroup align="center" cols="5">
              <thead>
                <row>
                  <entry align="center">#</entry>

                  <entry align="center">Type</entry>

                  <entry align="center">File Type</entry>

                  <entry align="center">Count</entry>

                  <entry align="center">Total Duration</entry>
                </row>
              </thead>

              <tbody>
                <row>
                  <entry>4555392d-1b37-407c-98d3-60fb86cb9d57</entry>

                  <entry/>

                  <entry/>

                  <entry/>

                  <entry/>
                </row>

                <row>
                  <entry/>

                  <entry>check</entry>

                  <entry>input</entry>

                  <entry>5</entry>

                  <entry>0.164</entry>
                </row>

                <row>
                  <entry/>

                  <entry>check</entry>

                  <entry>output</entry>

                  <entry>5</entry>

                  <entry>1.456</entry>
                </row>

                <row>
                  <entry/>

                  <entry>compute</entry>

                  <entry>input</entry>

                  <entry>5</entry>

                  <entry>0.693</entry>
                </row>

                <row>
                  <entry/>

                  <entry>compute</entry>

                  <entry>output</entry>

                  <entry>5</entry>

                  <entry>0.758</entry>
                </row>
              </tbody>
            </tgroup>
          </table></para>
      </section>
    </section>

    <section>
      <title>pegasus-plots</title>

      <para>Pegasus-plots generates graphs and charts to visualize workflow
      execution. To generate graphs and charts run the command as shown
      below.</para>

      <programlisting>$ <emphasis>pegasus-plots  -p all  /scratch/grid-setup/run0001/</emphasis>


...

******************************************** SUMMARY ********************************************

Graphs and charts generated by pegasus-plots can be viewed by opening the generated html file in the web browser  :
/scratch/grid-setup/run0001/plots/index.html

**************************************************************************************************</programlisting>

      <para>By default the output gets generated to plots folder inside the
      submit directory. The output that is generated by pegasus-plots is based
      on the value set for command line option 'p'(plotting_level).In the
      sample run the command line option 'p' is set to 'all' to generate all
      the charts and graphs for the workflow run. Please consult the
      pegasus-plots man page to find a detailed description of various command
      line options. pegasus-plots generates an index.html file which provides
      links to all the generated charts and plots. A sample index.html page is
      shown below.</para>

      <figure>
        <title>pegasus-plot index page</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/pegasus_plots_index.png" width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>pegasus-plots generates the following plots and charts.</para>

      <para><emphasis role="bold">Dax Graph</emphasis></para>

      <para>Graph representation of the DAX file. A sample page is shown
      below.</para>

      <figure>
        <title>DAX Graph</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/dax_page.png" width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para><emphasis role="bold">Dag Graph</emphasis></para>

      <para>Graph representation of the DAG file. A sample page is shown
      below.</para>

      <figure>
        <title>DAG Graph</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/dag_page.png" width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para><emphasis role="bold">Gantt workflow execution
      chart</emphasis></para>

      <para>Gantt chart of the workflow execution run. A sample page is shown
      below.</para>

      <figure>
        <title>Gantt Chart</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/gantt_chart_page.png" width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The toolbar at the top provides zoom in/out , pan
      left/right/top/bottom and show/hide job name functionality.The toolbar
      at the bottom can be used to show/hide job states. Failed job instances
      are shown in red border in the chart. Clicking on a sub workflow job
      instance will take you to the corresponding sub workflow chart.</para>

      <para><emphasis role="bold">Host over time chart</emphasis></para>

      <para>Host over time chart of the workflow execution run. A sample page
      is shown below.</para>

      <figure>
        <title>Host over time chart</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/host_chart_page.png" width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The toolbar at the top provides zoom in/out , pan
      left/right/top/bottom and show/hide host name functionality.The toolbar
      at the bottom can be used to show/hide job states. Failed job instances
      are shown in red border in the chart. Clicking on a sub workflow job
      instance will take you to the corresponding sub workflow chart.</para>

      <para><emphasis role="bold">Time chart</emphasis></para>

      <para>Time chart shows job instance/invocation count and runtime of the
      workflow run over time. A sample page is shown below.</para>

      <figure>
        <title>Time chart</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/time_chart_page.png" width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The toolbar at the top provides zoom in/out and pan
      left/right/top/bottom functionality. The toolbar at the bottom can be
      used to switch between job instances/ invocations and day/hour
      filtering.</para>

      <para><emphasis role="bold">Breakdown chart</emphasis></para>

      <para>Breakdown chart shows invocation count and runtime of the workflow
      run grouped by transformation name. A sample page is shown below.</para>

      <figure>
        <title>Breakdown chart</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/breakdown_chart_page.png" width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The toolbar at the bottom can be used to switch between invocation
      count and runtime filtering. Legends can be clicked to get more
      details.</para>
    </section>
  </section>

  <section id="dashboard">
    <title>Dashboard</title>

    <para>As the number of jobs and tasks in workflows increase, the ability
    to track the progress and quickly debug a workflow becomes more and more
    important. The dashboard provides users with a tool to monitor and debug
    workflows both in real-time as well as after execution is already
    completed, through a browser.</para>

    <section>
      <title>Workflow Dashboard</title>

      <para>Pegasus Workflow Dashboard is bundled with Pegasus. The
      pegasus-service is developed in Python and uses the Flask framework to
      implement the web interface.The users can then connect to this server
      using a browser to monitor/debug workflows.</para>

      <para><note>
          <para>the workflow dashboard can only monitor workflows which have
          been executed using Pegasus 4.2.0 and above.</para>
        </note></para>

      <para>To start the Pegasus Dashboard execute the following
      command</para>

      <programlisting>$ pegasus-service --host 127.0.0.1 --port 5000

SSL is not configured: Using self-signed certificate
2015-04-13 16:14:23,074:Pegasus.service.server:79: WARNING: SSL is not configured: Using self-signed certificate
Service not running as root: Will not be able to switch users
2015-04-13 16:14:23,074:Pegasus.service.server:86: WARNING: Service not running as root: Will not be able to switch users</programlisting>

      <para>By default, the server is configured to listen only on
      localhost/127.0.0.1 on port 5000. A user can view the dashboard on
      <emphasis role="bold"><emphasis
      role="bold">https</emphasis>://localhost:5000/</emphasis></para>

      <para>To make the Pegasus Dashboard listen on all network interfaces OR
      on a different port, users can pass different values to the --host
      and/or --port options.</para>

      <para>By default, the dashboard server can only monitor workflows run by
      the current user i.e. the user who is running the
      pegasus-service.</para>

      <para>The Dashboard's home page lists all workflows, which have been run
      by the current-user. The home page shows the status of each of the
      workflow i.e. Running/Successful/Failed/Failing. The home page lists
      only the top level workflows (Pegasus supports hierarchical workflows
      i.e. workflows within a workflow). The rows in the table are color
      coded</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">Green</emphasis>: indicates workflow
          finished successfully.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Red</emphasis>: indicates workflow
          finished with a failure.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Blue</emphasis>: indicates a workflow is
          currently running.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Gray</emphasis>: indicates a workflow
          that was archived.</para>
        </listitem>
      </itemizedlist>

      <figure>
        <title>Dashboard Home Page</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/dashboard_home.png" width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>To view details specific to a workflow, the user can click on
      corresponding workflow label. The workflow details page lists workflow
      specific information like workflow label, workflow status, location of
      the submit directory, files, and metadata associated with the workflow
      etc. The details page also displays pie charts showing the distribution
      of jobs based on status.</para>

      <para>In addition, the details page displays a tab listing all
      sub-workflows and their statuses. Additional tabs exist which list
      information for all running, failed, successful, and failing
      jobs.</para>

      <para><note>
          <para>Failing jobs are currently running jobs (visible in Running
          tab), which have failed in previous attempts to execute them.</para>
        </note></para>

      <para>The information displayed for a job depends on it's status. For
      example, the failed jobs tab displays the job name, exit code, links to
      available standard output, and standard error contents.</para>

      <figure>
        <title>Dashboard Workflow Page</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/dashboard_workflow_details.png"
                       width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para><figure>
          <title>Dashboard Workflow Metadata</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="images/dashboard_workflow_meta.png"
                         width="100%"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para><figure>
          <title>Dashboard Workflow Files</title>

          <mediaobject>
            <imageobject>
              <imagedata fileref="images/dashboard_file_meta.png" width="100%"/>
            </imageobject>
          </mediaobject>
        </figure></para>

      <para>To view details specific to a job the user can click on the
      corresponding job's job label. The job details page lists information
      relevant to a specific job. For example, the page lists information like
      job name, exit code, run time, etc.</para>

      <para>The job instance section of the job details page lists all
      attempts made to run the job i.e. if a job failed in its first attempt
      due to transient errors, but ran successfully when retried, the job
      instance section shows two entries; one for each attempt to run the
      job.</para>

      <para>The job details page also shows tab's for failed, and successful
      task invocations (Pegasus allows users to group multiple smaller task's
      into a single job i.e. a job may consist of one or more tasks)</para>

      <figure>
        <title>Dashboard Job Description Page</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/dashboard_job_details.png" width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The task invocation details page provides task specific
      information like task name, exit code, duration, metadata associated
      with the task, etc. Task details differ from job details, as they are
      more granular in nature.</para>

      <figure>
        <title>Dashboard Invocation Page</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/dashboard_invocation_details.png"
                       width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The dashboard also has web pages for workflow statistics and
      workflow charts, which graphically renders information provided by the
      pegasus-statistics and pegasus-plots command respectively.</para>

      <para>The Statistics page shows the following statistics.</para>

      <orderedlist>
        <listitem>
          <para>Workflow level statistics</para>
        </listitem>

        <listitem>
          <para>Job breakdown statistics</para>
        </listitem>

        <listitem>
          <para>Job specific statistics</para>
        </listitem>

        <listitem>
          <para>Integrity statistics</para>
        </listitem>
      </orderedlist>

      <figure>
        <title>Dashboard Statistics Page</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/dashboard_statistics.png" width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The Charts page shows the following charts.</para>

      <orderedlist>
        <listitem>
          <para>Job Distribution by Count/Time</para>
        </listitem>

        <listitem>
          <para>Time Chart by Job/Invocation</para>
        </listitem>

        <listitem>
          <para>Workflow Execution Gantt Chart</para>
        </listitem>
      </orderedlist>

      <para>The chart below shows the invocation distribution by count or
      time.</para>

      <figure>
        <title>Dashboard Plots - Job Distribution</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/dashboard_plots_job_dist.png"
                       width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The time chart shown below shows the number of jobs/invocations in
      the workflow and their total runtime</para>

      <figure>
        <title>Dashboard Plots - Time Chart</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/dashboard_plots_time_charts.png"
                       width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The workflow gantt chart lays out the execution of the jobs in the
      workflow over time.</para>

      <figure>
        <title>Dashboard Plots - Workflow Gantt Chart</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/dashboard_plots_wf_gantt.png"
                       width="100%"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>
  </section>

  <section id="notifications">
    <title>Notifications</title>

    <para>The Pegasus Workflow Mapper now supports job and workflow level
    notifications. You can specify in the DAX with the job or the
    workflow</para>

    <itemizedlist>
      <listitem>
        <para>the event when the notification needs to be sent</para>
      </listitem>

      <listitem>
        <para>the executable that needs to be invoked.</para>
      </listitem>
    </itemizedlist>

    <para>The notifications are issued from the submit host by the
    pegasus-monitord daemon that monitors the Condor logs for the workflow.
    When a notification is issued, pegasus-monitord while invoking the
    notifying executable sets certain environment variables that contain
    information about the job and workflow state.</para>

    <para>The Pegasus release comes with default notification clients that
    send notifications via email or jabber.</para>

    <section>
      <title>Specifying Notifications in the DAX</title>

      <para>Currently, you can specify notifications for the jobs and the
      workflow by the use of invoke elements.</para>

      <para>Invoke elements can be sub elements for the following elements in
      the DAX schema.<itemizedlist>
          <listitem>
            <para>job - to associate notifications with a compute job in the
            DAX.</para>
          </listitem>

          <listitem>
            <para>dax - to associate notifications with a dax job in the
            DAX.</para>
          </listitem>

          <listitem>
            <para>dag - to associate notifications with a dag job in the
            DAX.</para>
          </listitem>

          <listitem>
            <para>executable - to associate notifications with a job that uses
            a particular notification</para>
          </listitem>
        </itemizedlist></para>

      <para>The invoke element can be specified at the root element level of
      the DAX to indicate workflow level notifications.</para>

      <para>The invoke element may be specified multiple times, as needed. It
      has a mandatory <emphasis role="bold">when</emphasis> attribute with the
      following value set</para>

      <table id="notification_conditions_table">
        <title>Invoke Element attributes and meaning.</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry align="center">Enumeration of Values for when
              attribute</entry>

              <entry align="center">Meaning</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>never</entry>

              <entry>(default). Never notify of anything. This is useful to
              temporarily disable an existing notifications.</entry>
            </row>

            <row>
              <entry>start</entry>

              <entry>create a notification when the job is submitted.</entry>
            </row>

            <row>
              <entry>on_error</entry>

              <entry>after a job finishes with failure (exitcode !=
              0).</entry>
            </row>

            <row>
              <entry>on_success</entry>

              <entry>after a job finishes with success (exitcode ==
              0).</entry>
            </row>

            <row>
              <entry>at_end</entry>

              <entry>after a job finishes, regardless of exitcode.</entry>
            </row>

            <row>
              <entry>all</entry>

              <entry>like start and at_end combined.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>You can specify multiple invoke elements corresponding to same
      when attribute value in the DAX. This will allow you to have multiple
      notifications for the same event.</para>

      <para>Here is an example that illustrates that.</para>

      <programlisting>&lt;job id="ID000001" namespace="example" name="mDiffFit" version="1.0"
       node-label="preprocess" &gt;
    &lt;argument&gt;-a top -T 6  -i &lt;file name="f.a"/&gt;  -o &lt;file name="f.b1"/&gt;&lt;/argument&gt;

    &lt;!-- profiles are optional --&gt;
    &lt;profile namespace="execution" key="site"&gt;isi_viz&lt;/profile&gt;
    &lt;profile namespace="condor" key="getenv"&gt;true&lt;/profile&gt;

    &lt;uses name="f.a" link="input"  register="false" transfer="true" type="data" /&gt;
    &lt;uses name="f.b" link="output" register="false" transfer="true" type="data" /&gt;

    &lt;!-- 'WHEN' enumeration: never, start, on_error, on_success, at_end, all --&gt;
    <emphasis role="bold">&lt;invoke when="start"&gt;/path/to/notify1 arg1 arg2&lt;/invoke&gt;
    &lt;invoke when="start"&gt;/path/to/notify1 arg3 arg4&lt;/invoke&gt;
    &lt;invoke when="on_success"&gt;/path/to/notify2 arg3 arg4&lt;/invoke&gt;</emphasis>
  &lt;/job&gt;</programlisting>

      <para>In the above example the executable notify1 will be invoked twice
      when a job is submitted ( when="start" ), once with arguments arg1 and
      arg2 and second time with arguments arg3 and arg4.</para>

      <para>The DAX Generator API <link
      linkend="dax_generator_api">chapter</link> has information about how to
      add notifications to the DAX using the DAX api's.</para>
    </section>

    <section id="pegasus_notify_file">
      <title>Notify File created by Pegasus in the submit directory</title>

      <para>Pegasus while planning a workflow writes out a notify file in the
      submit directory that contains all the notifications that need to be
      sent for the workflow. pegasus-monitord picks up this notifications file
      to determine what notifications need to be sent and when.</para>

      <orderedlist>
        <listitem>
          <para>ENTITY_TYPE ID NOTIFICATION_CONDITION ACTION</para>

          <itemizedlist>
            <listitem>
              <para>ENTITY_TYPE can be either of the following keywords</para>

              <itemizedlist>
                <listitem>
                  <para>WORKFLOW - indicates workflow level
                  notification</para>
                </listitem>

                <listitem>
                  <para>JOB - indicates notifications for a job in the
                  executable workflow</para>
                </listitem>

                <listitem>
                  <para>DAXJOB - indicates notifications for a DAX Job in the
                  executable workflow</para>
                </listitem>

                <listitem>
                  <para>DAGJOB - indicates notifications for a DAG Job in the
                  executable workflow</para>
                </listitem>
              </itemizedlist>
            </listitem>

            <listitem>
              <para>ID indicates the identifier for the entity. It has
              different meaning depending on the entity type - -</para>

              <itemizedlist>
                <listitem>
                  <para>workflow - ID is wf_uuid</para>
                </listitem>

                <listitem>
                  <para>JOB|DAXJOB|DAGJOB - ID is the job identifier in the
                  executable workflow ( DAG ).</para>
                </listitem>
              </itemizedlist>
            </listitem>

            <listitem>
              <para>NOTIFICATION_CONDITION is the condition when the
              notification needs to be sent. The notification conditions are
              enumerated in <link linkend="notification_conditions_table">this
              table</link></para>
            </listitem>

            <listitem>
              <para>ACTION is what needs to happen when condition is
              satisfied. It is executable + arguments</para>
            </listitem>
          </itemizedlist>
        </listitem>

        <listitem>
          <para>INVOCATION JOB_IDENTIFIER INV.ID NOTIFICATION_CONDITION
          ACTION</para>

          <para>The INVOCATION lines are only generated for clustered jobs, to
          specifiy the finer grained notifications for each constitutent
          job/invocation .</para>

          <itemizedlist>
            <listitem>
              <para>JOB IDENTIFIER is the job identifier in the executable
              workflow ( DAG ).</para>
            </listitem>

            <listitem>
              <para>INV.ID indicates the index of the task in the clustered
              job for which the notification needs to be sent.</para>
            </listitem>

            <listitem>
              <para>NOTIFICATION_CONDITION is the condition when the
              notification needs to be sent. The notification conditions are
              enumerated in <link
              linkend="notification_conditions_table">Table 1</link></para>
            </listitem>

            <listitem>
              <para>ACTION is what needs to happen when condition is
              satisfied. It is executable + arguments</para>
            </listitem>
          </itemizedlist>
        </listitem>
      </orderedlist>

      <para>A sample notifications file generated is listed below.</para>

      <programlisting>WORKFLOW d2c4f79c-8d5b-4577-8c46-5031f4d704e8 on_error /bin/date1

INVOCATION merge_vahi-preprocess-1.0_PID1_ID1 1 on_success /bin/date_executable
INVOCATION merge_vahi-preprocess-1.0_PID1_ID1 1 on_success /bin/date_executable
INVOCATION merge_vahi-preprocess-1.0_PID1_ID1 1 on_error /bin/date_executable

INVOCATION merge_vahi-preprocess-1.0_PID1_ID1 2 on_success /bin/date_executable
INVOCATION merge_vahi-preprocess-1.0_PID1_ID1 2 on_error /bin/date_executable

DAXJOB subdax_black_ID000003 on_error /bin/date13
JOB    analyze_ID00004    on_success /bin/date
</programlisting>
    </section>

    <section>
      <title>Configuring pegasus-monitord for notifications</title>

      <para>Whenever pegasus-monitord enters a workflow (or sub-workflow)
      directory, it will read the notifications file generated by Pegasus.
      Pegasus-monitord will match events in the running workflow against the
      notifications specified in the notifications file and will initiate the
      script specified in a notification when that notification matches an
      event in the workflow. It is important to note that there will be a
      delay between a certain event happening in the workflow, and
      pegasus-monitord processing the log file and executing the corresponding
      notification script.</para>

      <para>The following command line options (and properties) can change how
      pegasus-monitord handles notifications:</para>

      <itemizedlist>
        <listitem>
          <para>--no-notifications (pegasus.monitord.notifications=False):
          Will disable notifications completely.</para>
        </listitem>

        <listitem>
          <para>--notifications-max=nn
          (pegasus.monitord.notifications.max=nn): Will limit the number of
          concurrent notification scripts to nn. Once pegasus-monitord reaches
          this number, it will wait until one notification script finishes
          before starting a new one. Notifications happening during this time
          will be queued by the system. The default number of concurrent
          notification scripts for pegasus-monitord is 10.</para>
        </listitem>

        <listitem>
          <para>--notifications-timeout=nn
          (pegasus.monitord.notifications.timeout=nn): This setting is used to
          change how long will pegasus-monitord wait for a notification script
          to finish. By default pegasus-monitord will wait for as long as it
          takes (possibly indefinitely) until a notification script ends. With
          this option, pegasus-monitord will wait for at most nn seconds
          before killing the notification script.</para>
        </listitem>
      </itemizedlist>

      <para>It is also important to understand that pegasus-monitord will not
      issue any notifications when it is executed in replay mode.</para>

      <section>
        <title>Environment set for the notification scripts</title>

        <para>Whenever a notification in the notifications file matches an
        event in the running workflow, pegasus-monitord will run the
        corresponding script specified in the ACTION field of the
        notifications file. Pegasus-monitord will set the following
        environment variables for each notification script is starts:</para>

        <itemizedlist>
          <listitem>
            <para>PEGASUS_EVENT: The NOTIFICATION_CONDITION that caused the
            notification. In the case of the "all" condition, pegasus-monitord
            will substitute it for the actual event that caused the match
            (e.g. "start" or "at_end").</para>
          </listitem>

          <listitem>
            <para>PEGASUS_EVENT_TIMESTAMP: Timestamp in EPOCH format for the
            event (better for automated processing).</para>
          </listitem>

          <listitem>
            <para>PEGASUS_EVENT_TIMESTAMP_ISO: Same as above, but in ISO
            format (better for human readability).</para>
          </listitem>

          <listitem>
            <para>PEGASUS_SUBMIT_DIR: The submit directory for the workflow
            (usually the value from "submit_dir" in the braindump.txt
            file)</para>
          </listitem>

          <listitem>
            <para>PEGASUS_STDOUT: For workflow notifications, this will
            correspond to the dagman.out file for that workflow. For job and
            invocation notifications, this field will contain the output file
            (stdout) for that particular job instance.</para>
          </listitem>

          <listitem>
            <para>PEGASUS_STDERR: For job and invocation notifications, this
            field will contain the error file (stderr) for the particular
            executable job instance. This field does not exist in case of
            workflow notifications.</para>
          </listitem>

          <listitem>
            <para>PEGASUS_WFID: Contains the workflow id for this notification
            in the form of DAX_LABEL + DAX_INDEX (from the braindump.txt
            file).</para>
          </listitem>

          <listitem>
            <para>PEGASUS_JOBID: For workflow notifications, this contains the
            worfkflow wf_uuid (from the braindump.txt file). For job and
            invocation notifications, this field contains the job identifier
            in the executable workflow ( DAG ) for the particular
            notification.</para>
          </listitem>

          <listitem>
            <para>PEGASUS_INVID: Contains the index of the task in the
            clustered job for the notification.</para>
          </listitem>

          <listitem>
            <para>PEGASUS_STATUS: For workflow notifications, this contains
            DAGMan's exit code. For job and invocation notifications, this
            field contains the exit code for the particular job/task. Please
            note that this field is not present for 'start' notification
            events.</para>
          </listitem>
        </itemizedlist>
      </section>
    </section>

    <section>
      <title>Default Notification Scripts</title>

      <para>Pegasus ships with two reference notification scripts. These can
      be used as starting point when creating your own notification scripts,
      or if the default one is all you need, you can use them directly in your
      workflows. The scripts are:</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">libexec/notification/email</emphasis> -
          sends email, including the output from
          <command>pegasus-status</command> (default) or
          <command>pegasus-analyzer</command>.</para>

          <screen><emphasis role="bold">$ ./libexec/notification/email --help</emphasis>
Usage: email [options]

Options:
  -h, --help            show this help message and exit
  -t TO_ADDRESS, --to=TO_ADDRESS
                        The To: email address. Defines the recipient for the
                        notification.
  -f FROM_ADDRESS, --from=FROM_ADDRESS
                        The From: email address. Defaults to the required To:
                        address.
  -r REPORT, --report=REPORT
                        Include workflow report. Valid values are: none
                        pegasus-analyzer pegasus-status (default)
</screen>
        </listitem>

        <listitem>
          <para><emphasis role="bold">libexec/notification/jabber </emphasis>-
          sends simple notifications to Jabber/GTalk. This can be useful for
          job failures.</para>

          <screen><emphasis role="bold">$ ./libexec/notification/jabber --help</emphasis>
Usage: jabber [options]

Options:
  -h, --help            show this help message and exit
  -i JABBER_ID, --jabberid=JABBER_ID
                        Your jabber id. Example: user@jabberhost.com
  -p PASSWORD, --password=PASSWORD
                        Your jabber password
  -s HOST, --host=HOST  Jabber host, if different from the host in your jabber
                        id. For Google talk, set this to talk.google.com
  -r RECIPIENT, --recipient=RECIPIENT
                        Jabber id of the recipient. Not necessary if you want
                        to send to your own jabber id
</screen>
        </listitem>
      </itemizedlist>

      <para>For example, if the DAX generator is written in Python and you
      want notifications on 'at_end' events (successful or failed):</para>

      <programlisting># job level notifications - in this case for at_end events
job.invoke('at_end', pegasus_home + "/libexec/notifications/email --to me@somewhere.edu")</programlisting>

      <para>Please see the <link linkend="notifications_example">notifications
      example</link> to see a full workflow using notifications.</para>
    </section>
  </section>

  <section id="monitoring">
    <title>Monitoring Database</title>

    <para>Pegasus launches a monitoring daemon called pegasus-monitord per
    workflow ( a single daemon is launched if a user submits a hierarchal
    workflow ) . pegasus-monitord parses the workflow and job logs in the
    submit directory and populates to a database. This chapter gives an
    overview of the pegasus-monitord and describes the schema of the runtime
    database.</para>

    <section id="monitoring_pegasus-monitord" os="">
      <title>pegasus-monitord</title>

      <para><emphasis role="bold">Pegasus-monitord</emphasis> is used to
      follow workflows, parsing the output of DAGMan's dagman.out file. In
      addition to generating the jobstate.log file, which contains the various
      states that a job goes through during the workflow execution, <emphasis
      role="bold">pegasus-monitord</emphasis> can also be used to mine
      information from jobs' submit and output files, and either populate a
      database, or write a file with NetLogger events containing this
      information. <emphasis role="bold">Pegasus-monitord</emphasis> can also
      send notifications to users in real-time as it parses the workflow
      execution logs.</para>

      <para><emphasis role="bold">Pegasus-monitord</emphasis> is automatically
      invoked by <emphasis role="bold">pegasus-run</emphasis>, and tracks
      workflows in real-time. By default, it produces the jobstate.log file,
      and a SQLite database, which contains all the information listed in the
      <link linkend="stampede_schema_overview">Stampede schema</link>. When a
      workflow fails, and is re-submitted with a rescue DAG, <emphasis
      role="bold">pegasus-monitord</emphasis> will automatically pick up from
      where it left previously and continue to write the jobstate.log file and
      populate the database.</para>

      <para>If, after the workflow has already finished, users need to
      re-create the jobstate.log file, or re-populate the database from
      scratch, <emphasis role="bold">pegasus-monitord</emphasis>'s <emphasis
      role="bold">--replay</emphasis> option should be used when running it
      manually.</para>

      <section>
        <title>Populating to different backend databases</title>

        <para>In addition to SQLite, <emphasis
        role="bold">pegasus-monitord</emphasis> supports other types of
        databases, such as MySQL and Postgres. Users will need to install the
        low-level database drivers, and can use the <emphasis
        role="bold">--dest</emphasis> command-line option, or the <emphasis
        role="bold">pegasus.monitord.output</emphasis> property to select
        where the logs should go.</para>

        <para>As an example, the command:</para>

        <programlisting>$ pegasus-monitord -r diamond-0.dag.dagman.out</programlisting>

        <para>will launch <emphasis role="bold">pegasus-monitord</emphasis> in
        replay mode. In this case, if a jobstate.log file already exists, it
        will be rotated and a new file will be created. It will also
        create/use a SQLite database in the workflow's run directory, with the
        name of diamond-0.stampede.db. If the database already exists, it will
        make sure to remove any references to the current workflow before it
        populates the database. In this case, <emphasis
        role="bold">pegasus-monitord</emphasis> will process the workflow
        information from start to finish, including any restarts that may have
        happened.</para>

        <para>Users can specify an alternative database for the events, as
        illustrated by the following examples:</para>

        <programlisting>$ pegasus-monitord -r -d mysql://username:userpass@hostname/database_name diamond-0.dag.dagman.out</programlisting>

        <programlisting>$ pegasus-monitord -r -d sqlite:////tmp/diamond-0.db diamond-0.dag.dagman.out</programlisting>

        <para>In the first example, <emphasis
        role="bold">pegasus-monitord</emphasis> will send the data to the
        <emphasis role="bold">database_name</emphasis> database located at
        server <emphasis role="bold">hostname</emphasis>, using the <emphasis
        role="bold">username</emphasis> and <emphasis
        role="bold">userpass</emphasis> provided. In the second example,
        <emphasis role="bold">pegasus-monitord</emphasis> will store the data
        in the /tmp/diamond-0.db SQLite database.</para>

        <note>
          <para>For absolute paths four slashes are required when specifying
          an alternative database path in SQLite.</para>
        </note>

        <para>Users should also be aware that in all cases, with the exception
        of SQLite, the database should exist before <emphasis
        role="bold">pegasus-monitord</emphasis> is run (as it creates all
        needed tables but does not create the database itself).</para>

        <para>Finally, the following example:</para>

        <programlisting>$ pegasus-monitord -r --dest diamond-0.bp diamond-0.dag.dagman.out</programlisting>

        <para>sends events to the diamond-0.bp file. (please note that in
        replay mode, any data on the file will be overwritten).</para>

        <para>One important detail is that while processing a workflow,
        <emphasis role="bold">pegasus-monitord</emphasis> will automatically
        detect if/when sub-workflows are initiated, and will automatically
        track those sub-workflows as well. In this case, although <emphasis
        role="bold">pegasus-monitord</emphasis> will create a separate
        jobstate.log file in each workflow directory, the database at the
        top-level workflow will contain the information from not only the main
        workflow, but also from all sub-workflows.</para>
      </section>

      <section id="monitoring-files">
        <title>Monitoring related files in the workflow directory</title>

        <para><emphasis role="bold">Pegasus-monitord</emphasis> generates a
        number of files in each workflow directory:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">jobstate.log</emphasis>: contains a
            summary of workflow and job execution.</para>
          </listitem>
        </itemizedlist>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">monitord.log</emphasis>: contains any
            log messages generated by <emphasis
            role="bold">pegasus-monitord</emphasis>. It is not overwritten
            when it restarts. This file is not generated in replay mode, as
            all log messages from <emphasis
            role="bold">pegasus-monitord</emphasis> are output to the console.
            Also, when sub-workflows are involved, only the top-level workflow
            will have this log file. Starting with release 4.0 and 3.1.1,
            monitord.log file is rotated if it exists already.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">monitord.started</emphasis>: contains
            a timestamp indicating when <emphasis
            role="bold">pegasus-monitord</emphasis> was started. This file get
            overwritten every time <emphasis
            role="bold">pegasus-monitord</emphasis> starts.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">monitord.done</emphasis>: contains a
            timestamp indicating when <emphasis
            role="bold">pegasus-monitord</emphasis> finished. This file is
            overwritten every time <emphasis
            role="bold">pegasus-monitord</emphasis> starts.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">monitord.info</emphasis>: contains
            <emphasis role="bold">pegasus-monitord</emphasis> state
            information, which allows it to resume processing if a workflow
            does not finish properly and a rescue dag is submitted. This file
            is erased when <emphasis role="bold">pegasus-monitord</emphasis>
            is executed in replay mode.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">monitord.recover</emphasis>: contains
            <emphasis role="bold">pegasus-monitord</emphasis> state
            information that allows it to detect that a previous instance of
            <emphasis role="bold">pegasus-monitord</emphasis> failed (or was
            killed) midway through parsing a workflow's execution logs. This
            file is only present while <emphasis
            role="bold">pegasus-monitord</emphasis> is running, as it is
            deleted when it ends and the <emphasis
            role="bold">monitord.info</emphasis> file is generated.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">monitord.subwf.db</emphasis>: contains
            information that aids <emphasis
            role="bold">pegasus-monitord</emphasis> to track when
            sub-workflows fail and are re-planned/re-tried. It is overwritten
            when <emphasis role="bold">pegasus-monitord</emphasis> is started
            in replay mode.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">monitord-notifications.log</emphasis>:
            contains the log file for notification-related messages. Normally,
            this file only includes logs for failed notifications, but can be
            populated with all notification information when <emphasis
            role="bold">pegasus-monitord</emphasis> is run in verbose mode via
            the <emphasis role="bold">-v</emphasis> command-line
            option.</para>
          </listitem>
        </itemizedlist>
      </section>

      <section>
        <title>Multiple End points</title>

        <para>pegasus-monitord can be used to publish <link
        linkend="stampede_wf_events">events</link> to different backends at
        the same time. The configuration of this is managed through properties
        matching <emphasis
        role="bold">pegasus.catalog.workflow.&lt;variable-name&gt;.url</emphasis>
        .</para>

        <para>For example, to enable populating to an AMQP end point and a
        file format in addition to default sqlite you can configure as
        follows</para>

        <programlisting>pegasus.catalog.workflow.amqp.url amqp://vahi:XXXXX@amqp.isi.edu:5672/panorama/monitoring 
pegasus.catalog.workflow.file.url file:///lfs1/work/monitord/amqp/nl.bp </programlisting>

        <para>If you want to only override the default sqlite population ,
        then you can specify pegasus.catalog.workflow.url property .</para>
      </section>
    </section>

    <section id="stampede_schema_overview">
      <title>Overview of the Workflow Database Schema.</title>

      <para>Pegasus takes in a DAX which is composed of tasks. Pegasus plans
      it into a Condor DAG / Executable workflow that consists of Jobs. In
      case of Clustering, multiple tasks in the DAX can be captured into a
      single job in the Executable workflow. When DAGMan executes a job, a job
      instance is populated . Job instances capture information as seen by
      DAGMan. In case DAGMan retires a job on detecting a failure , a new job
      instance is populated. When DAGMan finds a job instance has finished ,
      an invocation is associated with job instance. In case of clustered job,
      multiple invocations will be associated with a single job instance. If a
      Pre script or Post Script is associated with a job instance, then
      invocations are populated in the database for the corresponding job
      instance.</para>

      <para>The current schema version is <emphasis role="bold">4.0</emphasis>
      that is stored in the schema_info table.</para>

      <figure>
        <title>Workflow Database Schema</title>

        <mediaobject>
          <imageobject>
            <imagedata fileref="images/stampede_schema_overview-small.png"
                       id="stampede_schema_overview_figure"/>
          </imageobject>
        </mediaobject>
      </figure>

      <section id="schema_upgrade_tool">
        <title>Stampede Schema Upgrade Tool</title>

        <para>Starting Pegasus 4.x the monitoring and statistics database
        schema has changed. If you want to use the pegasus-statistics,
        pegasus-analyzer and pegasus-plots against a 3.x database you will
        need to upgrade the schema first using the schema upgrade tool
        /usr/share/pegasus/sql/schema_tool.py or
        /path/to/pegasus-4.x/share/pegasus/sql/schema_tool.py</para>

        <para>Upgrading the schema is required for people using the MySQL
        database for storing their monitoring information if it was setup with
        3.x monitoring tools.</para>

        <para>If your setup uses the default SQLite database then the new
        databases run with Pegasus 4.x are automatically created with the
        correct schema. In this case you only need to upgrade the SQLite
        database from older runs if you wish to query them with the newer
        clients.</para>

        <para>To upgrade the database</para>

        <programlisting>For SQLite Database

<emphasis role="bold">cd /to/the/workflow/directory/with/3.x.monitord.db</emphasis>

Check the db version<emphasis role="bold">

/usr/share/pegasus/sql/schema_tool.py -c connString=sqlite:////to/the/workflow/directory/with/workflow.stampede.db</emphasis>
2012-02-29T01:29:43.330476Z INFO   netlogger.analysis.schema.schema_check.SchemaCheck.init |
2012-02-29T01:29:43.330708Z INFO   netlogger.analysis.schema.schema_check.SchemaCheck.check_schema.start |
2012-02-29T01:29:43.348995Z INFO   netlogger.analysis.schema.schema_check.SchemaCheck.check_schema
                                   | Current version set to: 3.1.
2012-02-29T01:29:43.349133Z ERROR  netlogger.analysis.schema.schema_check.SchemaCheck.check_schema
                                   | Schema version 3.1 found - expecting 4.0 - database admin will need to run upgrade tool.


Convert the Database to be version 4.x compliant<emphasis role="bold">

/usr/share/pegasus/sql/schema_tool.py -u connString=sqlite:////to/the/workflow/directory/with/workflow.stampede.db
</emphasis>2012-02-29T01:35:35.046317Z INFO   netlogger.analysis.schema.schema_check.SchemaCheck.init |
2012-02-29T01:35:35.046554Z INFO   netlogger.analysis.schema.schema_check.SchemaCheck.check_schema.start |
2012-02-29T01:35:35.064762Z INFO   netlogger.analysis.schema.schema_check.SchemaCheck.check_schema
                                  | Current version set to: 3.1.
2012-02-29T01:35:35.064902Z ERROR  netlogger.analysis.schema.schema_check.SchemaCheck.check_schema
                                  | Schema version 3.1 found - expecting 4.0 - database admin will need to run upgrade tool.
2012-02-29T01:35:35.065001Z INFO   netlogger.analysis.schema.schema_check.SchemaCheck.upgrade_to_4_0
                                  | Upgrading to schema version 4.0.

Verify if the database has been converted to Version 4.x<emphasis role="bold">

/usr/share/pegasus/sql/schema_tool.py -c connString=sqlite:////to/the/workflow/directory/with/workflow.stampede.db</emphasis>
2012-02-29T01:39:17.218902Z INFO   netlogger.analysis.schema.schema_check.SchemaCheck.init |
2012-02-29T01:39:17.219141Z INFO   netlogger.analysis.schema.schema_check.SchemaCheck.check_schema.start |
2012-02-29T01:39:17.237492Z INFO   netlogger.analysis.schema.schema_check.SchemaCheck.check_schema | Current version set to: 4.0.
2012-02-29T01:39:17.237624Z INFO   netlogger.analysis.schema.schema_check.SchemaCheck.check_schema | Schema up to date.

For upgrading a MySQL database the steps remain the same. The only thing that changes is the connection String to the database
E.g.<emphasis role="bold">

/usr/share/pegasus/sql/schema_tool.py -u connString=mysql://username:password@server:port/dbname

</emphasis></programlisting>

        <para>After the database has been upgraded you can use either 3.x or
        4.x clients to query the database with <emphasis
        role="bold">pegasus-statistics</emphasis>, as well as <emphasis
        role="bold">pegasus-plots </emphasis>and <emphasis
        role="bold">pegasus-analyzer.</emphasis></para>
      </section>

      <section>
        <title>Storing of Exitcode in the database</title>

        <para>Kickstart records capture raw status in addition to the exitcode
        . The exitcode is derived from the raw status. Starting with Pegasus
        4.0 release, all exitcode columns ( i.e invocation and job instance
        table columns ) are stored with the raw status by pegasus-monitord. If
        an exitcode is encountered while parsing the dagman log files , the
        value is converted to the corresponding raw status before it is
        stored. All user tools, pegasus-analyzer and pegasus-statistics then
        convert the raw status to exitcode when retrieving from the
        database.</para>
      </section>

      <section>
        <title>Multiplier Factor</title>

        <para>Starting with the 4.0 release, there is a multiplier factor
        associated with the jobs in the job_instance table. It defaults to
        one, unless the user associates a Pegasus profile key named <emphasis
        role="bold">cores</emphasis> with the job in the DAX. The factor can
        be used for getting more accurate statistics for jobs that run on
        multiple processors/cores or mpi jobs.</para>

        <para>The multiplier factor is used for computing the following
        metrics by pegasus statistics.</para>

        <itemizedlist>
          <listitem>
            <para>In the summary, the workflow cumulative job wall time</para>
          </listitem>

          <listitem>
            <para>In the summary, the cumulative job wall time as seen from
            the submit side</para>
          </listitem>

          <listitem>
            <para>In the jobs file, the multiplier factor is listed along-with
            the multiplied kickstart time.</para>
          </listitem>

          <listitem>
            <para>In the breakdown file, where statistics are listed per
            transformation the mean, min , max and average values take into
            account the multiplier factor.</para>
          </listitem>
        </itemizedlist>
      </section>
    </section>
  </section>

  <section id="stampede_wf_events">
    <title>Stampede Workflow Events</title>

    <para>All the events generated by the system ( Pegasus planner and
    monitoring daemon) are formatted as Netlogger BP events. The netlogger
    events that Pegasus generates are described in Yang schema file that can
    be found in the share/pegasus/schema/ directory. The stampede yang schema
    is described below.</para>

    <literallayout>
</literallayout>

    <section>
      <title>Typedefs</title>

      <para>The following typedefs are used in the yang schema to describe the
      certain event attributes.</para>

      <itemizedlist spacing="compact">
        <listitem>
          <para>distinguished-name</para>

          <programlisting>typedef distinguished-name {
   type string;
}</programlisting>
        </listitem>

        <listitem>
          <para>uuid</para>

          <programlisting>typedef uuid {
   type string {
       length "36";
       pattern
          '[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}';
   }
}
</programlisting>
        </listitem>

        <listitem>
          <para>intbool</para>

          <programlisting>typedef intbool {
   type uint8 {
       range "0 .. 1";
   }
}</programlisting>
        </listitem>

        <listitem>
          <para>nl_ts</para>

          <programlisting>typedef nl_ts {
    type string {
        pattern
          '(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(\.\d+)?(Z|[\+\-]\d{2}:\d{2}))|(\d{1,9}(\.\d+)?)';
     }
}
</programlisting>
        </listitem>

        <listitem>
          <para>peg_inttype</para>

          <programlisting>typedef peg_inttype {
    type uint8 {
        range "0 .. 11";
    }
}</programlisting>
        </listitem>

        <listitem>
          <para>peg_strtype</para>

          <programlisting>typedef peg_strtype {
    type enumeration {
        enum "unknown" {
          value 0;
        }
        enum "compute" {
          value 1;
        }
        enum "stage-in-tx" {
          value 2;
        }
        enum "stage-out-tx" {
          value 3;
        }
        enum "registration" {
          value 4;
        }
        enum "inter-site-tx" {
          value 5;
        }
        enum "create-dir" {
          value 6;
        }
        enum "staged-compute" {
          value 7;
        }
        enum "cleanup" {
          value 8;
        }
        enum "chmod" {
          value 9;
        }
        enum "dax" {
          value 10;
        }
        enum "dag" {
          value 11;
        }
   }
}</programlisting>
        </listitem>

        <listitem>
          <para>condor_jobstates</para>

          <programlisting>typedef condor_jobstates {
    type enumeration {
        enum "PRE_SCRIPT_STARTED" {
          value 0;
        }
        enum "PRE_SCRIPT_TERMINATED" {
          value 1;
        }
        enum "PRE_SCRIPT_SUCCESS" {
          value 2;
        }
        enum "PRE_SCRIPT_FAILED" {
          value 3;
        }
        enum "SUBMIT" {
          value 4;
        }
        enum "GRID_SUBMIT" {
          value 5;
        }
        enum "GLOBUS_SUBMIT" {
          value 6;
        }
        enum "SUBMIT_FAILED" {
          value 7;
        }
        enum "EXECUTE" {
          value 8;
        }
        enum "REMOTE_ERROR" {
          value 9;
        }
        enum "IMAGE_SIZE" {
          value 10;
        }
        enum "JOB_TERMINATED" {
          value 11;
        }
        enum "JOB_SUCCESS" {
          value 12;
        }
        enum "JOB_FAILURE" {
          value 13;
        }
        enum "JOB_HELD" {
          value 14;
        }
        enum "JOB_EVICTED" {
          value 15;
        }
        enum "JOB_RELEASED" {
          value 16;
        }
        enum "POST_SCRIPT_STARTED" {
          value 17;
        }
        enum "POST_SCRIPT_TERMINATED" {
          value 18;
        }
        enum "POST_SCRIPT_SUCCESS" {
          value 19;
        }
        enum "POST_SCRIPT_FAILED" {
          value 20;
        }
    }
}
</programlisting>
        </listitem>

        <listitem>
          <para>condor_wfstates</para>

          <programlisting>typedef condor_wfstates {
    type enumeration {
        enum "WORKFLOW_STARTED" {
          value 0;
        }
        enum "WORKFLOW_TERMINATED" {
          value 1;
        }
    }
}</programlisting>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>Groupings</title>

      <para>Groupings are groups of common attributes that different type of
      events refer to. The following groupings are defined.</para>

      <para><itemizedlist spacing="compact">
          <listitem>
            <para><emphasis role="bold">base-event</emphasis> - Common
            components in all events</para>

            <itemizedlist spacing="compact">
              <listitem>
                <para>ts - Timestamp, ISO8601 or numeric seconds since
                1/1/1970"</para>
              </listitem>

              <listitem>
                <para>level - Severity level of event. Subset of NetLogger BP
                levels. For '*.end' events, if status is non-zero then level
                should be Error."</para>
              </listitem>

              <listitem>
                <para>xwf.id - DAG workflow UUID</para>
              </listitem>
            </itemizedlist>

            <programlisting>grouping base-event {
      description
        "Common components in all events";
      leaf ts {
        type nl_ts;
        mandatory true;
        description
          "Timestamp, ISO8601 or numeric seconds since 1/1/1970";
      }

      leaf level {
        type enumeration {
          enum "Info" {
            value 0;
          }
          enum "Error" {
            value 1;
          }
        }
        description
          "Severity level of event. "
            + "Subset of NetLogger BP levels. "
            + "For '*.end' events, if status is non-zero then level should be Error.";
      }

      leaf xwf.id {
        type uuid;
        description "DAG workflow id";
      }
 }  // grouping base-event</programlisting>
          </listitem>

          <listitem>
            <para><emphasis role="bold">base-job-inst</emphasis> - Common
            components for all job instance events</para>

            <itemizedlist spacing="compact">
              <listitem>
                <para>all attributes from base-event</para>
              </listitem>

              <listitem>
                <para>job_inst.id - Job instance identifier i.e the submit
                sequence generated by monitord.</para>
              </listitem>

              <listitem>
                <para>js.id - Jobstate identifier</para>
              </listitem>

              <listitem>
                <para>job.id - Identifier for corresponding job in the
                DAG</para>
              </listitem>
            </itemizedlist>

            <programlisting>grouping base-job-inst {
      description
        "Common components for all job instance events";
      uses base-event;

      leaf job_inst.id {
        type int32;
        mandatory true;
        description
          "Job instance identifier i.e the submit sequence generated by monitord";
      }

      leaf js.id {
        type int32;
        description "Jobstate identifier";
      }

      leaf job.id {
        type string;
        mandatory true;
        description
          "Identifier for corresponding job in the DAG";
      }
    }</programlisting>
          </listitem>

          <listitem>
            <para><emphasis role="bold">sched-job-inst</emphasis> - Scheduled
            job instance.</para>

            <itemizedlist spacing="compact">
              <listitem>
                <para>all attributes from base-job-inst</para>
              </listitem>

              <listitem>
                <para>sched.id - Identifier for job in scheduler</para>
              </listitem>
            </itemizedlist>

            <programlisting>grouping sched-job-inst {
      description "Scheduled job instance";
      uses base-job-inst;

      leaf sched.id {
        type string;
        mandatory true;
        description
          "Identifier for job in scheduler";
      }
} </programlisting>
          </listitem>

          <listitem>
            <para><emphasis role="bold">base-metadata</emphasis></para>

            <itemizedlist spacing="compact">
              <listitem>
                <para>uses</para>
              </listitem>

              <listitem>
                <para>key</para>
              </listitem>

              <listitem>
                <para>value</para>
              </listitem>
            </itemizedlist>

            <programlisting>grouping base-metadata {
      description
        "Common components for all metadata events that describe metadata for an entity.";
      uses base-event;

      leaf key {
        type string;
        mandatory true;
        description
          "Key for the metadata tuple";
      }

      leaf value {
        type string;
        description
          "Corresponding value of the key";
      }
}  // grouping base-metadata</programlisting>
          </listitem>
        </itemizedlist></para>
    </section>

    <section>
      <title>Events</title>

      <para>The system generates following types of events, that are described
      below.</para>

      <itemizedlist spacing="compact">
        <listitem>
          <para><link
          linkend="stampede_wf_plan_event">stampede.wf.plan</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_static_start_event">stampede.static.start</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_static_end_event">stampede.static.end</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_xwf_start_event">stampede.xwf.start</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_xwf_end_event">stampede.xwf.end</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_task_info_event">stampede.task.info</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_task_edge_event">stampede.task.edge</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_wf_map_task_job_event">stampede.wf.map.task_job</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_xwf_map_subwf_job_event">stampede.xwf.map.subwf_job</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_int_metric">stampede.int.metric</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_job_info_event">stampede.job.info</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_job_edge_event">stampede.job.edge</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_job_inst_pre_start_event">stampede.job_inst.pre.start</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_job_inst_pre_term_event">stampede.job_inst.pre.term</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_job_inst_pre_end_event">stampede.job_inst.pre.end</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_job_inst_submit_start_event">stampede.job_inst.submit.start</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_job_inst_submit_end_event">stampede.job_inst.submit.end</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_job_inst_held_start_event">stampede.job_inst.held.start</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_job_inst_held_end_event">stampede.job_inst.held.end</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_job_inst_main_start_event">stampede.job_inst.main.start</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_job_inst_main_term_event">stampede.job_inst.main.term</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_job_inst_main_end_event">stampede.job_inst.main.end</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_job_inst_composite_event">stampede.job_inst.composite</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_job_inst_post_start_event">stampede.job_inst.post.start</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_job_inst_post_term_event">stampede.job_inst.post.term</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_job_inst_post_end_event">stampede.job_inst.post.end</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_job_inst_host_info_event">stampede.job_inst.host.info</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_job_inst_image_info_event">stampede.job_inst.image.info</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_job_inst_tag_event">stampede.job_inst.tag</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_inv_start_event">stampede.inv.start</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_inv_end_event">stampede.inv.end</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_static_meta_start_event">stampede.static.meta.start</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_static_meta_end_event">stampede.static.meta.end</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_xwf_meta_event">stampede.xwf.meta</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_task_meta_event">stampede.task.meta</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_task_monitoring">stampede.task.monitoring</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_rc_meta_event">stampede.rc.meta</link></para>
        </listitem>

        <listitem>
          <para><link
          linkend="stampede_wf_map_file_event">stampede.wf.map.file</link></para>
        </listitem>
      </itemizedlist>

      <para>The events are described in detail below</para>

      <itemizedlist spacing="compact">
        <listitem id="stampede_wf_plan_event">
          <para><emphasis role="bold">stampede.wf.plan</emphasis></para>

          <programlisting>container stampede.wf.plan {
            uses base-event;

            leaf submit.hostname {
              type inet:host;
              mandatory true;
              description
                "The hostname of the Pegasus submit host";
            }

            leaf dax.label {
              type string;
              default "workflow";
              description
                "Label for abstract workflow specification";
            }

            leaf dax.index {
              type string;
              default "workflow";
              description
                "Index for the DAX";
            }

            leaf dax.version {
              type string;
              mandatory true;
              description
                "Version number for DAX";
            }

            leaf dax.file {
              type string;
              mandatory true;
              description
                "Filename for for the DAX";
            }

            leaf dag.file.name {
              type string;
              mandatory true;
              description
                "Filename for the DAG";
            }

            leaf planner.version {
              type string;
              mandatory true;
              description
                "Version string for Pegasus planner, e.g. 3.0.0cvs";
            }

            leaf grid_dn {
              type distinguished-name;
              description
                "Grid DN of submitter";
            }

            leaf user {
              type string;
              description
                "User name of submitter";
            }

            leaf submit.dir {
              type string;
              mandatory true;
              description
                "Directory path from which workflow was submitted";
            }

            leaf argv {
              type string;
              description
                "All arguments given to planner on command-line";
            }

            leaf parent.xwf.id {
              type uuid;
              description
                "Parent workflow in DAG, if any";
            }

            leaf root.xwf.id {
              type string;
              mandatory true;
              description
                "Root of workflow hierarchy, in DAG. "
                  + "Use this workflow's UUID if it is the root";
            }
}  // container stampede.wf.plan</programlisting>
        </listitem>

        <listitem id="stampede_static_start_event">
          <para><emphasis role="bold">stampede.static.start</emphasis></para>

          <programlisting>container stampede.static.start {
     uses base-event;
} </programlisting>
        </listitem>

        <listitem id="stampede_static_end_event">
          <para><emphasis role="bold">stampede.static.end</emphasis></para>

          <programlisting>container stampede.static.end {
    uses base-event;
}  //</programlisting>
        </listitem>

        <listitem id="stampede_xwf_start_event">
          <para><emphasis role="bold">stampede.xwf.start</emphasis></para>

          <programlisting>container stampede.xwf.start {
            uses base-event;

            leaf restart_count {
              type uint32;
              mandatory true;
              description
                "Number of times workflow was restarted (due to failures)";
            }
}  // container stampede.xwf.start</programlisting>
        </listitem>

        <listitem id="stampede_xwf_end_event">
          <para><emphasis role="bold">stampede.xwf.end</emphasis></para>

          <programlisting>container stampede.xwf.end {
            uses base-event;

            leaf restart_count {
              type uint32;
              mandatory true;
              description
                "Number of times workflow was restarted (due to failures)";
            }

            leaf status {
              type int16;
              mandatory true;
              description
                "Status of workflow. 0=success, -1=failure";
            }
}  // container stampede.xwf.end
</programlisting>
        </listitem>

        <listitem id="stampede_task_info_event">
          <para><emphasis role="bold">stampede.task.info</emphasis></para>

          <programlisting>container stampede.task.info {
            description
              "Information about task in DAX";
            uses base-event;

            leaf transformation {
              type string;
              mandatory true;
              description
                "Logical name of the underlying executable";
            }

            leaf argv {
              type string;
              description
                "All arguments given to transformation on command-line";
            }

            leaf type {
              type peg_inttype;
              mandatory true;
              description "Type of task";
            }

            leaf type_desc {
              type peg_strtype;
              mandatory true;
              description
                "String description of task type";
            }

            leaf task.id {
              type string;
              mandatory true;
              description
                "Identifier for this task in the DAX";
            }
          }  // container stampede.task.info</programlisting>
        </listitem>

        <listitem id="stampede_task_edge_event">
          <para><emphasis role="bold">stampede.task.edge</emphasis></para>

          <programlisting>container stampede.task.edge {
            description
              "Represents child/parent relationship between two tasks in DAX";
            uses base-event;

            leaf parent.task.id {
              type string;
              mandatory true;
              description "Parent task";
            }

            leaf child.task.id {
              type string;
              mandatory true;
              description "Child task";
            }
}  // container stampede.task.edge</programlisting>
        </listitem>

        <listitem id="stampede_wf_map_task_job_event">
          <para><emphasis
          role="bold">stampede.wf.map.task_job</emphasis><programlisting>container stampede.wf.map.task_job {

            description
              "Relates a DAX task to a DAG job.";
            uses base-event;

            leaf task.id {
              type string;
              mandatory true;
              description
                "Identifier for the task in the DAX";
            }

            leaf job.id {
              type string;
              mandatory true;
              description
                "Identifier for corresponding job in the DAG";
            }
}  // container stampede.wf.map.task_job</programlisting></para>
        </listitem>

        <listitem id="stampede_xwf_map_subwf_job_event">
          <para><emphasis
          role="bold">stampede.xwf.map.subwf_job</emphasis><programlisting>container stampede.xwf.map.subwf_job {

            description
              "Relates a sub workflow to the corresponding job instance";
            uses base-event;

            leaf subwf.id {
              type string;
              mandatory true;
              description
                "Sub Workflow Identified / UUID";
            }

            leaf job.id {
              type string;
              mandatory true;
              description
                "Identifier for corresponding job in the DAG";
            }

            leaf job_inst.id {
              type int32;
              mandatory true;
              description
                "Job instance identifier i.e the submit sequence generated by monitord";
            }
}  // container stampede.xwf.map.subwf_job</programlisting></para>
        </listitem>

        <listitem id="stampede_job_info_event">
          <para><emphasis
          role="bold">stampede.job.info</emphasis><programlisting>container stampede.job.info {

            description
              "A description of a job in the DAG";
            uses base-event;

            leaf job.id {
              type string;
              mandatory true;
              description
                "Identifier for job in the DAG";
            }

            leaf submit_file {
              type string;
              mandatory true;
              description
                "Name of file being submitted to the scheduler";
            }

            leaf type {
              type peg_inttype;
              mandatory true;
              description "Type of task";
            }

            leaf type_desc {
              type peg_strtype;
              mandatory true;
              description
                "String description of task type";
            }

            leaf clustered {
              type intbool;
              mandatory true;
              description
                "Whether job is clustered or not";
            }

            leaf max_retries {
              type uint32;
              mandatory true;
              description
                "How many retries are allowed for this job before giving up";
            }

            leaf task_count {
              type uint32;
              mandatory true;
              description
                "Number of DAX tasks for this job. "
                  + "Auxiliary jobs without a task in the DAX will have the value '0'";
            }

            leaf executable {
              type string;
              mandatory true;
              description
                "Program to execute";
            }

            leaf argv {
              type string;
              description
                "All arguments given to executable (on command-line)";
            }
}  // container stampede.job.info</programlisting></para>
        </listitem>

        <listitem id="stampede_job_edge_event">
          <para><emphasis
          role="bold">stampede.job.edge</emphasis><programlisting>container stampede.job.edge {

            description
              "Parent/child relationship between two jobs in the DAG";
            uses base-event;

            leaf parent.job.id {
              type string;
              mandatory true;
              description "Parent job";
            }

            leaf child.job.id {
              type string;
              mandatory true;
              description "Child job";
            }
}  // container stampede.job.edge</programlisting></para>
        </listitem>

        <listitem id="stampede_job_inst_pre_start_event">
          <para><emphasis
          role="bold">stampede.job_inst.pre.start</emphasis><programlisting>container stampede.job_inst.pre.start {

            description
              "Start of a prescript for a job instance";
            uses base-job-inst;
}  // container stampede.job_inst.pre.start</programlisting></para>
        </listitem>

        <listitem id="stampede_job_inst_pre_term_event">
          <para><emphasis
          role="bold">stampede.job_inst.pre.term</emphasis><programlisting>container stampede.job_inst.pre.term {
            description
              "Job prescript is terminated (success or failure not yet known)";
}  // container stampede.job_inst.pre.term</programlisting></para>
        </listitem>

        <listitem id="stampede_job_inst_pre_end_event">
          <para><emphasis
          role="bold">stampede.job_inst.pre.end</emphasis><programlisting>container stampede.job_inst.pre.end {
            description
              "End of a prescript for a job instance";
            uses base-job-inst;

            leaf status {
              type int32;
              mandatory true;
              description
                "Status of prescript. 0 is success, -1 is error";
            }

            leaf exitcode {
              type int32;
              mandatory true;
              description
                "the exitcode with which the prescript exited";
            }
}  // container stampede.job_inst.pre.end</programlisting></para>
        </listitem>

        <listitem id="stampede_job_inst_submit_start_event">
          <para><emphasis
          role="bold">stampede.job_inst.submit.start</emphasis><programlisting>container stampede.job_inst.submit.start {
            description
              "When job instance is going to be submitted. "
                + "Scheduler job id is not yet known";
            uses sched-job-inst;
}  // container stampede.job_inst.submit.start</programlisting></para>
        </listitem>

        <listitem id="stampede_job_inst_submit_end_event">
          <para><emphasis
          role="bold">stampede.job_inst.submit.end</emphasis><programlisting>container stampede.job_inst.submit.end {
            description
              "When executable job is submitted";
            uses sched-job-inst;

            leaf status {
              type int16;
              mandatory true;
              description
                "Status of workflow. 0=success, -1=failure";
            }
}  // container stampede.job_inst.submit.end</programlisting></para>
        </listitem>

        <listitem id="stampede_job_inst_held_start_event">
          <para><emphasis
          role="bold">stampede.job_inst.held.start</emphasis><programlisting>container stampede.job_inst.held.start {
            description
              "When Condor holds the jobs";
            uses sched-job-inst;
}  // container stampede.job_inst.held.start</programlisting></para>
        </listitem>

        <listitem id="stampede_job_inst_held_end_event">
          <para><emphasis
          role="bold">stampede.job_inst.held.end</emphasis><programlisting>container stampede.job_inst.held.end {
            description
              "When the job is released after being held";
            uses sched-job-inst;

            leaf status {
              type int16;
              mandatory true;
              description
                "Status of workflow. 0=success, -1=failure";
            }
}  // container stampede.job_inst.held.end</programlisting></para>
        </listitem>

        <listitem id="stampede_job_inst_main_start_event">
          <para><emphasis
          role="bold">stampede.job_inst.main.start</emphasis><programlisting>container stampede.job_inst.main.start {
            description
              "Start of execution of a scheduler job";
            uses sched-job-inst;

            leaf stdin.file {
              type string;
              description
                "Path to file containing standard input of job";
            }

            leaf stdout.file {
              type string;
              mandatory true;
              description
                "Path to file containing standard output of job";
            }

            leaf stderr.file {
              type string;
              mandatory true;
              description
                "Path to file containing standard error of job";
            }
}  // container stampede.job_inst.main.start
</programlisting></para>
        </listitem>

        <listitem id="stampede_job_inst_main_term_event">
          <para><emphasis
          role="bold">stampede.job_inst.main.term</emphasis></para>

          <programlisting>container stampede.job_inst.main.term {
            description
              "Job is terminated (success or failure not yet known)";
            uses sched-job-inst;

            leaf status {
              type int32;
              mandatory true;
              description
                "Execution status. 0=means job terminated, -1=job was evicted, not terminated";
            }
}  // container stampede.job_inst.main.term</programlisting>
        </listitem>

        <listitem id="stampede_job_inst_main_end_event">
          <para><emphasis
          role="bold">stampede.job_inst.main.end</emphasis><programlisting>container stampede.job_inst.main.end {
            description
              "End of main part of scheduler job";
            uses sched-job-inst;

            leaf stdin.file {
              type string;
              description
                "Path to file containing standard input of job";
            }

            leaf stdout.file {
              type string;
              mandatory true;
              description
                "Path to file containing standard output of job";
            }

            leaf stdout.text {
              type string;
              description
                "Text containing output of job";
            }

            leaf stderr.file {
              type string;
              mandatory true;
              description
                "Path to file containing standard error of job";
            }

            leaf stderr.text {
              type string;
              description
                "Text containing standard error of job";
            }

            leaf user {
              type string;
              description
                "Scheduler's name for user";
            }

            leaf site {
              type string;
              mandatory true;
              description
                "DAX name for the site at which the job ran";
            }

            leaf work_dir {
              type string;
              description
                "Path to working directory";
            }

            leaf local.dur {
              type decimal64 {
                fraction-digits 6;
              }
              units "seconds";
              description
                "Duration as seen at the local node";
            }

            leaf status {
              type int32;
              mandatory true;
              description
                "Execution status. 0=success, -1=failure";
            }

            leaf exitcode {
              type int32;
              mandatory true;
              description
                "the exitcode with which the executable exited";
            }

            leaf multiplier_factor {
              type int32;
              mandatory true;
              description
                "the multiplier factor for use in statistics";
            }

            leaf cluster.start {
              type nl_ts;
              description
                "When the enclosing cluster started";
            }

            leaf cluster.dur {
              type decimal64 {
                fraction-digits 6;
              }
              units "seconds";
              description
                "Duration of enclosing cluster";
            }
}  // container stampede.job_inst.main.end</programlisting></para>
        </listitem>

        <listitem id="stampede_job_inst_post_start_event">
          <para><emphasis
          role="bold">stampede.job_inst.post.start</emphasis><programlisting>container stampede.job_inst.post.start {
            description
              "Start of a postscript for a job instance";
            uses sched-job-inst;
}  // container stampede.job_inst.post.start
</programlisting></para>
        </listitem>

        <listitem id="stampede_job_inst_post_term_event">
          <para><emphasis
          role="bold">stampede.job_inst.post.term</emphasis><programlisting>container stampede.job_inst.post.term {
            description
              "Job postscript is terminated (success or failure not yet known)";
            uses sched-job-inst;
}  // container stampede.job_inst.post.term</programlisting></para>
        </listitem>

        <listitem id="stampede_job_inst_post_end_event">
          <para><emphasis
          role="bold">stampede.job_inst.post.end</emphasis><programlisting>container stampede.job_inst.post.end {
            description
              "End of a postscript for a job instance";
            uses sched-job-inst;

            leaf status {
              type int32;
              mandatory true;
              description
                "Status of postscript. 0 is success, -1=failure";
            }

            leaf exitcode {
              type int32;
              mandatory true;
              description
                "the exitcode with which the postscript exited";
            }
}  // container stampede.job_inst.post.end</programlisting></para>
        </listitem>

        <listitem id="stampede_job_inst_host_info_event">
          <para><emphasis
          role="bold">stampede.job_inst.host.info</emphasis><programlisting>container stampede.job_inst.host.info {
            description
              "Host information associated with a job instance";
            uses base-job-inst;

            leaf site {
              type string;
              mandatory true;
              description "Site name";
            }

            leaf hostname {
              type inet:host;
              mandatory true;
              description "Host name";
            }

            leaf ip {
              type inet:ip-address;
              mandatory true;
              description "IP address";
            }

            leaf total_memory {
              type uint64;
              description
                "Total RAM on host";
            }

            leaf uname {
              type string;
              description
                "Operating system name";
            }
}  // container stampede.job_inst.host.info</programlisting></para>
        </listitem>

        <listitem id="stampede_job_inst_image_info_event">
          <para><emphasis
          role="bold">stampede.job_inst.image.info</emphasis><programlisting>container stampede.job_inst.image.info {
            description
              "Image size associated with a job instance";
            uses base-job-inst;

            leaf size {
              type uint64;
              description "Image size";
            }

            leaf sched.id {
              type string;
              mandatory true;
              description
                "Identifier for job in scheduler";
            }
}  // container stampede.job_inst.image.info</programlisting></para>
        </listitem>

        <listitem id="stampede_job_inst_tag_event">
          <para><emphasis
          role="bold">stampede.job_inst.tag</emphasis><programlisting>container stampede.job_inst.tag {
            description
              "A tag event to tag errors at a job_instance level";
            uses base-job-inst;

            leaf name {
              type string;
              description "Name of tagged event such as int.error";
            }

            leaf count {
              type int32;
              mandatory true;
              description
                "count of occurences of the events of type name for the job_instance";
            }
}  // container stampede.job_inst.tag</programlisting></para>
        </listitem>

        <listitem id="stampede_job_inst_composite_event">
          <para><emphasis
          role="bold">stampede.job_inst.composite</emphasis><programlisting>container stampede.job_inst.composite{
            description
              "A de-normalized composite event at the job_instance level that captures all the job information. Useful when populating AMQP";
            uses base-job-inst;

           leaf jobtype {
              type string;
              description
                "Type of job as classified by the planner.";
            }

           leaf stdin.file {
              type string;
              description
                "Path to file containing standard input of job";
            }

            leaf stdout.file {
              type string;
              mandatory true;
              description
                "Path to file containing standard output of job";
            }

            leaf stdout.text {
              type string;
              description
                "Text containing output of job";
            }

            leaf stderr.file {
              type string;
              mandatory true;
              description
                "Path to file containing standard error of job";
            }

            leaf stderr.text {
              type string;
              description
                "Text containing standard error of job";
            }

            leaf user {
              type string;
              description
                "Scheduler's name for user";
            }

            leaf site {
              type string;
              mandatory true;
              description
                "DAX name for the site at which the job ran";
            }

            leaf hostname {
              type inet:host;
              mandatory true;
              description "Host name";
            }

            leaf  {
              type string;
              description
                "Path to working directory";
            }

            leaf local.dur {
              type decimal64 {
                fraction-digits 6;
              }
              units "seconds";
              description
                "Duration as seen at the local node";
            }

            leaf status {
              type int32;
              mandatory true;
              description
                "Execution status. 0=success, -1=failure";
            }

            leaf exitcode {
              type int32;
              mandatory true;
              description
                "the exitcode with which the executable exited";
            }

            leaf multiplier_factor {
              type int32;
              mandatory true;
              description
                "the multiplier factor for use in statistics";
            }

            leaf cluster.start {
              type nl_ts;
              description
                "When the enclosing cluster started";
            }

            leaf cluster.dur {
              type decimal64 {
                fraction-digits 6;
              }
              units "seconds";
              description
                "Duration of enclosing cluster";
            }

            leaf int_error_count {
              type int32;
              mandatory true;
              description
                "number of integrity errors encountered";
            }
}  // container stampede.job_inst.composite</programlisting></para>
        </listitem>

        <listitem id="stampede_inv_start_event">
          <para><emphasis
          role="bold">stampede.inv.start</emphasis><programlisting>container stampede.inv.start {
            description
              "Start of an invocation";
            uses base-event;

            leaf job_inst.id {
              type int32;
              mandatory true;
              description
                "Job instance identifier i.e the submit sequence generated by monitord";
            }

            leaf job.id {
              type string;
              mandatory true;
              description
                "Identifier for corresponding job in the DAG";
            }

            leaf inv.id {
              type int32;
              mandatory true;
              description
                "Identifier for invocation. "
                  + "Sequence number, with -1=prescript and -2=postscript";
            }
}  // container stampede.inv.start
</programlisting></para>
        </listitem>

        <listitem id="stampede_inv_end_event">
          <para><emphasis
          role="bold">stampede.inv.end</emphasis><programlisting>container stampede.inv.end {
            description
              "End of an invocation";
            uses base-event;

            leaf job_inst.id {
              type int32;
              mandatory true;
              description
                "Job instance identifier i.e the submit sequence generated by monitord";
            }

            leaf inv.id {
              type int32;
              mandatory true;
              description
                "Identifier for invocation. "
                  + "Sequence number, with -1=prescript and -2=postscript";
            }

            leaf job.id {
              type string;
              mandatory true;
              description
                "Identifier for corresponding job in the DAG";
            }

            leaf start_time {
              type nl_ts;
              description
                "The start time of the event";
            }

            leaf dur {
              type decimal64 {
                fraction-digits 6;
              }
              units "seconds";
              description
                "Duration of invocation";
            }

            leaf remote_cpu_time {
              type decimal64 {
                fraction-digits 6;
              }
              units "seconds";
              description
                "remote CPU time computed as the stime  + utime";
            }

            leaf exitcode {
              type int32;
              description
                "the exitcode with which the executable exited";
            }

            leaf transformation {
              type string;
              mandatory true;
              description
                "Transformation associated with this invocation";
            }

            leaf executable {
              type string;
              mandatory true;
              description
                "Program executed for this invocation";
            }

            leaf argv {
              type string;
              description
                "All arguments given to executable on command-line";
            }

            leaf task.id {
              type string;
              description
                "Identifier for related task in the DAX";
            }
}  // container stampede.inv.end</programlisting></para>
        </listitem>

        <listitem id="stampede_int_metric">
          <para><emphasis
          role="bold">stampede.int.metric</emphasis><programlisting>container stampede.int.metric {
            description
              "additional task events picked up from the job stdout";
            uses base-event;

            leaf job_inst.id {
              type int32;
              mandatory true;
              description
                 "Job instance identifier i.e the submit sequence generated by monitord";
            }

            leaf job.id {
              type string;
              mandatory true;
              description
                 "Identifier for corresponding job in the DAG";
            }

            leaf type{
              type string;
              description
                "enumerated type of metrics check|compute";
            }

            leaf file_type{
              type string;
              description
                "enumerated type of file types input|output";
            }

            leaf count{
              type int32;
              description
                "number of integrity events grouped by type , file_type ";
            }

            leaf duration{
              type float;
              description
                "duration in seconds it took to perform these events ";
            }
}  // container stampede.int.metric</programlisting></para>
        </listitem>

        <listitem id="stampede_static_meta_start_event">
          <para><emphasis
          role="bold">stampede.static.meta.start</emphasis><programlisting>container stampede.static.meta.start {
            uses base-event;
}  // container stampede.static.meta.start
</programlisting></para>
        </listitem>

        <listitem id="stampede_static_meta_end_event">
          <para><emphasis
          role="bold">stampede.static.meta.end</emphasis><programlisting>container stampede.static.meta.end { 
            uses base-event; 
} // container stampede.static.meta.end </programlisting></para>
        </listitem>

        <listitem id="stampede_xwf_meta_event">
          <para><emphasis role="bold">stampede.xwf.meta</emphasis></para>

          <programlisting>container stampede.xwf.meta {
            description
              "Metadata associated with a workflow";
            uses base-metadata;
}  // container stampede.xwf.meta
</programlisting>
        </listitem>

        <listitem id="stampede_task_meta_event">
          <para><emphasis
          role="bold">stampede.task.meta</emphasis><programlisting>container stampede.task.meta {
            description
              "Metadata associated with a task";
            uses base-metadata;

            leaf task.id {
              type string;
              description
                "Identifier for related task in the DAX";
            }
}  // container stampede.task.meta</programlisting></para>
        </listitem>

        <listitem id="stampede_task_monitoring">
          <para><emphasis
          role="bold">stampede.task.monitoring</emphasis><programlisting>container stampede.task.monitoring {
            description
              "additional task events picked up from the job stdout";
            uses base-event;

            leaf job_inst.id {
              type int32;
              mandatory true;
              description
                 "Job instance identifier i.e the submit sequence generated by monitord";
            }

            leaf job.id {
              type string;
              mandatory true;
              description
                 "Identifier for corresponding job in the DAG";
            }

            leaf monitoring_event{
              type string;
              description
                "the name of the monitoring event parsed from the job stdout";
            }

            leaf key{
              type string;
              description
                "user defined keys in their payload in the event defined in the job stdout";
            }
}  // container stampede.task.meta</programlisting></para>
        </listitem>

        <listitem id="stampede_rc_meta_event">
          <para><emphasis
          role="bold">stampede.rc.meta</emphasis><programlisting>container stampede.rc.meta {
            description
              "Metadata associated with a file in the replica catalog";
            uses base-metadata;

            leaf lfn.id {
              type string;
              description
                "Logical File Identifier for the file";
            }
}  // container stampede.rc.meta</programlisting></para>
        </listitem>

        <listitem id="stampede_wf_map_file_event">
          <para><emphasis role="bold">stampede.wf.map.file</emphasis></para>

          <programlisting>container stampede.wf.map.file {
            description
              "Event that captures what task generates or consumes a particular file";
            uses base-event;

            leaf lfn.id {
              type string;
              description
                "Logical File Identifier for the file";
            }

            leaf task.id {
              type string;
              description
                "Identifier for related task in the DAX";
            }
}  // container stampede.wf.map.file</programlisting>
        </listitem>
      </itemizedlist>
    </section>
  </section>

  <section id="monitoring_amqp">
    <title>Publishing to AMQP Message Servers</title>

    <para>The<link linkend="stampede_wf_events"> workflow events</link>
    generated by <emphasis>pegasus-monitord </emphasis>can also be used to
    publish to an AMQP message server such as RabbitMQ in addition to the
    stampede workflow database.</para>

    <note>
      <para>A thing to keep in mind. The workflow events are documented as
      conforming to the netlogger requirements. When events are pushed to an
      AMQP endpoint, the . in the keys are replaced by _ .</para>
    </note>

    <section>
      <title id="monitoring_amqp_configuration">Configuration</title>

      <para>In order to get <emphasis>pegasus-monitord</emphasis> to populate
      to a message queue, you can set the following property</para>

      <programlisting>pegasus.catalog.workflow.amqp.url amqp://[USERNAME:PASSWORD@]amqp.isi.edu[:port]/&lt;exchange_name&gt; </programlisting>

      <para>The routing key set for the messages matches the name of the
      stampede workflow event being sent. By default, if you enable AMQP
      population only the following events are sent to the server</para>

      <itemizedlist>
        <listitem>
          <para>stampede.job_inst.tag</para>
        </listitem>

        <listitem>
          <para>stampede.inv.end</para>
        </listitem>

        <listitem>
          <para>stampede.wf.plan</para>
        </listitem>
      </itemizedlist>

      <para>To configure additional events, you can specify a comma separated
      list of events that need to be sent using the property <emphasis
      role="bold">pegasus.catalog.workflow.amqp.events </emphasis>. For
      example</para>

      <programlisting>pegasus.catalog.workflow.amqp.events = stampede.xwf.*,stampede.static.*</programlisting>

      <note>
        <para>To get all events you can just specify * as the value to the
        property.</para>
      </note>
    </section>

    <section id="amqp_rabbitmq_es">
      <title>Monitord, RabbitMQ, ElasticSearch Example</title>

      <para>The AMQP support in Monitord is still a work in progress, but even
      the current functionality provides basic support for getting the
      monitoring data into ElasticSearch. In our development environment, we
      use a RabbitMQ instance with a simple exhange/queue. The configuration
      required for Pegasus is:</para>

      <programlisting># help Pegasus developers collect data on integrity failures                                        
pegasus.monitord.encoding = json                                                                    
pegasus.catalog.workflow.amqp.url = amqp://friend:donatedata@msgs.pegasus.isi.edu:5672/prod/workflows
          </programlisting>

      <para>On the other side of the queue, Logstash is configured to receive
      the messages and forward them to ElasticSearch. The Logstash pipeline
      looks something like:</para>

      <programlisting>
input {
  rabbitmq {
    type =&gt; "workflow-events"
    host =&gt; "msg.pegasus.isi.edu"
    vhost =&gt; "prod"
    queue =&gt; "workflows-es"
    heartbeat =&gt; 30
    durable =&gt; true
    password =&gt; "XXXXXX"
    user =&gt; "prod-logstash"
  }
}

filter {
  if [type] == "workflow-events" {
    mutate {
      convert =&gt; {
        "dur" =&gt; "float"
        "remote_cpu_time" =&gt; "float"
      }
    }
    date {
      # set @timestamp from the ts of the actual event
      match =&gt; [ "ts", "UNIX" ]
    }
    date {
      match =&gt; [ "start_time", "UNIX" ]
      target =&gt; "start_time_human"
    }
    fingerprint {
      # create unique document ids
      source =&gt; "ts"
      concatenate_sources =&gt; true
      method =&gt; "SHA1"
      key =&gt; "Pegasus Event"
      target =&gt; "[@metadata][fingerprint]"
    }
  }
}
  
output {
  if [type] == "workflow-events" {
    elasticsearch {
      "hosts" =&gt; ["es1.isi.edu:9200", "es2.isi.edu:9200"]
      "sniffing" =&gt; false
      "document_type" =&gt; "workflow-events"
      "document_id" =&gt; "%{[@metadata][fingerprint]}"
      "index" =&gt; "workflow-events-%{+YYYY.MM.dd}"
      "template" =&gt; "/usr/share/logstash/templates/workflow-events.json"
      "template_name" =&gt; "workflow-events-*"
      "template_overwrite" =&gt; true
    }
  }

}
          </programlisting>

      <para>Once the data is ElasticSearch, you can easily create for example
      Grafana dashboard like:</para>

      <imagedata fileref="images/grafana.png" width="100%"/>
    </section>

    <section id="elk_stack_infrastructure">
      <title>A Pre-Configured Data Collection Pipeline</title>

      <para>In this <ulink
          url="https://github.com/pegasus-isi/dibbs-data-collection-setup">
          <citetitle>repository</citetitle>
        </ulink>, we provide a containerized data-collection/visualization
      pipeline similar to what we use in production. The figure below
      illustrates the processes involved in the pipeline and how they are
      connected to one another. For more information regarding setup and
      usage, please visit the link referenced above.</para>

      <imagedata fileref="images/data-collection-pipeline.svg" width="100%"/>
    </section>
  </section>
</chapter>
