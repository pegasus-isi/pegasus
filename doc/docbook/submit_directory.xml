<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="submit_directory">
  <title>Submit Directory Details</title>

  <para>This chapter describes the submit directory content after Pegasus has
  planned a workflow. Pegasus takes in an abstract workflow ( DAX ) and
  generates an executable workflow (DAG) in the submit directory.</para>

  <para>This document also describes the various Replica Selection Strategies
  in Pegasus.</para>

  <section id="submit_directory_layout">
    <title>Layout</title>

    <para>Each executable workflow is associated with a submit directory, and
    includes the following:</para>

    <orderedlist>
      <listitem>
        <para><emphasis role="bold">&lt;daxlabel-daxindex&gt;.dag
        </emphasis></para>

        <para>This is the Condor DAGMman dag file corresponding to the
        executable workflow generated by Pegasus. The dag file describes the
        edges in the DAG and information about the jobs in the DAG. Pegasus
        generated .dag file usually contains the following information for
        each job</para>

        <orderedlist>
          <listitem>
            <para>The job submit file for each job in the DAG.</para>
          </listitem>

          <listitem>
            <para>The post script that is to be invoked when a job completes.
            This is usually located at <emphasis
            role="bold">$PEGASUS_HOME/bin/exitpost</emphasis> and parses the
            kickstart record in the job's<emphasis role="bold">.out
            file</emphasis> and determines the exitcode.</para>
          </listitem>

          <listitem>
            <para>JOB RETRY - the number of times the job is to be retried in
            case of failure. In Pegasus, the job postscript exits with a non
            zero exitcode if it determines a failure occurred.</para>
          </listitem>
        </orderedlist>
      </listitem>

      <listitem>
        <para><emphasis
        role="bold">&lt;daxlabel-daxindex&gt;.dag.dagman.out</emphasis></para>

        <para>When a DAG ( .dag file ) is executed by Condor DAGMan , the
        DAGMan writes out it's output to the <emphasis
        role="bold">&lt;daxlabel-daxindex&gt;.dag.dagman.out file</emphasis> .
        This file tells us the progress of the workflow, and can be used to
        determine the status of the workflow. Most of pegasus tools mine the
        <emphasis role="bold">dagman.out</emphasis> or <emphasis
        role="bold">jobstate.log</emphasis> to determine the progress of the
        workflows.</para>
      </listitem>

      <listitem>
        <para><emphasis
        role="bold">&lt;daxlabel-daxindex&gt;.static.bp</emphasis></para>

        <para>This file contains netlogger events that link jobs in the DAG
        with the jobs in the DAX. This file is parsed by pegasus-monitord when
        a workflow starts and populated to the stampede backend.</para>
      </listitem>

      <listitem>
        <para><emphasis
        role="bold">&lt;daxlabel-daxindex&gt;.notify</emphasis></para>

        <para>This file contains all the notifications that need to be set for
        the workflow and the jobs in the executable workflow. The format of
        notify file is described <link
        linkend="pegasus_notify_file">here</link></para>
      </listitem>

      <listitem>
        <para><emphasis
        role="bold">&lt;daxlabel-daxindex&gt;.replica.store</emphasis></para>

        <para>This is a file based replica catalog, that only lists file
        locations are mentioned in the DAX.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">&lt;daxlabel-daxindex&gt;.dot
        </emphasis></para>

        <para>Pegasus creates a dot file for the executable workflow in
        addition to the .dag file. This can be used to visualize the
        executable workflow using the dot program.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">&lt;job&gt;.sub</emphasis></para>

        <para>Each job in the executable workflow is associated with it's own
        submit file. The submit file tells Condor how to execute the
        job.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">&lt;job&gt;.out.00n</emphasis></para>

        <para>The stdout of the executable referred in the job submit file. In
        Pegasus, most jobs are launched via kickstart. Hence, this file
        contains the kickstart XML provenance record that captures runtime
        provenance on the remote node where the job was executed. n varies
        from 1-N where N is the JOB RETRY value in the .dag file. The exitpost
        executable is invoked on the &lt;job&gt;.out file and it moves the
        &lt;job&gt;.out to &lt;job&gt;.out.00n so that the the job's .out
        files are preserved across retries.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">&lt;job&gt;.err.00n</emphasis></para>

        <para>The stderr of the executable referred in the job submit file. In
        case of Pegasus, mostly the jobs are launched via kickstart. Hence,
        this file contains stderr of kickstart. This is usually empty unless
        there in an error in kickstart e.g. kickstart segfaults , or kickstart
        location specified in the submit file is incorrect. The exitpost
        executable is invoked on the <emphasis
        role="bold">&lt;job&gt;.out</emphasis> file and it moves the <emphasis
        role="bold"> &lt;job&gt;.err to &lt;job&gt;.err.00n</emphasis> so that
        the the job's <emphasis role="bold">.out</emphasis> files are
        preserved across retries.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">jobstate.log</emphasis></para>

        <para>The jobstate.log file is written out by the pegasus-monitord
        daemon that is launched when a workflow is submitted for execution by
        pegasus-run. The pegasus-monitord daemon parses the dagman.out file
        and writes out the jobstate.log that is easier to parse. The
        jobstate.log captures the various states through which a job goes
        during the workflow. There are other monitoring related files that are
        explained in the monitoring <link
        linkend="monitoring-files">chapter</link>.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">braindump.txt </emphasis></para>

        <para>Contains information about pegasus version, dax file, dag file,
        dax label.</para>
      </listitem>
    </orderedlist>
  </section>

  <section id="condor_dagman_file">
    <title>Condor DAGMan File</title>

    <para>The Condor DAGMan file ( .dag ) is the input to Condor DAGMan ( the
    workflow executor used by Pegasus ) .</para>

    <para>Pegasus generated .dag file usually contains the following
    information for each job:</para>

    <orderedlist>
      <listitem>
        <para>The job submit file for each job in the DAG.</para>
      </listitem>

      <listitem>
        <para>The post script that is to be invoked when a job completes. This
        is usually found in <emphasis
        role="bold">$PEGASUS_HOME/bin/exitpost</emphasis> and parses the
        kickstart record in the job's .out file and determines the
        exitcode.</para>
      </listitem>

      <listitem>
        <para>JOB RETRY - the number of times the job is to be retried in case
        of failure. In case of Pegasus, job postscript exits with a non zero
        exitcode if it determines a failure occurred.</para>
      </listitem>

      <listitem>
        <para>The pre script to be invoked before running a job. This is
        usually for the dax jobs in the DAX. The pre script is pegasus-plan
        invocation for the subdax.</para>
      </listitem>
    </orderedlist>

    <para>In the last section of the DAG file the relations between the jobs (
    that identify the underlying DAG structure ) are highlighted.</para>

    <section>
      <title>Sample Condor DAG File</title>

      <para><programlisting>#####################################################################
# PEGASUS WMS GENERATED DAG FILE
# DAG blackdiamond
# Index = 0, Count = 1
######################################################################

<emphasis role="bold">JOB</emphasis> create_dir_blackdiamond_0_isi_viz create_dir_blackdiamond_0_isi_viz.sub
<emphasis role="bold">SCRIPT POST</emphasis> create_dir_blackdiamond_0_isi_viz /pegasus/bin/pegasus-exitcode   \
                                   /submit-dir/create_dir_blackdiamond_0_isi_viz.out
<emphasis role="bold">RETRY</emphasis> create_dir_blackdiamond_0_isi_viz 3

JOB create_dir_blackdiamond_0_local create_dir_blackdiamond_0_local.sub
SCRIPT POST create_dir_blackdiamond_0_local /pegasus/bin/pegasus-exitcode   
                                   /submit-dir/create_dir_blackdiamond_0_local.out

JOB pegasus_concat_blackdiamond_0 pegasus_concat_blackdiamond_0.sub

JOB stage_in_local_isi_viz_0 stage_in_local_isi_viz_0.sub
SCRIPT POST stage_in_local_isi_viz_0 /pegasus/bin/pegasus-exitcode   \
                                     /submit-dir/stage_in_local_isi_viz_0.out

JOB chmod_preprocess_ID000001_0 chmod_preprocess_ID000001_0.sub
SCRIPT POST chmod_preprocess_ID000001_0 /pegasus/bin/pegasus-exitcode \
                                        /submit-dir/chmod_preprocess_ID000001_0.out

JOB preprocess_ID000001 preprocess_ID000001.sub
SCRIPT POST preprocess_ID000001 /pegasus/bin/pegasus-exitcode   \
                                         /submit-dir/preprocess_ID000001.out

JOB subdax_black_ID000002 subdax_black_ID000002.sub
<emphasis role="bold">SCRIPT PRE</emphasis> subdax_black_ID000002 /pegasus/bin/pegasus-plan  \
      -Dpegasus.user.properties=/submit-dir/./dag_1/test_ID000002/pegasus.3862379342822189446.properties\
      -Dpegasus.log.*=/submit-dir/subdax_black_ID000002.pre.log \
      -Dpegasus.dir.exec=app_domain/app -Dpegasus.dir.storage=duncan -Xmx1024 -Xms512\
      --dir /pegasus-features/dax-3.2/dags \
      --relative-dir user/pegasus/blackdiamond/run0005/user/pegasus/blackdiamond/run0005/./dag_1 \
      --relative-submit-dir user/pegasus/blackdiamond/run0005/./dag_1/test_ID000002\
      --basename black --sites dax_site \
      --output local --force  --nocleanup  \
      --verbose  --verbose  --verbose  --verbose  --verbose  --verbose  --verbose \
      --verbose  --monitor  --deferred  --group pegasus --rescue 0 \
      --dax /submit-dir/./dag_1/test_ID000002/dax/blackdiamond_dax.xml 

JOB stage_out_local_isi_viz_0_0 stage_out_local_isi_viz_0_0.sub
SCRIPT POST stage_out_local_isi_viz_0_0 /pegasus/bin/pegasus-exitcode   /submit-dir/stage_out_local_isi_viz_0_0.out

<emphasis role="bold">SUBDAG EXTERNAL</emphasis> subdag_black_ID000003 /Users/user/Pegasus/work/dax-3.2/black.dag DIR /duncan/test

JOB clean_up_stage_out_local_isi_viz_0_0 clean_up_stage_out_local_isi_viz_0_0.sub
SCRIPT POST clean_up_stage_out_local_isi_viz_0_0 /lfs1/devel/Pegasus/pegasus/bin/pegasus-exitcode  \
                                          /submit-dir/clean_up_stage_out_local_isi_viz_0_0.out

JOB clean_up_preprocess_ID000001 clean_up_preprocess_ID000001.sub
SCRIPT POST clean_up_preprocess_ID000001 /lfs1/devel/Pegasus/pegasus/bin/pegasus-exitcode  \
                                     /submit-dir/clean_up_preprocess_ID000001.out
<emphasis role="bold">
PARENT create_dir_blackdiamond_0_isi_viz CHILD pegasus_concat_blackdiamond_0</emphasis>
PARENT create_dir_blackdiamond_0_local CHILD pegasus_concat_blackdiamond_0
PARENT stage_out_local_isi_viz_0_0 CHILD clean_up_stage_out_local_isi_viz_0_0
PARENT stage_out_local_isi_viz_0_0 CHILD clean_up_preprocess_ID000001
PARENT preprocess_ID000001 CHILD subdax_black_ID000002
PARENT preprocess_ID000001 CHILD stage_out_local_isi_viz_0_0
PARENT subdax_black_ID000002 CHILD subdag_black_ID000003
PARENT stage_in_local_isi_viz_0 CHILD chmod_preprocess_ID000001_0
PARENT stage_in_local_isi_viz_0 CHILD preprocess_ID000001
PARENT chmod_preprocess_ID000001_0 CHILD preprocess_ID000001
PARENT pegasus_concat_blackdiamond_0 CHILD stage_in_local_isi_viz_0
######################################################################
# End of DAG
######################################################################
</programlisting></para>
    </section>
  </section>

  <section id="kickstart_xml_record">
    <title>Kickstart XML Record</title>

    <para>Kickstart is a light weight C executable that is shipped with the
    pegasus worker package. All jobs are launced via Kickstart on the remote
    end, unless explicitly disabled at the time of running
    pegasus-plan.</para>

    <para>Kickstart does not work with:</para>

    <orderedlist>
      <listitem>
        <para>Condor Standard Universe Jobs</para>
      </listitem>

      <listitem>
        <para>MPI Jobs</para>
      </listitem>
    </orderedlist>

    <para>Pegasus automatically disables kickstart for the above jobs.</para>

    <para>Kickstart captures useful runtime provenance information about the
    job launched by it on the remote note, and puts in an XML record that it
    writes to its own stdout. The stdout appears in the workflow submit
    directory as &lt;job&gt;.out.00n . The following information is captured
    by kickstart and logged:</para>

    <orderedlist>
      <listitem>
        <para>The exitcode with which the job it launched exited.</para>
      </listitem>

      <listitem>
        <para>The duration of the job</para>
      </listitem>

      <listitem>
        <para>The start time for the job</para>
      </listitem>

      <listitem>
        <para>The node on which the job ran</para>
      </listitem>

      <listitem>
        <para>The stdout and stderr of the job</para>
      </listitem>

      <listitem>
        <para>The arguments with which it launched the job</para>
      </listitem>

      <listitem>
        <para>The environment that was set for the job before it was
        launched.</para>
      </listitem>

      <listitem>
        <para>The machine information about the node that the job ran
        on</para>
      </listitem>
    </orderedlist>

    <para>Amongst the above information, the dagman.out file gives a coarser
    grained estimate of the job duration and start time.</para>

    <section>
      <title>Reading a Kickstart Output File</title>

      <para>The kickstart file below has the following fields
      highlighted:</para>

      <orderedlist>
        <listitem>
          <para>The host on which the job executed and the ipaddress of that
          host</para>
        </listitem>

        <listitem>
          <para>The duration and start time of the job. The time here is in
          reference to the clock on the remote node where the job is
          executed.</para>
        </listitem>

        <listitem>
          <para>The exitcode with which the job executed</para>
        </listitem>

        <listitem>
          <para>The arguments with which the job was launched.</para>
        </listitem>

        <listitem>
          <para>The directory in which the job executed on the remote
          site</para>
        </listitem>

        <listitem>
          <para>The stdout of the job</para>
        </listitem>

        <listitem>
          <para>The stderr of the job</para>
        </listitem>

        <listitem>
          <para>The environment of the job</para>
        </listitem>
      </orderedlist>

      <programlisting>&lt;?xml version="1.0" encoding="ISO-8859-1"?&gt;

&lt;invocation xmlns="http://pegasus.isi.edu/schema/invocation" \
      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" \
       xsi:schemaLocation="http://pegasus.isi.edu/schema/invocation http://pegasus.isi.edu/schema/iv-2.0.xsd" \
       version="2.0" start="2009-01-30T19:17:41.157-06:00" duration="0.321" transformation="pegasus::dirmanager"\
      derivation="pegasus::dirmanager:1.0" resource="cobalt" wf-label="scb" \
      wf-stamp="2009-01-30T17:12:55-08:00"<emphasis role="bold"> hostaddr="141.142.30.219" hostname="co-login.ncsa.uiuc.edu"</emphasis>\
      pid="27714" uid="29548" user="vahi" gid="13872" group="bvr" umask="0022"&gt;

<emphasis role="bold">&lt;mainjob start="2009-01-30T19:17:41.426-06:00" duration="0.052" pid="27783"&gt;
</emphasis>
&lt;usage utime="0.036" stime="0.004" minflt="739" majflt="0" nswap="0" nsignals="0" nvcsw="36" nivcsw="3"/&gt;

<emphasis role="bold">&lt;status raw="0"&gt;&lt;regular exitcode="0"/&gt;&lt;/status&gt;</emphasis>

&lt;statcall error="0"&gt;
&lt;!-- deferred flag: 0 --&gt;
&lt;file name="/u/ac/vahi/SOFTWARE/pegasus/default/bin/dirmanager"&gt;23212F7573722F62696E2F656E762070&lt;/file&gt;
&lt;statinfo mode="0100755" size="8202" inode="85904615883" nlink="1" blksize="16384" \
    blocks="24" mtime="2008-09-22T18:52:37-05:00" atime="2009-01-30T14:54:18-06:00" \
    ctime="2009-01-13T19:09:47-06:00" uid="29548" user="vahi" gid="13872" group="bvr"/&gt;
&lt;/statcall&gt;

<emphasis role="bold">&lt;argument-vector&gt;
&lt;arg nr="1"&gt;--create&lt;/arg&gt;
&lt;arg nr="2"&gt;--dir&lt;/arg&gt;
&lt;arg nr="3"&gt;/u/ac/vahi/globus-test/EXEC/vahi/pegasus/scb/run0001&lt;/arg&gt;
&lt;/argument-vector&gt;</emphasis>

&lt;/mainjob&gt;<emphasis role="bold">

&lt;cwd&gt;/u/ac/vahi/globus-test/EXEC&lt;/cwd&gt;</emphasis>

&lt;usage utime="0.012" stime="0.208" minflt="4232" majflt="0" nswap="0" nsignals="0" nvcsw="15" nivcsw="74"/&gt;
&lt;machine page-size="16384" provider="LINUX"&gt;
&lt;stamp&gt;2009-01-30T19:17:41.157-06:00&lt;/stamp&gt;
&lt;uname system="linux" nodename="co-login" release="2.6.16.54-0.2.5-default" machine="ia64"&gt;#1 SMP Mon Jan 21\
         13:29:51 UTC 2008&lt;/uname&gt;
&lt;ram total="148299268096" free="123371929600" shared="0" buffer="2801664"/&gt;
&lt;swap total="1179656486912" free="1179656486912"/&gt;
&lt;boot idle="1315786.920"&gt;2009-01-15T10:19:50.283-06:00&lt;/boot&gt;
&lt;cpu count="32" speed="1600" vendor=""&gt;&lt;/cpu&gt;
&lt;load min1="3.50" min5="3.50" min15="2.60"/&gt;
&lt;proc total="841" running="5" sleeping="828" stopped="5" vmsize="10025418752" rss="2524299264"/&gt;
&lt;task total="1125" running="6" sleeping="1114" stopped="5"/&gt;
&lt;/machine&gt;
&lt;statcall error="0" id="stdin"&gt;
&lt;!-- deferred flag: 0 --&gt;
&lt;file name="/dev/null"/&gt;
&lt;statinfo mode="020666" size="0" inode="68697" nlink="1" blksize="16384" blocks="0" \
     mtime="2007-05-04T05:54:02-05:00" atime="2007-05-04T05:54:02-05:00" \
   ctime="2009-01-15T10:21:54-06:00" uid="0" user="root" gid="0" group="root"/&gt;
&lt;/statcall&gt;

<emphasis role="bold">&lt;statcall error="0" id="stdout"&gt;
&lt;temporary name="/tmp/gs.out.s9rTJL" descriptor="3"/&gt;
&lt;statinfo mode="0100600" size="29" inode="203420686" nlink="1" blksize="16384" blocks="128" \
 mtime="2009-01-30T19:17:41-06:00" atime="2009-01-30T19:17:41-06:00"\
 ctime="2009-01-30T19:17:41-06:00" uid="29548" user="vahi" gid="13872" group="bvr"/&gt;
&lt;data&gt;mkdir finished successfully.
&lt;/data&gt;
&lt;/statcall&gt;
&lt;statcall error="0" id="stderr"&gt;
&lt;temporary name="/tmp/gs.err.kobn3S" descriptor="5"/&gt;
&lt;statinfo mode="0100600" size="0" inode="203420689" nlink="1" blksize="16384" blocks="0" \
 mtime="2009-01-30T19:17:41-06:00" atime="2009-01-30T19:17:41-06:00" \
ctime="2009-01-30T19:17:41-06:00" uid="29548" user="vahi" gid="13872" group="bvr"/&gt;
&lt;/statcall&gt;
</emphasis>
&lt;statcall error="0" id="gridstart"&gt;
&lt;!-- deferred flag: 0 --&gt;
&lt;file name="/u/ac/vahi/SOFTWARE/pegasus/default/bin/kickstart"&gt;7F454C46020101000000000000000000&lt;/file&gt;
&lt;statinfo mode="0100755" size="255445" inode="85904615876" nlink="1" blksize="16384" blocks="504" \
  mtime="2009-01-30T18:06:28-06:00" atime="2009-01-30T19:17:41-06:00"\
 ctime="2009-01-30T18:06:28-06:00" uid="29548" user="vahi" gid="13872" group="bvr"/&gt;
&lt;/statcall&gt;
&lt;statcall error="0" id="logfile"&gt;
&lt;descriptor number="1"/&gt;
&lt;statinfo mode="0100600" size="0" inode="53040253" nlink="1" blksize="16384" blocks="0" \
 mtime="2009-01-30T19:17:39-06:00" atime="2009-01-30T19:17:39-06:00" \
ctime="2009-01-30T19:17:39-06:00" uid="29548" user="vahi" gid="13872" group="bvr"/&gt;
&lt;/statcall&gt;
&lt;statcall error="0" id="channel"&gt;
&lt;fifo name="/tmp/gs.app.Ien1m0" descriptor="7" count="0" rsize="0" wsize="0"/&gt;
&lt;statinfo mode="010640" size="0" inode="203420696" nlink="1" blksize="16384" blocks="0" \
  mtime="2009-01-30T19:17:41-06:00" atime="2009-01-30T19:17:41-06:00" \
ctime="2009-01-30T19:17:41-06:00" uid="29548" user="vahi" gid="13872" group="bvr"/&gt;
&lt;/statcall&gt;

<emphasis role="bold">&lt;environment&gt;
&lt;env key="GLOBUS_GRAM_JOB_CONTACT"&gt;https://co-login.ncsa.uiuc.edu:50001/27456/1233364659/&lt;/env&gt;
&lt;env key="GLOBUS_GRAM_MYJOB_CONTACT"&gt;URLx-nexus://co-login.ncsa.uiuc.edu:50002/&lt;/env&gt;
&lt;env key="GLOBUS_LOCATION"&gt;/usr/local/prews-gram-4.0.7-r1/&lt;/env&gt;
....
&lt;/environment&gt;
</emphasis>
&lt;resource&gt;
&lt;soft id="RLIMIT_CPU"&gt;unlimited&lt;/soft&gt;
&lt;hard id="RLIMIT_CPU"&gt;unlimited&lt;/hard&gt;
&lt;soft id="RLIMIT_FSIZE"&gt;unlimited&lt;/soft&gt;
....
&lt;/resource&gt;
&lt;/invocation&gt;</programlisting>
    </section>
  </section>

  <section id="jobstate_log_file">
    <title>Jobstate.Log File</title>

    <para>The jobstate.log file logs the various states that a job goes
    through during workflow execution. It is created by the <emphasis
    role="bold">pegasus-monitord</emphasis> daemon that is launched when a
    workflow is submitted to Condor DAGMan by pegasus-run. <emphasis
    role="bold">pegasus-monitord</emphasis> parses the dagman.out file and
    writes out the jobstate.log file, the format of which is more amenable to
    parsing.</para>

    <note>
      <para>The jobstate.log file is not created if a user uses
      condor_submit_dag to submit a workflow to Condor DAGMan.</para>
    </note>

    <para>The jobstate.log file can be created after a workflow has finished
    executing by running <emphasis role="bold">pegasus-monitord</emphasis> on
    the .dagman.out file in the workflow submit directory.</para>

    <para>Below is a snippet from the jobstate.log for a single job executed
    via condorg:</para>

    <programlisting>1239666049 create_dir_blackdiamond_0_isi_viz SUBMIT 3758.0 isi_viz - 1
1239666059 create_dir_blackdiamond_0_isi_viz EXECUTE 3758.0 isi_viz - 1
1239666059 create_dir_blackdiamond_0_isi_viz GLOBUS_SUBMIT 3758.0 isi_viz - 1
1239666059 create_dir_blackdiamond_0_isi_viz GRID_SUBMIT 3758.0 isi_viz - 1
1239666064 create_dir_blackdiamond_0_isi_viz JOB_TERMINATED 3758.0 isi_viz - 1
1239666064 create_dir_blackdiamond_0_isi_viz JOB_SUCCESS 0 isi_viz - 1
1239666064 create_dir_blackdiamond_0_isi_viz POST_SCRIPT_STARTED - isi_viz - 1
1239666069 create_dir_blackdiamond_0_isi_viz POST_SCRIPT_TERMINATED 3758.0 isi_viz - 1
1239666069 create_dir_blackdiamond_0_isi_viz POST_SCRIPT_SUCCESS - isi_viz - 1</programlisting>

    <para>Each entry in jobstate.log has the following:</para>

    <orderedlist>
      <listitem>
        <para>The ISO timestamp for the time at which the particular event
        happened.</para>
      </listitem>

      <listitem>
        <para>The name of the job.</para>
      </listitem>

      <listitem>
        <para>The event recorded by DAGMan for the job.</para>
      </listitem>

      <listitem>
        <para>The condor id of the job in the queue on the submit node.</para>
      </listitem>

      <listitem>
        <para>The pegasus site to which the job is mapped.</para>
      </listitem>

      <listitem>
        <para>The job time requirements from the submit file.</para>
      </listitem>

      <listitem>
        <para>The job submit sequence for this workflow.</para>
      </listitem>
    </orderedlist>

    <table>
      <title>The job lifecycle when executed as part of the
      workflow</title>

      <tgroup cols="2">
        <tbody>
          <row>
            <entry><emphasis role="bold">STATE/EVENT</emphasis></entry>

            <entry><emphasis role="bold">DESCRIPTION</emphasis></entry>
          </row>

          <row>
            <entry>SUBMIT</entry>

            <entry>job is submitted by condor schedd for execution.</entry>
          </row>

          <row>
            <entry>EXECUTE</entry>

            <entry>condor schedd detects that a job has started
            execution.</entry>
          </row>

          <row>
            <entry>GLOBUS_SUBMIT</entry>

            <entry>the job has been submitted to the remote resource. It's
            only written for GRAM jobs (i.e. gt2 and gt4).</entry>
          </row>

          <row>
            <entry>GRID_SUBMIT</entry>

            <entry>same as GLOBUS_SUBMIT event. The ULOG_GRID_SUBMIT event is
            written for all grid universe jobs./</entry>
          </row>

          <row>
            <entry>JOB_TERMINATED</entry>

            <entry>job terminated on the remote node.</entry>
          </row>

          <row>
            <entry>JOB_SUCCESS</entry>

            <entry>job succeeded on the remote host, condor id will be zero
            (successful exit code).</entry>
          </row>

          <row>
            <entry>JOB_FAILURE</entry>

            <entry>job failed on the remote host, condor id will be the job's
            exit code.</entry>
          </row>

          <row>
            <entry>POST_SCRIPT_STARTED</entry>

            <entry>post script started by DAGMan on the submit host, usually
            to parse the kickstart output</entry>
          </row>

          <row>
            <entry>POST_SCRIPT_TERMINATED</entry>

            <entry>post script finished on the submit node.</entry>
          </row>

          <row>
            <entry>POST_SCRIPT_SUCCESS | POST_SCRIPT_FAILURE</entry>

            <entry>post script succeeded or failed.</entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <para>There are other monitoring related files that are explained in the
    monitoring <link linkend="monitoring-files">chapter</link>.</para>

    <section id="submit_directory-delays">
      <title>Pegasus Workflow Job States and Delays</title>

      <para>The various job states that a job goes through ( as caputured in
      the dagman.out and jobstate.log file) during it's lifecycle are
      illustrated below. The figure below highlights the various local and
      remote delays during job lifecycle.</para>

      <mediaobject>
        <imageobject>
          <imagedata contentdepth="100%"
                     fileref="images/Pegasus_Job_State_Delay.jpg" scalefit="1"
                     width="100%"/>
        </imageobject>
      </mediaobject>
    </section>
  </section>

  <section id="braindump_file">
    <title>Braindump File</title>

    <para>The braindump file is created per workflow in the submit file and
    contains metadata about the workflow.</para>

    <table>
      <title>Information Captured in Braindump File</title>

      <tgroup cols="2">
        <tbody>
          <row>
            <entry><emphasis role="bold">KEY</emphasis></entry>

            <entry><emphasis role="bold">DESCRIPTION</emphasis></entry>
          </row>

          <row>
            <entry>user</entry>

            <entry>the username of the user that ran pegasus-plan</entry>
          </row>

          <row>
            <entry>grid_dn</entry>

            <entry>the Distinguished Name in the proxy</entry>
          </row>

          <row>
            <entry>submit_hostname</entry>

            <entry>the hostname of the submit host</entry>
          </row>

          <row>
            <entry>root_wf_uuid</entry>

            <entry>the workflow uuid of the root workflow</entry>
          </row>

          <row>
            <entry>wf_uuid</entry>

            <entry>the workflow uuid of the current workflow i.e the one whose
            submit directory the braindump file is.</entry>
          </row>

          <row>
            <entry>dax</entry>

            <entry>the path to the dax file</entry>
          </row>

          <row>
            <entry>dax_label</entry>

            <entry>the label attribute in the adag element of the dax</entry>
          </row>

          <row>
            <entry>dax_index</entry>

            <entry>the index in the dax.</entry>
          </row>

          <row>
            <entry>dax_version</entry>

            <entry>the version of the DAX schema that DAX referred to.</entry>
          </row>

          <row>
            <entry>pegasus_wf_name</entry>

            <entry>the workflow name constructed by pegasus when
            planning</entry>
          </row>

          <row>
            <entry>timestamp</entry>

            <entry>the timestamp when planning occured</entry>
          </row>

          <row>
            <entry>basedir</entry>

            <entry>the base submit directory</entry>
          </row>

          <row>
            <entry>submit_dir</entry>

            <entry>the full path for the submit directory</entry>
          </row>

          <row>
            <entry>properties</entry>

            <entry>the full path to the properties file in the submit
            directory</entry>
          </row>

          <row>
            <entry>planner</entry>

            <entry>the planner used to construct the executable workflow.
            always pegasus</entry>
          </row>

          <row>
            <entry>planner_version</entry>

            <entry>the versions of the planner</entry>
          </row>

          <row>
            <entry>pegasus_build</entry>

            <entry>the build timestamp</entry>
          </row>

          <row>
            <entry>planner_arguments</entry>

            <entry>the arguments with which the planner is invoked.</entry>
          </row>

          <row>
            <entry>jsd</entry>

            <entry>the path to the jobstate file</entry>
          </row>

          <row>
            <entry>rundir</entry>

            <entry>the rundir in the numbering scheme for the submit
            directories</entry>
          </row>

          <row>
            <entry>pegasushome</entry>

            <entry>the root directory of the pegasus installation</entry>
          </row>

          <row>
            <entry>vogroup</entry>

            <entry>the vo group to which the user belongs to. Defaults to
            pegasus</entry>
          </row>

          <row>
            <entry>condor_log</entry>

            <entry>the full path to condor common log in the submit
            directory</entry>
          </row>

          <row>
            <entry>notify</entry>

            <entry>the notify file that contains any notifications that need
            to be sent for the workflow.</entry>
          </row>

          <row>
            <entry>dag</entry>

            <entry>the basename of the dag file created</entry>
          </row>

          <row>
            <entry>type</entry>

            <entry>the type of executable workflow. Can be dag | shell</entry>
          </row>
        </tbody>
      </tgroup>
    </table>

    <para>A Sample Braindump File is displayed below:</para>

    <programlisting>user vahi
grid_dn null
submit_hostname obelix
root_wf_uuid a4045eb6-317a-4710-9a73-96a745cb1fe8
wf_uuid a4045eb6-317a-4710-9a73-96a745cb1fe8
dax /data/scratch/vahi/examples/synthetic-scec/Test.dax
dax_label Stampede-Test
dax_index 0
dax_version 3.3
pegasus_wf_name Stampede-Test-0
timestamp 20110726T153746-0700
basedir /data/scratch/vahi/examples/synthetic-scec/dags
submit_dir /data/scratch/vahi/examples/synthetic-scec/dags/vahi/pegasus/Stampede-Test/run0005
properties pegasus.6923599674234553065.properties
planner /data/scratch/vahi/software/install/pegasus/default/bin/pegasus-plan
planner_version 3.1.0cvs
pegasus_build 20110726221240Z
planner_arguments "--conf ./conf/properties --dax Test.dax --sites local --output local --dir dags --force --submit "
jsd jobstate.log
rundir run0005
pegasushome /data/scratch/vahi/software/install/pegasus/default
vogroup pegasus
condor_log Stampede-Test-0.log
notify Stampede-Test-0.notify
dag Stampede-Test-0.dag
type dag
</programlisting>
  </section>

  <section id="static_bp_file">
    <title>Pegasus static.bp File</title>

    <para>Pegasus creates a workflow.static.bp file that links jobs in the DAG
    with the jobs in the DAX. The contents of the file are in netlogger
    format. The purpose of this file is to be able to link an invocation
    record of a task to the corresponding job in the DAX</para>

    <para>The workflow is replaced by the name of the workflow i.e. same
    prefix as the .dag file</para>

    <para>In the file there are five types of events:</para>

    <itemizedlist>
      <listitem>
        <para>task.info</para>

        <para>This event is used to capture information about all the tasks in
        the DAX( abstract workflow)</para>
      </listitem>

      <listitem>
        <para>task.edge</para>

        <para>This event is used to capture information about the edges
        between the tasks in the DAX ( abstract workflow )</para>
      </listitem>

      <listitem>
        <para>job.info</para>

        <para>This event is used to capture information about the jobs in the
        DAG ( executable workflow generated by Pegasus )</para>
      </listitem>

      <listitem>
        <para>job.edge</para>

        <para>This event is used to capture information about edges between
        the jobs in the DAG ( executable workflow ).</para>
      </listitem>

      <listitem>
        <para>wf.map.task_job</para>

        <para>This event is used to associate the tasks in the DAX with the
        corresponding jobs in the DAG.</para>
      </listitem>
    </itemizedlist>
  </section>
</chapter>
