<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="data_management">
  <title>Data Management</title>

  <section id="replica_selection">
    <title>Replica Selection</title>

    <para>Each job in the DAX maybe associated with input LFN&amp;rsquor;s
    denoting the files that are required for the job to run. To determine the
    physical replica (PFN) for a LFN, Pegasus queries the Replica catalog to
    get all the PFN&amp;rsquor;s (replicas) associated with a LFN. The Replica
    Catalog may return multiple PFN's for each of the LFN's queried. Hence,
    Pegasus needs to select a single PFN amongst the various PFN's returned
    for each LFN. This process is known as replica selection in Pegasus. Users
    can specify the replica selector to use in the properties file.</para>

    <para>This document describes the various Replica Selection Strategies in
    Pegasus.</para>

    <section>
      <title>Configuration</title>

      <para>The user properties determine what replica selector Pegasus
      Workflow Mapper uses. The property <emphasis
      role="bold">pegasus.selector.replica</emphasis> is used to specify the
      replica selection strategy. Currently supported Replica Selection
      strategies are</para>

      <orderedlist>
        <listitem>
          <para>Default</para>
        </listitem>

        <listitem>
          <para>Restricted</para>
        </listitem>

        <listitem>
          <para>Regex</para>
        </listitem>
      </orderedlist>

      <para>The values are case sensitive. For example the following property
      setting will throw a Factory Exception .</para>

      <programlisting>pegasus.selector.replica  default</programlisting>

      <para>The correct way to specify is</para>

      <programlisting>pegasus.selector.replica  Default</programlisting>
    </section>

    <section>
      <title>Supported Replica Selectors</title>

      <para>The various Replica Selectors supported in Pegasus Workflow Mapper
      are explained below</para>

      <section>
        <title>Default</title>

        <para>This is the default replica selector used in the Pegasus
        Workflow Mapper. If the property pegasus.selector.replica is not
        defined in properties, then Pegasus uses this selector.</para>

        <para>This selector looks at each PFN returned for a LFN and checks to
        see if</para>

        <orderedlist>
          <listitem>
            <para>the PFN is a file URL (starting with file:///)</para>
          </listitem>

          <listitem>
            <para>the PFN has a pool attribute matching to the site handle of
            the site where the compute job that requires the input file is to
            be run.</para>
          </listitem>
        </orderedlist>

        <para>If a PFN matching the conditions above exists then that is
        returned by the selector .</para>

        <para><emphasis role="bold">Else,</emphasis> a random PFN is selected
        amongst all the PFN&amp;rsquor;s that have a pool attribute matching
        to the site handle of the site where a compute job is to be
        run.</para>

        <para><emphasis role="bold">Else,</emphasis> a random pfn is selected
        amongst all the PFN&amp;rsquor;s</para>

        <para>To use this replica selector set the following
        property<programlisting>pegasus.selector.replica                  Default</programlisting></para>
      </section>

      <section>
        <title>Restricted</title>

        <para>This replica selector, allows the user to specify good sites and
        bad sites for staging in data to a particular compute site. A good
        site for a compute site X, is a preferred site from which replicas
        should be staged to site X. If there are more than one good sites
        having a particular replica, then a random site is selected amongst
        these preferred sites.</para>

        <para>A bad site for a compute site X, is a site from which
        replica&amp;rsquor;s should not be staged. The reason of not accessing
        replica from a bad site can vary from the link being down, to the user
        not having permissions on that site&amp;rsquor;s data.</para>

        <para>The good | bad sites are specified by the following
        properties</para>

        <programlisting>pegasus.replica.*.prefer.stagein.sites
pegasus.replica.*.ignore.stagein.sites</programlisting>

        <para>where the * in the property name denotes the name of the compute
        site. A * in the property key is taken to mean all sites. The value to
        these properties is a comma separated list of sites.</para>

        <para>For example the following settings</para>

        <programlisting>pegasus.selector.replica.*.prefer.stagein.sites            usc
pegasus.replica.uwm.prefer.stagein.sites                   isi,cit
</programlisting>

        <para>means that prefer all replicas from site usc for staging in to
        any compute site. However, for uwm use a tighter constraint and prefer
        only replicas from site isi or cit. The pool attribute associated with
        the PFN's tells the replica selector to what site a replica/PFN is
        associated with.</para>

        <para>The pegasus.replica.*.prefer.stagein.sites property takes
        precedence over pegasus.replica.*.ignore.stagein.sites property i.e.
        if for a site X, a site Y is specified both in the ignored and the
        preferred set, then site Y is taken to mean as only a preferred site
        for a site X.</para>

        <para>To use this replica selector set the following property</para>

        <programlisting>pegasus.selector.replica                  Restricted</programlisting>
      </section>

      <section>
        <title>Regex</title>

        <para>This replica selector allows the user allows the user to
        specific regex expressions that can be used to rank various
        PFN&amp;rsquor;s returned from the Replica Catalog for a particular
        LFN. This replica selector selects the highest ranked PFN i.e the
        replica with the lowest rank value.</para>

        <para>The regular expressions are assigned different rank, that
        determine the order in which the expressions are employed. The rank
        values for the regex can expressed in user properties using the
        property.</para>

        <programlisting>pegasus.selector.replica.regex.rank.<emphasis
            role="bold">[value]</emphasis>                  regex-expression</programlisting>

        <para>The <emphasis role="bold">[value]</emphasis> in the above
        property is an integer value that denotes the rank of an expression
        with a rank value of 1 being the highest rank.</para>

        <para>For example, a user can specify the following regex expressions
        that will ask Pegasus to prefer file URL's over gsiftp url's from
        example.isi.edu</para>

        <programlisting>pegasus.selector.replica.regex.rank.1                       file://.*
pegasus.selector.replica.regex.rank.2                       gsiftp://example\.isi\.edu.*</programlisting>

        <para>User can specify as many regex expressions as they want.</para>

        <para>Since Pegasus is in Java , the regex expression support is what
        Java supports. It is pretty close to what is supported by Perl. More
        details can be found at
        http://java.sun.com/j2se/1.5.0/docs/api/java/util/regex/Pattern.html</para>

        <para>Before applying any regular expressions on the PFN&amp;rsquor;s
        for a particular LFN that has to be staged to a site X, the file
        URL&amp;rsquor;s that don't match the site X are explicitly filtered
        out.</para>

        <para>To use this replica selector set the following
        property<programlisting>pegasus.selector.replica                  Regex</programlisting></para>
      </section>

      <section>
        <title>Local</title>

        <para>This replica selector always prefers replicas from the local
        host ( pool attribute set to local ) and that start with a file: URL
        scheme. It is useful, when users want to stagein files to a remote
        site from the submit host using the Condor file transfer
        mechanism.</para>

        <para>To use this replica selector set the following
        property<programlisting>pegasus.selector.replica                  Default</programlisting></para>
      </section>
    </section>
  </section>

  <section id="transfer">
    <title>Data Transfers</title>

    <para>As part of the Workflow Mapping Process, Pegasus does data
    management for the executable workflow . It queries a Replica Catalog to
    discover the locations of the input datasets and adds data movement and
    registration nodes in the workflow to</para>

    <orderedlist>
      <listitem>
        <para>stage-in input data to the staging sites ( a site associated
        with the compute job to be used for staging. In the shared filesystem
        setup, staging site is the same as the execution sites where the jobs
        in the workflow are executed )</para>
      </listitem>

      <listitem>
        <para>stage-out output data generated by the workflow to the final
        storage site.</para>
      </listitem>

      <listitem>
        <para>stage-in intermediate data between compute sites if
        required.</para>
      </listitem>

      <listitem>
        <para>data registration nodes to catalog the locations of the output
        data on the final storage site into the replica catalog.</para>
      </listitem>
    </orderedlist>

    <para>The separate data movement jobs that are added to the executable
    workflow are responsible for staging data to a workflow specific directory
    accessible to the staging server on a staging site associated with the
    compute sites. Depending on the data staging configuration, the staging
    site for a compute site is the compute site itself. In the default case,
    the staging server is usually on the headnode of the compute site and has
    access to the shared filesystem between the worker nodes and the head
    node. Pegasus adds a directory creation job in the executable workflow
    that creates the workflow specific directory on the staging server.</para>

    <para>In addition to data, Pegasus does transfer user executables to the
    compute sites if the executables are not installed on the remote sites
    before hand. This chapter gives an overview of how transfers of data and
    executables is managed in Pegasus.</para>

    <section id="ref_data_staging_configuration">
      <title>Data Staging Configuration</title>

      <para>Pegasus can be broadly setup to run workflows in the following
      configurations</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">Shared File System</emphasis></para>

          <para>This setup applies to where the head node and the worker nodes
          of a cluster share a filesystem. Compute jobs in the workflow run in
          a directory on the shared filesystem.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">NonShared FileSystem</emphasis></para>

          <para>This setup applies to where the head node and the worker nodes
          of a cluster don't share a filesystem. Compute jobs in the workflow
          run in a local directory on the worker node</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Condor Pool Without a shared
          filesystem</emphasis></para>

          <para>This setup applies to a condor pool where the worker nodes
          making up a condor pool don't share a filesystem. All data IO is
          achieved using Condor File IO. This is a special case of the non
          shared filesystem setup, where instead of using pegasus-transfer to
          transfer input and output data, Condor File IO is used.</para>
        </listitem>
      </itemizedlist>

      <para>For the purposes of data configuration various sites, and
      directories are defined below.</para>

      <orderedlist>
        <listitem>
          <para><emphasis role="bold">Submit Host</emphasis></para>

          <para>The host from where the workflows are submitted . This is
          where Pegasus and Condor DAGMan are installed. This is referred to
          as the <emphasis role="bold">"local"</emphasis> site in the site
          catalog .</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Compute Site</emphasis></para>

          <para>The site where the jobs mentioned in the DAX are executed.
          There needs to be an entry in the Site Catalog for every compute
          site. The compute site is passed to pegasus-plan using <emphasis
          role="bold">--sites</emphasis> option</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Staging Site</emphasis></para>

          <para>A site to which the separate transfer jobs in the executable
          workflow ( jobs with stage_in , stage_out and stage_inter prefixes
          that Pegasus adds using the transfer refiners) stage the input data
          to and the output data from to transfer to the final output site.
          Currently, the staging site is always the compute site where the
          jobs execute.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Output Site</emphasis></para>

          <para>The output site is the final storage site where the users want
          the output data from jobs to go to. The output site is passed to
          pegasus-plan using the <emphasis role="bold">--output</emphasis>
          option. The stageout jobs in the workflow stage the data from the
          staging site to the final storage site.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Input Site</emphasis></para>

          <para>The site where the input data is stored. The locations of the
          input data are catalogued in the Replica Catalog, and the
          <emphasis>"site"</emphasis> attribute of the locations gives us the
          site handle for the input site.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Workflow Execution
          Directory</emphasis></para>

          <para>This is the directory created by the create dir jobs in the
          executable workflow on the Staging Site. This is a directory per
          workflow per staging site. Currently, the Staging site is always the
          Compute Site.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Worker Node Directory</emphasis></para>

          <para>This is the directory created on the worker nodes per job
          usually by the job wrapper that launches the job.</para>
        </listitem>
      </orderedlist>

      <section>
        <title>Shared File System</title>

        <para>By default Pegasus is setup to run workflows in the shared file
        system setup, where the worker nodes and the head node of a cluster
        share a filesystem.</para>

        <figure>
          <title>Shared File System Setup</title>

          <mediaobject>
            <imageobject role="html">
              <imagedata align="center" contentdepth="450px"
                         fileref="images/data-configuration-sharedfs.png"/>
            </imageobject>

            <imageobject role="fo">
              <imagedata align="center" contentdepth="4in"
                         fileref="images/data-configuration-sharedfs.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>The data flow is as follows in this case</para>

        <orderedlist>
          <listitem>
            <para>Stagein Job executes ( either on Submit Host or Head Node )
            to stage in input data from Input Sites ( 1---n) to a workflow
            specific execution directory on the shared filesystem.</para>
          </listitem>

          <listitem>
            <para>Compute Job starts on a worker node in the workflow
            execution directory. Accesses the input data using Posix IO</para>
          </listitem>

          <listitem>
            <para>Compute Job executes on the worker node and writes out
            output data to workflow execution directory using Posix IO</para>
          </listitem>

          <listitem>
            <para>Stageout Job executes ( either on Submit Host or Head Node )
            to stage out output data from the workflow specific execution
            directory to a directory on the final output site.</para>
          </listitem>
        </orderedlist>

        <tip>
          <para>Set <emphasis role="bold">
          pegasus.data.configuration</emphasis> to <emphasis role="bold">
          sharedfs</emphasis> to run in this configuration.</para>
        </tip>
      </section>

      <section>
        <title>Non Shared Filesystem</title>

        <para>In this setup , Pegasus runs workflows on local file-systems of
        worker nodes with the the worker nodes not sharing a filesystem. The
        data transfers happen between the worker node and a staging / data
        coordination site. The staging site server can be a file server on the
        head node of a cluster or can be on a separate machine.</para>

        <para><emphasis role="bold">Setup</emphasis> <itemizedlist>
            <listitem>
              <para>compute and staging site are the different</para>
            </listitem>

            <listitem>
              <para>head node and worker nodes of compute site don't share a
              filesystem</para>
            </listitem>

            <listitem>
              <para>Input Data is staged from remote sites.</para>
            </listitem>

            <listitem>
              <para>Remote Output Site i.e site other than compute site. Can
              be submit host.</para>
            </listitem>
          </itemizedlist></para>

        <figure>
          <title>Non Shared Filesystem Setup</title>

          <mediaobject id="Figure2">
            <imageobject role="html">
              <imagedata align="center" contentdepth="450px"
                         fileref="images/data-configuration-nonsharedfs.png"/>
            </imageobject>

            <imageobject role="fo">
              <imagedata align="center" contentdepth="4in"
                         fileref="images/data-configuration-nonsharedfs.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>The data flow is as follows in this case</para>

        <orderedlist>
          <listitem>
            <para>Stagein Job executes ( either on Submit Host or on staging
            site ) to stage in input data from Input Sites ( 1---n) to a
            workflow specific execution directory on the staging site.</para>
          </listitem>

          <listitem>
            <para>Compute Job starts on a worker node in a local execution
            directory. Accesses the input data using pegasus transfer to
            transfer the data from the staging site to a local directory on
            the worker node</para>
          </listitem>

          <listitem>
            <para>The compute job executes in the worker node, and executes on
            the worker node.</para>
          </listitem>

          <listitem>
            <para>The compute Job writes out output data to the local
            directory on the worker node using Posix IO</para>
          </listitem>

          <listitem>
            <para>Output Data is pushed out to the staging site from the
            worker node using pegasus-transfer.</para>
          </listitem>

          <listitem>
            <para>Stageout Job executes ( either on Submit Host or staging
            site ) to stage out output data from the workflow specific
            execution directory to a directory on the final output
            site.</para>
          </listitem>
        </orderedlist>

        <para>In this case, the compute jobs are wrapped as <link
        linkend="pegasuslite">PegasusLite</link> instances.</para>

        <para>This mode is especially useful for running in the cloud
        environments where you don't want to setup a shared filesystem between
        the worker nodes. Running in that mode is explained in detail <link
        linkend="amazon_aws">here.</link></para>

        <tip>
          <para>Set p <emphasis
          role="bold">egasus.data.configuration</emphasis> to <emphasis
          role="bold">nonsharedfs</emphasis> to run in this configuration. The
          staging site can be specified using the <emphasis
          role="bold">--staging-site</emphasis> option to pegasus-plan.</para>
        </tip>

        <para>In this setup, Pegasus always stages the input files through the
        staging site i.e the stage-in job stages in data from the input site
        to the staging site. The PegasusLite jobs that start up on the worker
        nodes, then pull the input data from the staging site for each job. In
        some cases, it might be useful to setup the PegasusLite jobs to pull
        input data directly from the input site without going through the
        staging server. This is based on the assumption that the worker nodes
        can access the input site. Starting 4.3 release, users can enable
        this. However, you should be aware that the access to the input site
        is no longer throttled ( as in case of stage in jobs). If large number
        of compute jobs start at the same time in a workflow, the input server
        will see a connection from each job.</para>

        <tip>
          <para>Set <emphasis role="bold">
          pegasus.transfer.bypass.input.staging</emphasis> to <emphasis
          role="bold">true</emphasis>to enable the bypass of staging of input
          files via the staging server.</para>
        </tip>
      </section>

      <section>
        <title>Condor Pool Without a Shared Filesystem</title>

        <para>This setup applies to a condor pool where the worker nodes
        making up a condor pool don't share a filesystem. All data IO is
        achieved using Condor File IO. This is a special case of the non
        shared filesystem setup, where instead of using pegasus-transfer to
        transfer input and output data, Condor File IO is used.</para>

        <para><emphasis role="bold">Setup</emphasis> <itemizedlist>
            <listitem>
              <para>Submit Host and staging site are same</para>
            </listitem>

            <listitem>
              <para>head node and worker nodes of compute site don't share a
              filesystem</para>
            </listitem>

            <listitem>
              <para>Input Data is staged from remote sites.</para>
            </listitem>

            <listitem>
              <para>Remote Output Site i.e site other than compute site. Can
              be submit host.</para>
            </listitem>
          </itemizedlist></para>

        <figure>
          <title>Condor Pool Without a Shared Filesystem</title>

          <mediaobject id="Figure13">
            <imageobject role="html">
              <imagedata align="center" contentdepth="450px"
                         fileref="images/data-configuration-condorio.png"/>
            </imageobject>

            <imageobject role="fo">
              <imagedata align="center" contentdepth="4in"
                         fileref="images/data-configuration-condorio.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>The data flow is as follows in this case</para>

        <orderedlist>
          <listitem>
            <para>Stagein Job executeson the submit host to stage in input
            data from Input Sites ( 1---n) to a workflow specific execution
            directory on the submit host</para>
          </listitem>

          <listitem>
            <para>Compute Job starts on a worker node in a local execution
            directory. Before the compute job starts, Condor transfers the
            input data for the job from the workflow execution directory on
            thesubmit host to the local execution directory on the worker
            node.</para>
          </listitem>

          <listitem>
            <para>The compute job executes in the worker node, and executes on
            the worker node.</para>
          </listitem>

          <listitem>
            <para>The compute Job writes out output data to the local
            directory on the worker node using Posix IO</para>
          </listitem>

          <listitem>
            <para>When the compute job finishes, Condor transfers the output
            data for the job from the local execution directory on the worker
            node to the workflow execution directory on the submit
            host.</para>
          </listitem>

          <listitem>
            <para>Stageout Job executes ( either on Submit Host or staging
            site ) to stage out output data from the workflow specific
            execution directory to a directory on the final output
            site.</para>
          </listitem>
        </orderedlist>

        <para>In this case, the compute jobs are wrapped as <link
        linkend="pegasuslite">PegasusLite</link> instances.</para>

        <para>This mode is especially useful for running in the cloud
        environments where you don't want to setup a shared filesystem between
        the worker nodes. Running in that mode is explained in detail <link
        linkend="amazon_aws">here.</link></para>

        <tip>
          <para>Set p <emphasis
          role="bold">egasus.data.configuration</emphasis> to <emphasis
          role="bold">condorio</emphasis> to run in this configuration. In
          this mode, the staging site is automatically set to site <emphasis
          role="bold">local</emphasis></para>
        </tip>

        <para>In this setup, Pegasus always stages the input files through the
        submit host i.e the stage-in job stages in data from the input site to
        the submit host (local site). The input data is then transferred to
        remote worker nodes from the submit host using Condor file transfers.
        In the case, where the input data is locally accessible at the submit
        host i.e the input site and the submit host are the same, then it is
        possible to bypass the creation of separate stage in jobs that copy
        the data to the workflow specific directory on the submit host.
        Instead, Condor file transfers can be setup to transfer the input
        files directly from the locally accessible input locations ( file
        URL's with "<emphasis>site</emphasis>" attribute set to local)
        specified in the replica catalog. Starting 4.3 release, users can
        enable this.</para>

        <tip>
          <para>Set <emphasis role="bold">
          pegasus.transfer.bypass.input.staging</emphasis> to <emphasis
          role="bold">true</emphasis>to bypass the creation of separate stage
          in jobs.</para>
        </tip>
      </section>
    </section>

    <section id="local_vs_remote_transfers">
      <title>Local versus Remote Transfers</title>

      <para>As far as possible, Pegasus will ensure that the transfer jobs
      added to the executable workflow are executed on the submit host. By
      default, Pegasus will schedule a transfer to be executed on the remote
      staging site only if there is no way to execute it on the submit host.
      For e.g if the file server specified for the staging site/compute site
      is a file server, then Pegasus will schedule all the stage in data
      movement jobs on the compute site to stage-in the input data for the
      workflow. Another case would be if a user has symlinking turned on. In
      that case, the transfer jobs that symlink against the input data on the
      compute site, will be executed remotely ( on the compute site ).</para>

      <para>Users can specify the property <emphasis role="bold">
      pegasus.transfer.*.remote.sites</emphasis> to change the default
      behaviour of Pegasus and force pegasus to run different types of
      transfer jobs for the sites specified on the remote site. The value of
      the property is a comma separated list of compute sites for which you
      want the transfer jobs to run remotely.</para>

      <para>The table below illustrates all the possible variations of the
      property.</para>

      <table>
        <title>Property Variations for pegasus.transfer.*.remote.sites</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry>Property Name</entry>

              <entry>Applies to</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>pegasus.transfer.stagein.remote.sites</entry>

              <entry>the stage in transfer jobs</entry>
            </row>

            <row>
              <entry>pegasus.transfer.stageout.remote.sites</entry>

              <entry>the stage out transfer jobs</entry>
            </row>

            <row>
              <entry>pegasus.transfer.inter.remote.sites</entry>

              <entry>the inter site transfer jobs</entry>
            </row>

            <row>
              <entry>pegasus.transfer.*.remote.sites</entry>

              <entry>all types of transfer jobs</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>The prefix for the transfer job name indicates whether the
      transfer job is to be executed locallly ( on the submit host ) or
      remotely ( on the compute site ). For example stage_in_local_ in a
      transfer job name stage_in_local_isi_viz_0 indicates that the transfer
      job is a stage in transfer job that is executed locally and is used to
      transfer input data to compute site isi_viz. The prefix naming scheme
      for the transfer jobs is <emphasis role="bold">
      [stage_in|stage_out|inter]_[local|remote]_</emphasis> .</para>
    </section>

    <section id="controlling_transfer_parallelism">
      <title>Controlling Transfer Parallelism</title>

      <para>When it comes to data transfers, Pegasus ships with a default
      configuration which is trying to strike a balance between performance
      and aggressiveness. We obviously want data transfers to be as quick as
      possibly, but we also do not want our transfers to overwhelm data
      services and systems. The default configuration consists of a
      combination of the maximum number of transfer jobs per level in the
      workflow, and how many threads such a pegasus-transfer job can
      spawn.</para>

      <para>Information on how to control the number of stagein and stageout
      jobs can be found in the <link linkend="data_movement_nodes"> Data
      Movement Nodes</link> section.</para>

      <para>How to control the number of threads pegasus-transfer can use
      depends on if you want to control standard transfer jobs, or
      PegasusLite. For the former, see the <link linkend="transfer_props">
      pegasus.transfer.threads</link> property, and for the latter the <link
      linkend="transfer_props"> pegasus.transfer.lite.threads</link>
      property.</para>
    </section>

    <section>
      <title>Symlinking Against Input Data</title>

      <para>If input data for a job already exists on a compute site, then it
      is possible for Pegasus to symlink against that data. In this case, the
      remote stage in transfer jobs that Pegasus adds to the executable
      workflow will symlink instead of doing a copy of the data.</para>

      <para>Pegasus determines whether a file is on the same site as the
      compute site, by inspecting the <emphasis>"site</emphasis>" attribute
      associated with the URL in the Replica Catalog. If the
      <emphasis>"site"</emphasis> attribute of an input file location matches
      the compute site where the job is scheduled, then that particular input
      file is a candidate for symlinking.</para>

      <para>For Pegasus to symlink against existing input data on a compute
      site, following must be true</para>

      <orderedlist>
        <listitem>
          <para>Property <emphasis role="bold">
          pegasus.transfer.links</emphasis> is set to <emphasis role="bold">
          true</emphasis></para>
        </listitem>

        <listitem>
          <para>The input file location in the Replica Catalog has the
          <emphasis>"site"</emphasis> attribute matching the compute
          site.</para>
        </listitem>
      </orderedlist>

      <tip>
        <para>To confirm if a particular input file is symlinked instead of
        being copied, look for the destination URL for that file in
        stage_in_remote*.in file. The destination URL will start with
        symlink:// .</para>
      </tip>

      <para>In the symlinking case, Pegasus strips out URL prefix from a URL
      and replaces it with a file URL.</para>

      <para>For example if a user has the following URL catalogued in the
      Replica Catalog for an input file f.input</para>

      <programlisting>f.input   gsiftp://server.isi.edu/shared/storage/input/data/f.input site="isi"</programlisting>

      <para>and the compute job that requires this file executes on a compute
      site named isi , then if symlinking is turned on the data stage in job
      (stage_in_remote_viz_0 ) will have the following source and destination
      specified for the file</para>

      <programlisting>#viz viz
file:///shared/storage/input/data/f.input  symlink://shared-scratch/workflow-exec-dir/f.input
</programlisting>
    </section>

    <section id="data_movement_nodes">
      <title>Addition of Separate Data Movement Nodes to Executable
      Workflow</title>

      <para>Pegasus relies on a Transfer Refiner that comes up with the
      strategy on how many data movement nodes are added to the executable
      workflow. All the compute jobs scheduled to a site share the same
      workflow specific directory. The transfer refiners ensure that only one
      copy of the input data is transferred to the workflow execution
      directory. This is to prevent data clobbering . Data clobbering can
      occur when compute jobs of a workflow share some input files, and have
      different stage in transfer jobs associated with them that are staging
      the shared files to the same destination workflow execution
      directory.</para>

      <para>Pegasus supports three different transfer refiners that dictate
      how the stagein and stageout jobs are added for the workflow.The default
      Transfer Refiner used in Pegasus is the BalancedCluster Refiner that
      allows the user to specify how many local|remote stagein|stageout jobs
      are created per execution site.</para>

      <para>The behavior of the refiners (BalancedCluster and Cluster) are
      controlled by specifying certain pegasus profiles</para>

      <orderedlist>
        <listitem>
          <para>either with the execution sites in the site catalog</para>
        </listitem>

        <listitem>
          <para>OR globally in the properties file</para>
        </listitem>
      </orderedlist>

      <table>
        <title>Pegasus Profile Keys For the Cluster Transfer Refiner</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry>Profile Key</entry>

              <entry>Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>stagein.clusters</entry>

              <entry>This key determines the maximum number of stage-in jobs
              that are can executed locally or remotely per compute site per
              workflow.</entry>
            </row>

            <row>
              <entry>stagein.local.clusters</entry>

              <entry>This key provides finer grained control in determining
              the number of stage-in jobs that are executed locally and are
              responsible for staging data to a particular remote
              site.</entry>
            </row>

            <row>
              <entry>stagein.remote.clusters</entry>

              <entry>This key provides finer grained control in determining
              the number of stage-in jobs that are executed remotely on the
              remote site and are responsible for staging data to it.</entry>
            </row>

            <row>
              <entry>stageout.clusters</entry>

              <entry>This key determines the maximum number of stage-out jobs
              that are can executed locally or remotely per compute site per
              workflow.</entry>
            </row>

            <row>
              <entry>stageout.local.clusters</entry>

              <entry>This key provides finer grained control in determining
              the number of stage-out jobs that are executed locally and are
              responsible for staging data from a particular remote
              site.</entry>
            </row>

            <row>
              <entry>stageout.remote.clusters</entry>

              <entry>This key provides finer grained control in determining
              the number of stage-out jobs that are executed remotely on the
              remote site and are responsible for staging data from
              it.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <tip>
        <para>Which transfer refiner to use is controlled by property
        pegasus.transfer.refiner</para>
      </tip>

      <section>
        <title>BalancedCluster</title>

        <para>This is a new transfer refiner that was introduced in Pegasus
        4.4.0 and is the default one used in Pegasus. It does a round robin
        distribution of the files amongst the stagein and stageout jobs per
        level of the workflow. The figure below illustrates the behavior of
        this transfer refiner.</para>

        <figure>
          <title>BalancedCluster Transfer Refiner : Input Data To Workflow
          Specific Directory on Shared File System</title>

          <mediaobject id="img-balanced-cluster-transfer-refiner">
            <imageobject role="html">
              <imagedata align="center" contentdepth="650px"
                         fileref="images/balanced-cluster-transfer-refiner.png"/>
            </imageobject>

            <imageobject role="fo">
              <imagedata align="center" contentdepth="4in"
                         fileref="images/balanced-cluster-transfer-refiner.png"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Cluster</title>

        <para>This transfer refiner is similar to BalancedCluster but differs
        in the way how distribution of files happen across stagein and
        stageout jobs per level of the workflow. In this refiner, all the
        input files for a job get associated with a single transfer job. As
        illustrated in the figure below each compute usually gets associated
        with one stagein transfer job. In contrast, for the BalancedCluster a
        compute job maybe associated with multiple data stagein jobs.</para>

        <figure>
          <title>Cluster Transfer Refiner : Input Data To Workflow Specific
          Directory on Shared File System</title>

          <mediaobject id="img-cluster-transfer-refiner">
            <imageobject role="html">
              <imagedata align="center" contentdepth="650px"
                         fileref="images/cluster-transfer-refiner.png"/>
            </imageobject>

            <imageobject role="fo">
              <imagedata align="center" contentdepth="4in"
                         fileref="images/cluster-transfer-refiner.png"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Basic</title>

        <para>Pegasus also supports a basic Transfer Refiner that adds one
        stagein and stageout job per compute job of the workflow. This is not
        recommended to be used for large workflows as the number of data
        transfer nodes in the worst case are 2n where n is the number of
        compute jobs in the workflow.</para>
      </section>
    </section>

    <section>
      <title>Executable Used for Transfer Jobs</title>

      <para>Pegasus refers to a python script called <emphasis role="bold">
      pegasus-transfer</emphasis> as the executable in the transfer jobs to
      transfer the data. pegasus-transfer is a python based wrapper around
      various transfer clients . pegasus-transfer looks at source and
      destination url and figures out automatically which underlying client to
      use. pegasus-transfer is distributed with the PEGASUS and can be found
      at $PEGASUS_HOME/bin/pegasus-transfer.</para>

      <para>Currently, pegasus-transfer interfaces with the following transfer
      clients</para>

      <table>
        <title>Transfer Clients interfaced to by pegasus-transfer</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry>Transfer Client</entry>

              <entry>Used For</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>globus-url-copy</entry>

              <entry>staging files to and from a gridftp server.</entry>
            </row>

            <row>
              <entry>lcg-copy</entry>

              <entry>staging files to and from a SRM server.</entry>
            </row>

            <row>
              <entry>wget</entry>

              <entry>staging files from a HTTP server.</entry>
            </row>

            <row>
              <entry>cp</entry>

              <entry>copying files from a POSIX filesystem .</entry>
            </row>

            <row>
              <entry>ln</entry>

              <entry>symlinking against input files.</entry>
            </row>

            <row>
              <entry>pegasus-s3</entry>

              <entry>staging files to and from S3 bucket in the Amazon
              cloud</entry>
            </row>

            <row>
              <entry>gsutil</entry>

              <entry>staging files to and from Google Storage buckets</entry>
            </row>

            <row>
              <entry>scp</entry>

              <entry>staging files using scp</entry>
            </row>

            <row>
              <entry>iget</entry>

              <entry>staging files to and from a irods server.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>For remote sites, Pegasus constructs the default path to
      pegasus-transfer on the basis of PEGASUS_HOME env profile specified in
      the site catalog. To specify a different path to the pegasus-transfer
      client , users can add an entry into the transformation catalog with
      fully qualified logical name as <emphasis
      role="bold">pegasus::pegasus-transfer</emphasis></para>
    </section>

    <section>
      <title>Staging of Executables</title>

      <para>Users can get Pegasus to stage the user executables ( executables
      that the jobs in the DAX refer to ) as part of the transfer jobs to the
      workflow specific execution directory on the compute site. The URL
      locations of the executables need to be specified in the transformation
      catalog as the PFN and the type of executable needs to be set to
      <emphasis role="bold"> STAGEABLE</emphasis> .</para>

      <para>The location of a transformation can be specified either in</para>

      <itemizedlist>
        <listitem>
          <para>DAX in the executables section. More details <link
          linkend="dax_transformation_catalog">here</link> .</para>
        </listitem>

        <listitem>
          <para>Transformation Catalog. More details <link
          linkend="transformation">here</link> .</para>
        </listitem>
      </itemizedlist>

      <para>A particular transformation catalog entry of type STAGEABLE is
      compatible with a compute site only if all the System Information
      attributes associated with the entry match with the System Information
      attributes for the compute site in the Site Catalog. The following
      attributes make up the System Information attributes</para>

      <orderedlist>
        <listitem>
          <para>arch</para>
        </listitem>

        <listitem>
          <para>os</para>
        </listitem>

        <listitem>
          <para>osrelease</para>
        </listitem>

        <listitem>
          <para>osversion</para>
        </listitem>
      </orderedlist>

      <section>
        <title>Transformation Mappers</title>

        <para>Pegasus has a notion of transformation mappers that determines
        what type of executables are picked up when a job is executed on a
        remote compute site. For transfer of executables, Pegasus constructs a
        soft state map that resides on top of the transformation catalog, that
        helps in determining the locations from where an executable can be
        staged to the remote site.</para>

        <para>Users can specify the following property to pick up a specific
        transformation mapper</para>

        <programlisting><emphasis role="bold">pegasus.catalog.transformation.mapper</emphasis> </programlisting>

        <para>Currently, the following transformation mappers are
        supported.</para>

        <table>
          <title>Transformation Mappers Supported in Pegasus</title>

          <tgroup cols="2">
            <thead>
              <row>
                <entry>Transformation Mapper</entry>

                <entry>Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Installed</entry>

                <entry>This mapper only relies on transformation catalog
                entries that are of type INSTALLED to construct the soft state
                map. This results in Pegasus never doing any transfer of
                executables as part of the workflow. It always prefers the
                installed executables at the remote sites</entry>
              </row>

              <row>
                <entry>Staged</entry>

                <entry>This mapper only relies on matching transformation
                catalog entries that are of type STAGEABLE to construct the
                soft state map. This results in the executable workflow
                referring only to the staged executables, irrespective of the
                fact that the executables are already installed at the remote
                end</entry>
              </row>

              <row>
                <entry>All</entry>

                <entry>This mapper relies on all matching transformation
                catalog entries of type STAGEABLE or INSTALLED for a
                particular transformation as valid sources for the transfer of
                executables. This the most general mode, and results in the
                constructing the map as a result of the cartesian product of
                the matches.</entry>
              </row>

              <row>
                <entry>Submit</entry>

                <entry>This mapper only on matching transformation catalog
                entries that are of type STAGEABLE and reside at the submit
                host (site local), are used while constructing the soft state
                map. This is especially helpful, when the user wants to use
                the latest compute code for his computations on the grid and
                that relies on his submit host.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>
    </section>

    <section>
      <title>Staging of Pegasus Worker Package</title>

      <para>Pegasus can optionally stage the pegasus worker package as part of
      the executable workflow to remote workflow specific execution directory.
      The pegasus worker package contains the pegasus auxillary executables
      that are required on the remote site. If the worker package is not
      staged as part of the executable workflow, then Pegasus relies on the
      installed version of the worker package on the remote site. To determine
      the location of the installed version of the worker package on a remote
      site, Pegasus looks for an environment profile PEGASUS_HOME for the site
      in the Site Catalog.</para>

      <para>Users can set the following property to true to turn on worker
      package staging</para>

      <programlisting><emphasis role="bold">pegasus.transfer.worker.package          true</emphasis> </programlisting>

      <para>By default, when worker package staging is turned on pegasus pulls
      the compatible worker package from the Pegasus Website. To specify a
      different worker package location, users can specify the transformation
      <emphasis role="bold">pegasus::worker</emphasis> in the transformation
      catalog with</para>

      <itemizedlist>
        <listitem>
          <para>type set to STAGEABLE</para>
        </listitem>

        <listitem>
          <para>System Information attributes of the transformation catalog
          entry match the System Information attributes of the compute
          site.</para>
        </listitem>

        <listitem>
          <para>the PFN specified should be a remote URL that can be pulled to
          the compute site.</para>
        </listitem>
      </itemizedlist>

      <section>
        <title>Worker Package Staging in Non Shared Filesystem setup</title>

        <para>Worker package staging is automatically set to true , when
        workflows are setup to run in a non shared filesystem setup i.e.
        <emphasis role="bold">pegasus.data.configuration</emphasis> is set to
        <emphasis role="bold">nonsharedfs</emphasis> or <emphasis
        role="bold">condorio</emphasis> . In these configurations, a
        stage_worker job is created that brings in the worker package to the
        submit directory of the workflow. For each job, the worker package is
        then transferred with the job using Condor File Transfers ( <emphasis
        role="bold">transfer_input_files</emphasis> ) . This transfer always
        happens unless, PEGASUS_HOME is specified in the site catalog for the
        site on which the job is scheduled to run.</para>

        <para>Users can explicitly set the following property to false, to
        turn off worker package staging by the Planner. This is applicable ,
        when running in the cloud and virtual machines / worker nodes already
        have the pegasus worker tools installed.</para>

        <programlisting><emphasis role="bold">pegasus.transfer.worker.package          false</emphasis> </programlisting>
      </section>
    </section>

    <section id="staging_job_checkpoint_files">
      <title>Staging of Job Checkpoint Files</title>

      <para>Pegasus has support for transferring job checkpoint files back to
      the staging site, when a job exceeds it's advertised running time. In
      order to use this feature, you need to</para>

      <orderedlist>
        <listitem>
          <para>Associate a job checkpoint file ( that the job creates ) with
          the job in the DAX. A checkpoint file is specified by setting the
          link attribute to checkpoint for the uses tag.</para>
        </listitem>

        <listitem>
          <para>Associate a Pegasus profile key named <emphasis role="bold">
          checkpoint_time</emphasis> with the job that specifies the expected
          runtime for the job in seconds.</para>
        </listitem>

        <listitem>
          <para>Associate a Pegasus profile key named <emphasis role="bold">
          maxwalltime</emphasis> with the job that specifies the max runtime
          before the job will be killed by the local resource manager ( such
          as PBS) deployed on the site. Usually, this value should be
          associated with the execution site in the site catalog. Note that
          this value is in minutes.</para>
        </listitem>
      </orderedlist>

      <para>Pegasus planner uses the above mentioned profile keys to setup
      pegasus-kickstart such that the job is sent a TERM signal when expected
      runtime of job is reached. A KILL signal is sent at (expected_walltime +
      (maxwalltime-expected_walltime)/2) seconds. This ensures that there is
      enough time for pegasus-lite to transfer the checkpoint file before the
      job is killed by the underlying scheduler.</para>
    </section>

    <section>
      <title>Using Amazon S3 as a Staging Site</title>

      <para>Pegasus can be configured to use Amazon S3 as a staging site. In
      this mode, Pegasus transfers workflow inputs from the input site to S3.
      When a job runs, the inputs for that job are fetched from S3 to the
      worker node, the job is executed, then the output files are transferred
      from the worker node back to S3. When the jobs are complete, Pegasus
      transfers the output data from S3 to the output site.</para>

      <para>In order to use S3, it is necessary to create a config file for
      the S3 transfer client, <link linkend="cli-pegasus-s3">
      pegasus-s3</link>. See the <link linkend="cli-pegasus-s3">man
      page</link> for details on how to create the config file. You also need
      to specify <link linkend="non_shared_fs">S3 as a staging
      site</link>.</para>

      <para>Next, you need to modify your site catalog to tell the location of
      your s3cfg file. See <link linkend="cred_staging">the section on
      credential staging</link>.</para>

      <para>The following site catalog shows how to specify the location of
      the s3cfg file on the local site and how to specify an Amazon S3 staging
      site:</para>

      <programlisting>&lt;sitecatalog xmlns="http://pegasus.isi.edu/schema/sitecatalog"
             xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
             xsi:schemaLocation="http://pegasus.isi.edu/schema/sitecatalog
             http://pegasus.isi.edu/schema/sc-3.0.xsd" version="3.0"&gt;
    &lt;site handle="local" arch="x86_64" os="LINUX"&gt;
        &lt;head-fs&gt;
            &lt;scratch&gt;
                &lt;shared&gt;
                    &lt;file-server protocol="file" url="file://" mount-point="/tmp/wf/work"/&gt;
                    &lt;internal-mount-point mount-point="/tmp/wf/work"/&gt;
                &lt;/shared&gt;
            &lt;/scratch&gt;
            &lt;storage&gt;
                &lt;shared&gt;
                    &lt;file-server protocol="file" url="file://" mount-point="/tmp/wf/storage"/&gt;
                    &lt;internal-mount-point mount-point="/tmp/wf/storage"/&gt;
                &lt;/shared&gt;
            &lt;/storage&gt;
        &lt;/head-fs&gt;
        <emphasis role="bold">&lt;profile namespace="env" key="S3CFG"&gt;/home/username/.s3cfg&lt;/profile&gt;</emphasis>
    &lt;/site&gt;
    <emphasis role="bold">&lt;site handle="s3" arch="x86_64" os="LINUX"&gt;
        &lt;head-fs&gt;
            &lt;scratch&gt;
                &lt;shared&gt;
                    &lt;!-- wf-scratch is the name of the S3 bucket that will be used --&gt;
                    &lt;file-server protocol="s3" url="s3://user@amazon" mount-point="/wf-scratch"/&gt;
                    &lt;internal-mount-point mount-point="/wf-scratch"/&gt;
                &lt;/shared&gt;
            &lt;/scratch&gt;
        &lt;/head-fs&gt;
    &lt;/site&gt;</emphasis>
    &lt;site handle="condorpool" arch="x86_64" os="LINUX"&gt;
        &lt;head-fs&gt;
            &lt;scratch/&gt;
            &lt;storage/&gt;
        &lt;/head-fs&gt;
        &lt;profile namespace="pegasus" key="style"&gt;condor&lt;/profile&gt;
        &lt;profile namespace="condor" key="universe"&gt;vanilla&lt;/profile&gt;
        &lt;profile namespace="condor" key="requirements"&gt;(Target.Arch == "X86_64")&lt;/profile&gt;
    &lt;/site&gt;
&lt;/sitecatalog&gt;
</programlisting>
    </section>

    <section>
      <title>iRODS data access</title>

      <para>iRODS can be used as a input data location, a storage site for
      intermediate data during workflow execution, or a location for final
      output data. Pegasus uses a URL notation to identify iRODS files.
      Example:</para>

      <programlisting>irods://some-host.org/path/to/file.txt</programlisting>

      <para>The path to the file is <emphasis role="bold"> relative</emphasis>
      to the internal iRODS location. In the example above, the path used to
      refer to the file in iRODS is <emphasis> path/to/file.txt</emphasis> (no
      leading /).</para>

      <para>See <link linkend="cred_staging">the section on credential
      staging</link> for information on how to set up an irodsEnv file to be
      used by Pegasus.</para>
    </section>
  </section>

  <section id="cred_staging">
    <title>Credentials Management</title>

    <para>Pegasus tries to do data staging from localhost by default, but some
    data scenarios makes some <link linkend="local_vs_remote_transfers">remote
    jobs do data staging</link>. An example of such a case is when running in
    <link linkend="ref_data_staging_configuration">nonsharedfs</link> mode.
    Depending on the transfer protocols used, the job may have to carry
    credentials to enable these data transfers. To specify where which
    credential to use and where Pegasus can find it, use environment variable
    profiles in your site catalog. The supported credential types are X.509
    grid proxies, Amazon AWS S3 keys, Google Cloud Platform OAuth token (.boto
    file), iRods password and SSH keys.</para>

    <para>Credentials are usually associated per site in the site catalog.
    Users can associate the credentials either as a Pegasus profile or an
    environment profile with the site.</para>

    <orderedlist>
      <listitem>
        <para>A pegasus profile with the value pointing to the path to the
        credential on the local site or the submit host. If a pegasus
        credential profile associated with the site, then Pegasus
        automatically transfers it along with the remote jobs.</para>
      </listitem>

      <listitem>
        <para>A env profile with the value pointing to the path to the
        credential on the remote site. If an env profile is specified, then no
        credential is transferred along with the job. Instead the job's
        environment is set to ensure that the job picks up the path to the
        credential on the remote site.</para>
      </listitem>
    </orderedlist>

    <tip>
      <para>Specifying credentials as Pegasus profiles was introduced in 4.4.0
      release.</para>
    </tip>

    <para>In case of data transfer jobs, it is possible to associate different
    credentials for a single file transfer ( one for the source server and the
    other for the destination server) . For example, when leveraging GridFTP
    transfers between two sides that accept different grid credentials such as
    XSEDE Stampede site and NCSA Bluewaters. In that case, Pegasus picks up
    the associated credentials from the site catalog entries for the source
    and the destination sites associated with the transfer.</para>

    <section id="x509_cred">
      <title>X.509 Grid Proxies</title>

      <para>If the grid proxy is required by transfer jobs, and the proxy is
      in the standard location, Pegasus will pick the proxy up automatically.
      For non-standard proxy locations, you can use the
      <varname>X509_USER_PROXY</varname> environment variable. Site catalog
      example:</para>

      <programlisting>&lt;profile namespace="pegasus" key="X509_USER_PROXY" &gt;/some/location/x509up&lt;/profile&gt;</programlisting>
    </section>

    <section id="s3_cred">
      <title>Amazon AWS S3</title>

      <para>If a workflow is using s3 URLs, Pegasus has to be told where to
      find the .s3cfg file. This format of the file is described in the <link
      linkend="cli-pegasus-s3">pegaus-s3 command line client's man
      page</link>. For the file to be picked up by the workflow, set the
      <varname>S3CFG</varname> profile to the location of the file. Site
      catalog example:</para>

      <programlisting>&lt;profile namespace="pegasus" pegasus="S3CFG" &gt;/home/user/.s3cfg&lt;/profile&gt;</programlisting>
    </section>

    <section id="gs_cred">
      <title>Google Storage</title>

      <para>If a workflow is using gs:// URLs, Pegasus needs access to a
      Google Storage service account. First generate the credential by
      following the instructions at:</para>

      <para><ulink
      url="https://cloud.google.com/storage/docs/authentication#service_accounts">https://cloud.google.com/storage/docs/authentication#service_accounts</ulink></para>

      <para>Download the credential in PKCS12 format, and then use "gsutil
      config -e" to generate a .boto file. For example:</para>

      <programlisting>
$ gsutil config -e
This command will create a boto config file at /home/username/.boto
containing your credentials, based on your responses to the following
questions.
What is your service account email address? some-identifier@developer.gserviceaccount.com
What is the full path to your private key file? /home/username/my-cred.p12
What is the password for your service key file [if you haven't set one
explicitly, leave this line blank]? 

Please navigate your browser to https://cloud.google.com/console#/project,
then find the project you will use, and copy the Project ID string from the
second column. Older projects do not have Project ID strings. For such projects,
click the project and then copy the Project Number listed under that project.

What is your project-id? your-project-id

Boto config file "/home/username/.boto" created. If you need to use a
proxy to access the Internet please see the instructions in that file.
            </programlisting>

      <para>Pegasus has to be told where to find both the .boto file as well
      as the PKCS12 file. For the files to be picked up by the workflow, set
      the <varname>BOTO_CONFIG</varname> and <varname>GOOGLE_PKCS12</varname>
      profiles for the storage site. Site catalog example:</para>

      <programlisting>
&lt;profile namespace="pegasus" key="BOTO_CONFIG" &gt;/home/user/.boto&lt;/profile&gt;
&lt;profile namespace="pegasus" key="GOOGLE_PKCS12" &gt;/home/user/.google-service-account.p12&lt;/profile&gt;
</programlisting>
    </section>

    <section id="irods_cred">
      <title>iRods Password</title>

      <para>If a workflow is using irods URLs, Pegasus has to be given an
      irodsEnv file. It is a standard file, with the addtion of an password
      attribute. Example:</para>

      <programlisting># iRODS personal configuration file.
#
# iRODS server host name:
irodsHost 'iren.renci.org'
# iRODS server port number:
irodsPort 1259

# Default storage resource name:
irodsDefResource 'renResc'
# Home directory in iRODS:
irodsHome '/tip-renci/home/mats'
# Current directory in iRODS:
irodsCwd '/tip-renci/home/mats'
# Account name:
irodsUserName 'mats'
# Zone:
irodsZone 'tip-renci' 

# this is used with Pegasus
irodsPassword 'somesecretpassword'</programlisting>

      <para>The location of the file can be given to the workflow using the
      <varname>irodsEnvFile</varname> environment profile. Site catalog
      example:</para>

      <programlisting>&lt;profile namespace="pegasus" key="irodsEnvFile" &gt;/home/user/.irods/.irodsEnv&lt;/profile&gt;</programlisting>
    </section>

    <section id="ssh_cred">
      <title>SSH Keys</title>

      <para>New in Pegasus 4.0 is the support for data staging with scp using
      ssh public/private key authentication. In this mode, Pegasus transports
      a private key with the jobs. The storage machines will have to have the
      public part of the key listed in ~/.ssh/authorized_keys.</para>

      <warning>
        <para>SSH keys should be handled in a secure manner. In order to keep
        your personal ssh keys secure, It is recommended that a special set of
        keys are created for use with the workflow. Note that Pegasus will not
        pick up ssh keys automatically. The user will have to specify which
        key to use with <varname> SSH_PRIVATE_KEY</varname>.</para>
      </warning>

      <para>The location of the ssh private key can be specified with the
      <varname>SSH_PRIVATE_KEY</varname> environment profile. Site catalog
      example:</para>

      <programlisting>&lt;profile namespace="pegasus" key="SSH_PRIVATE_KEY" &gt;/home/user/wf/wfsshkey&lt;/profile&gt;</programlisting>
    </section>
  </section>

  <section id="ref_output_mapper">
    <title>Output Mappers</title>

    <para>Starting 4.3 release, Pegasus has support for output mappers, that
    allow users fine grained control over how the output files on the output
    site are laid out. By default, Pegasus stages output products to the
    storage directory specified in the site catalog for the output site.
    Output mappers allow users finer grained control over where the output
    files are placed on the output site.</para>

    <para>The following mappers are supported currently</para>

    <orderedlist>
      <listitem>
        <para><emphasis role="bold">Flat</emphasis> : By default, Pegasus will
        place the output files in the storage directory specified in the site
        catalog for the output site.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Fixed</emphasis> : This mapper allows
        users to specify an externally accesible url to the storage directory
        in their properties file. To use this mapper, the following property
        needs to be set.</para>

        <itemizedlist>
          <listitem>
            <para>pegasus.dir.storage.mapper.fixed.url an externally
            accessible URL to the storage directory on the output site e.g.
            gsiftp://outputs.isi.edu/shared/outputs</para>
          </listitem>
        </itemizedlist>

        <para>Note: For hierarchal workflows, the above property needs to be
        set separately for each dax job, if you want the sub workflow outputs
        to goto a different directory.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Hashed</emphasis> : This mapper results in
        the creation of a deep directory structure on the output site, while
        populating the results. The base directory on the remote end is
        determined from the site catalog. Depending on the number of files
        being staged to the remote site a Hashed File Structure is created
        that ensures that only 256 files reside in one directory. To create
        this directory structure on the storage site, Pegasus relies on the
        directory creation feature of the underlying file servers such as
        theGrid FTP server, which appeared in globus 4.0.x</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Replica:</emphasis>This mapper determines
        the path for an output file on the output site by querying an output
        replica catalog. The output site is one that is passed on the command
        line. The output replica catalog can be configured by specifying the
        properties</para>

        <itemizedlist>
          <listitem>
            <para>pegasus.dir.storage.mapper.replica Regex|File</para>
          </listitem>

          <listitem>
            <para>pegasus.dir.storage.mapper.replica.file the RC file at the
            backend to use</para>
          </listitem>
        </itemizedlist>
      </listitem>
    </orderedlist>

    <tip>
      <para>The mappers can be configured by setting the property <emphasis
      role="bold">pegasus.dir.storage.mapper</emphasis></para>
    </tip>

    <note>
      <para>The Fixed mapper will be available starting 4.3.1 release.</para>
    </note>
  </section>

  <section id="data_cleanup">
    <title>Data Cleanup</title>

    <para>When executing large workflows, users often may run out of diskspace
    on the remote clusters / staging site. Pegasus provides a couple of ways
    of enabling automated data cleanup on the staging site ( i.e the scratch
    space used by the workflows). This is achieved by adding data cleanup jobs
    to the executable workflow that the Pegasus Mapper generates. These
    cleanup jobs are responsible for removing files and directories during the
    workflow execution. To enable data cleanup you can pass the --cleanup
    option to pegasus-plan . The value passed decides the cleanup strategy
    implemented</para>

    <orderedlist>
      <listitem>
        <para><emphasis role="bold">none </emphasis> disables cleanup
        altogether. The planner does not add any cleanup jobs in the
        executable workflow whatsoever.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">leaf</emphasis> the planner adds a leaf
        cleanup node per staging site that removes the directory created by
        the create dir job in the workflow</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">inplace t</emphasis>he mapper adds cleanup
        nodes per level of the workflow in addition to leaf cleanup nodes. The
        nodes remove files no longer required during execution. For example,
        an added cleanup node will remove input files for a particular compute
        job after the job has finished successfully. This is the default
        value.</para>
      </listitem>
    </orderedlist>

    <note>
      <para>For large workflows with lots of files, the inplace strategy may
      take a long time as the algorithm works at a per file level to figure
      out when it is safe to remove a file.</para>
    </note>

    <para>Behaviour of the cleanup strategies implemented in the Pegasus
    Mapper can be controlled by properties described <link
    linkend="cleanup_props">here</link> . </para>

    <section>
      <title>Data Cleanup in Hierarchal Workflows</title>

      <para>By default, for hierarchal workflows the inplace cleanup is always
      turned off. This is because the cleanup algorithm ( InPlace ) does not
      work across the sub workflows. For example, if you have two DAX jobs in
      your top level workflow and the child DAX job refers to a file generated
      during the execution of the parent DAX job, the InPlace cleanup
      algorithm when applied to the parent dax job will result in the file
      being deleted, when the sub workflow corresponding to parent DAX job is
      executed. This would result in failure of sub workflow corresponding to
      the child DAX job, as the file deleted is required to present during
      it's execution.</para>

      <para>In case there are no data dependencies across the dax jobs, then
      yes you can enable the InPlace algorithm for the sub daxes . To do this
      you can set the property</para>

      <itemizedlist>
        <listitem>
          <para>pegasus.file.cleanup.scope deferred</para>
        </listitem>
      </itemizedlist>

      <para>This will result in cleanup option to be picked up from the
      arguments for the DAX job in the top level DAX .</para>
    </section>

    <section>
      <title>Executables used for Directory Creation and Cleanup Jobs</title>

      <para>Starting 4.0, Pegasus has changed the way how the scratch
      directories are created on the staging site. The planner now prefers to
      schedule the directory creation and cleanup jobs locally. The jobs refer
      to python based tools, that call out to protocol specific clients to
      determine what client is picked up. For protocols, where specific remote
      cleanup and directory creation clients don't exist ( for example gridftp
      ), the python tools rely on the corresponding transfer tool to create a
      directory by initiating a transfer of an empty file. The python clients
      used to create directories and remove files are called</para>

      <itemizedlist>
        <listitem>
          <para>pegasus-create-dir</para>
        </listitem>

        <listitem>
          <para>pegasus-cleanup</para>
        </listitem>
      </itemizedlist>

      <para>Both these clients inspect the URL's to to determine what
      underlying client to pick up.</para>

      <table>
        <title>Clients interfaced to by pegasus-create-dir</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry>Client</entry>

              <entry>Used For</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>globus-url-copy</entry>

              <entry>to create directories against a gridftp/ftp
              server</entry>
            </row>

            <row>
              <entry>srm-mkdir</entry>

              <entry>to create directories against a SRM server.</entry>
            </row>

            <row>
              <entry>mkdir</entry>

              <entry>to create a directory on the local filesystem</entry>
            </row>

            <row>
              <entry>pegasus-s3</entry>

              <entry>to create a S3 bucket in the Amazon cloud</entry>
            </row>

            <row>
              <entry>gsutil</entry>

              <entry>to create a Google Storage bucket</entry>
            </row>

            <row>
              <entry>scp</entry>

              <entry>staging files using scp</entry>
            </row>

            <row>
              <entry>imkdir</entry>

              <entry>to create a directory against an IRODS server</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <table>
        <title>Clients interfaced to by pegasus-cleanup</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry>Client</entry>

              <entry>Used For</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>globus-url-copy</entry>

              <entry>to remove a file against a gridftp/ftp server. In this
              case a zero byte file is created</entry>
            </row>

            <row>
              <entry>srm-rm</entry>

              <entry>to remove files against a SRM server.</entry>
            </row>

            <row>
              <entry>rm</entry>

              <entry>to remove a file on the local filesystem</entry>
            </row>

            <row>
              <entry>pegasus-s3</entry>

              <entry>to remove a file from the s3 bucket.</entry>
            </row>

            <row>
              <entry>gsutil</entry>

              <entry>to remove an object from a Google Storage bucket</entry>
            </row>

            <row>
              <entry>scp</entry>

              <entry>to remove a file against a scp server. In this case a
              zero byte file is created.</entry>
            </row>

            <row>
              <entry>irm</entry>

              <entry>to remove a file against an IRODS server</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>The only case, where the create dir and cleanup jobs are scheduled
      to run remotely is when for the staging site, a file server is
      specified.</para>
    </section>
  </section>
</chapter>
