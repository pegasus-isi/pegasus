<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="data_management">
  <title>Data Management</title>

  <section id="replica_selection">
    <title>Replica Selection</title>

    <para>Each job in the DAX maybe associated with input LFN's denoting the
    files that are required for the job to run. To determine the physical
    replica (PFN) for a LFN, Pegasus queries the Replica catalog to get all
    the PFN's (replicas) associated with a LFN. The Replica Catalog may return
    multiple PFN's for each of the LFN's queried. Hence, Pegasus needs to
    select a single PFN amongst the various PFN's returned for each LFN. This
    process is known as replica selection in Pegasus. Users can specify the
    replica selector to use in the properties file.</para>

    <para>This document describes the various Replica Selection Strategies in
    Pegasus.</para>

    <section>
      <title>Configuration</title>

      <para>The user properties determine what replica selector Pegasus
      Workflow Mapper uses. The property <emphasis
      role="bold">pegasus.selector.replica</emphasis> is used to specify the
      replica selection strategy. Currently supported Replica Selection
      strategies are</para>

      <orderedlist>
        <listitem>
          <para>Default</para>
        </listitem>

        <listitem>
          <para>Regex</para>
        </listitem>

        <listitem>
          <para>Restricted</para>
        </listitem>

        <listitem>
          <para>Local</para>
        </listitem>
      </orderedlist>

      <para>The values are case sensitive. For example the following property
      setting will throw a Factory Exception .</para>

      <programlisting>pegasus.selector.replica  default</programlisting>

      <para>The correct way to specify is</para>

      <programlisting>pegasus.selector.replica  Default</programlisting>
    </section>

    <section>
      <title>Supported Replica Selectors</title>

      <para>The various Replica Selectors supported in Pegasus Workflow Mapper
      are explained below.</para>

      <note>
        <para>Starting 4.6.0 release the Default and Regex Replica Selectors
        return an ordered list with priorities set. pegasus-transfer at
        runtime will failover to alternate url's specified, if a higher
        priority source URL is inaccessible.</para>
      </note>

      <section id="replica_selection_default">
        <title>Default</title>

        <para>This is the default replica selector used in the Pegasus
        Workflow Mapper. If the property pegasus.selector.replica is not
        defined in properties, then Pegasus uses this selector.</para>

        <para>The selector orders the various candidate replica's according to
        the following rules</para>

        <orderedlist>
          <listitem>
            <para>valid file URL's . That is URL's that have the site
            attribute matching the site where the executable
            <emphasis>pegasus-transfer</emphasis> is executed.</para>
          </listitem>

          <listitem>
            <para>all URL's from preferred site (usually the compute
            site)</para>
          </listitem>

          <listitem>
            <para>all other remotely accessible ( non file) URL's</para>
          </listitem>
        </orderedlist>

        <para>To use this replica selector set the following
        property<programlisting>pegasus.selector.replica                  Default</programlisting></para>
      </section>

      <section>
        <title>Regex</title>

        <para>This replica selector allows the user to specific regular
        expressions that can be used to rank various PFN's returned from the
        Replica Catalog for a particular LFN. This replica selector orders the
        replicas based on the rank. Lower the rank higher the
        preference.</para>

        <para>The regular expressions are assigned different rank, that
        determine the order in which the expressions are employed. The rank
        values for the regex can expressed in user properties using the
        property.</para>

        <programlisting>pegasus.selector.replica.regex.rank.<emphasis
            role="bold">[value]</emphasis>                  regex-expression</programlisting>

        <para>The <emphasis role="bold">[value]</emphasis> in the above
        property is an integer value that denotes the rank of an expression
        with a rank value of 1 being the highest rank.</para>

        <para>For example, a user can specify the following regex expressions
        that will ask Pegasus to prefer file URL's over gsiftp url's from
        example.isi.edu</para>

        <programlisting>pegasus.selector.replica.regex.rank.1                       file://.*
pegasus.selector.replica.regex.rank.2                       gsiftp://example\.isi\.edu.*</programlisting>

        <para>User can specify as many regex expressions as they want.</para>

        <para>Since Pegasus is in Java , the regex expression support is what
        Java supports. It is pretty close to what is supported by Perl. More
        details can be found at
        http://java.sun.com/j2se/1.5.0/docs/api/java/util/regex/Pattern.html</para>

        <para>Before applying any regular expressions on the PFN's for a
        particular LFN that has to be staged to a site X, the file URL's that
        don't match the site X are explicitly filtered out.</para>

        <para>To use this replica selector set the following
        property<programlisting>pegasus.selector.replica                  Regex</programlisting></para>
      </section>

      <section>
        <title>Restricted</title>

        <para>This replica selector, allows the user to specify good sites and
        bad sites for staging in data to a particular compute site. A good
        site for a compute site X, is a preferred site from which replicas
        should be staged to site X. If there are more than one good sites
        having a particular replica, then a random site is selected amongst
        these preferred sites.</para>

        <para>A bad site for a compute site X, is a site from which replicas
        should not be staged. The reason of not accessing replica from a bad
        site can vary from the link being down, to the user not having
        permissions on that site's data.</para>

        <para>The good | bad sites are specified by the following
        properties</para>

        <programlisting>pegasus.replica.*.prefer.stagein.sites
pegasus.replica.*.ignore.stagein.sites</programlisting>

        <para>where the * in the property name denotes the name of the compute
        site. A * in the property key is taken to mean all sites. The value to
        these properties is a comma separated list of sites.</para>

        <para>For example the following settings</para>

        <programlisting>pegasus.selector.replica.*.prefer.stagein.sites            usc
pegasus.replica.uwm.prefer.stagein.sites                   isi,cit
</programlisting>

        <para>means that prefer all replicas from site usc for staging in to
        any compute site. However, for uwm use a tighter constraint and prefer
        only replicas from site isi or cit. The pool attribute associated with
        the PFN's tells the replica selector to what site a replica/PFN is
        associated with.</para>

        <para>The pegasus.replica.*.prefer.stagein.sites property takes
        precedence over pegasus.replica.*.ignore.stagein.sites property i.e.
        if for a site X, a site Y is specified both in the ignored and the
        preferred set, then site Y is taken to mean as only a preferred site
        for a site X.</para>

        <para>To use this replica selector set the following property</para>

        <programlisting>pegasus.selector.replica                  Restricted</programlisting>
      </section>

      <section id="replica_selection_local">
        <title>Local</title>

        <para>This replica selector always prefers replicas from the local
        host ( pool attribute set to local ) and that start with a file: URL
        scheme. It is useful, when users want to stagein files to a remote
        site from the submit host using the Condor file transfer
        mechanism.</para>

        <para>To use this replica selector set the following
        property<programlisting>pegasus.selector.replica                  Local</programlisting></para>
      </section>
    </section>
  </section>

  <section id="transfer">
    <title>Data Transfers</title>

    <para>As part of the Workflow Mapping Process, Pegasus does data
    management for the executable workflow . It queries a Replica Catalog to
    discover the locations of the input datasets and adds data movement and
    registration nodes in the workflow to</para>

    <orderedlist>
      <listitem>
        <para>stage-in input data to the staging sites ( a site associated
        with the compute job to be used for staging. In the shared filesystem
        setup, staging site is the same as the execution sites where the jobs
        in the workflow are executed )</para>
      </listitem>

      <listitem>
        <para>stage-out output data generated by the workflow to the final
        storage site.</para>
      </listitem>

      <listitem>
        <para>stage-in intermediate data between compute sites if
        required.</para>
      </listitem>

      <listitem>
        <para>data registration nodes to catalog the locations of the output
        data on the final storage site into the replica catalog.</para>
      </listitem>
    </orderedlist>

    <para>The separate data movement jobs that are added to the executable
    workflow are responsible for staging data to a workflow specific directory
    accessible to the staging server on a staging site associated with the
    compute sites. Depending on the data staging configuration, the staging
    site for a compute site is the compute site itself. In the default case,
    the staging server is usually on the headnode of the compute site and has
    access to the shared filesystem between the worker nodes and the head
    node. Pegasus adds a directory creation job in the executable workflow
    that creates the workflow specific directory on the staging server.</para>

    <para>In addition to data, Pegasus does transfer user executables to the
    compute sites if the executables are not installed on the remote sites
    before hand. This chapter gives an overview of how transfers of data and
    executables is managed in Pegasus.</para>

    <section id="ref_data_staging_configuration">
      <title>Data Staging Configuration</title>

      <para>Pegasus can be broadly setup to run workflows in the following
      configurations</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">Shared File System</emphasis></para>

          <para>This setup applies to where the head node and the worker nodes
          of a cluster share a filesystem. Compute jobs in the workflow run in
          a directory on the shared filesystem.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">NonShared FileSystem</emphasis></para>

          <para>This setup applies to where the head node and the worker nodes
          of a cluster don't share a filesystem. Compute jobs in the workflow
          run in a local directory on the worker node</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Condor Pool Without a shared
          filesystem</emphasis></para>

          <para>This setup applies to a condor pool where the worker nodes
          making up a condor pool don't share a filesystem. All data IO is
          achieved using Condor File IO. This is a special case of the non
          shared filesystem setup, where instead of using pegasus-transfer to
          transfer input and output data, Condor File IO is used.</para>
        </listitem>
      </itemizedlist>

      <para>For the purposes of data configuration various sites, and
      directories are defined below.</para>

      <orderedlist>
        <listitem>
          <para><emphasis role="bold">Submit Host</emphasis></para>

          <para>The host from where the workflows are submitted . This is
          where Pegasus and Condor DAGMan are installed. This is referred to
          as the <emphasis role="bold">"local"</emphasis> site in the site
          catalog .</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Compute Site</emphasis></para>

          <para>The site where the jobs mentioned in the DAX are executed.
          There needs to be an entry in the Site Catalog for every compute
          site. The compute site is passed to pegasus-plan using <emphasis
          role="bold">--sites</emphasis> option</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Staging Site</emphasis></para>

          <para>A site to which the separate transfer jobs in the executable
          workflow ( jobs with stage_in , stage_out and stage_inter prefixes
          that Pegasus adds using the transfer refiners) stage the input data
          to and the output data from to transfer to the final output site.
          Currently, the staging site is always the compute site where the
          jobs execute.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Output Site</emphasis></para>

          <para>The output site is the final storage site where the users want
          the output data from jobs to go to. The output site is passed to
          pegasus-plan using the <emphasis role="bold">--output</emphasis>
          option. The stageout jobs in the workflow stage the data from the
          staging site to the final storage site.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Input Site</emphasis></para>

          <para>The site where the input data is stored. The locations of the
          input data are catalogued in the Replica Catalog, and the
          <emphasis>"site"</emphasis> attribute of the locations gives us the
          site handle for the input site.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Workflow Execution
          Directory</emphasis></para>

          <para>This is the directory created by the create dir jobs in the
          executable workflow on the Staging Site. This is a directory per
          workflow per staging site. Currently, the Staging site is always the
          Compute Site.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Worker Node Directory</emphasis></para>

          <para>This is the directory created on the worker nodes per job
          usually by the job wrapper that launches the job.</para>
        </listitem>
      </orderedlist>

      <section>
        <title>Shared File System</title>

        <para>By default Pegasus is setup to run workflows in the shared file
        system setup, where the worker nodes and the head node of a cluster
        share a filesystem.</para>

        <figure>
          <title>Shared File System Setup</title>

          <mediaobject>
            <imageobject role="html">
              <imagedata align="center" contentdepth="450px"
                         fileref="images/data-configuration-sharedfs.png"/>
            </imageobject>

            <imageobject role="fo">
              <imagedata align="center" contentdepth="4in"
                         fileref="images/data-configuration-sharedfs.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>The data flow is as follows in this case</para>

        <orderedlist>
          <listitem>
            <para>Stagein Job executes ( either on Submit Host or Head Node )
            to stage in input data from Input Sites ( 1---n) to a workflow
            specific execution directory on the shared filesystem.</para>
          </listitem>

          <listitem>
            <para>Compute Job starts on a worker node in the workflow
            execution directory. Accesses the input data using Posix IO</para>
          </listitem>

          <listitem>
            <para>Compute Job executes on the worker node and writes out
            output data to workflow execution directory using Posix IO</para>
          </listitem>

          <listitem>
            <para>Stageout Job executes ( either on Submit Host or Head Node )
            to stage out output data from the workflow specific execution
            directory to a directory on the final output site.</para>
          </listitem>
        </orderedlist>

        <tip>
          <para>Set <emphasis role="bold">
          pegasus.data.configuration</emphasis> to <emphasis role="bold">
          sharedfs</emphasis> to run in this configuration.</para>
        </tip>
      </section>

      <section>
        <title>Non Shared Filesystem</title>

        <para>In this setup , Pegasus runs workflows on local file-systems of
        worker nodes with the the worker nodes not sharing a filesystem. The
        data transfers happen between the worker node and a staging / data
        coordination site. The staging site server can be a file server on the
        head node of a cluster or can be on a separate machine.</para>

        <para><emphasis role="bold">Setup</emphasis> <itemizedlist>
            <listitem>
              <para>compute and staging site are the different</para>
            </listitem>

            <listitem>
              <para>head node and worker nodes of compute site don't share a
              filesystem</para>
            </listitem>

            <listitem>
              <para>Input Data is staged from remote sites.</para>
            </listitem>

            <listitem>
              <para>Remote Output Site i.e site other than compute site. Can
              be submit host.</para>
            </listitem>
          </itemizedlist></para>

        <figure>
          <title>Non Shared Filesystem Setup</title>

          <mediaobject id="Figure2">
            <imageobject role="html">
              <imagedata align="center" contentdepth="450px"
                         fileref="images/data-configuration-nonsharedfs.png"/>
            </imageobject>

            <imageobject role="fo">
              <imagedata align="center" contentdepth="4in"
                         fileref="images/data-configuration-nonsharedfs.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>The data flow is as follows in this case</para>

        <orderedlist>
          <listitem>
            <para>Stagein Job executes ( either on Submit Host or on staging
            site ) to stage in input data from Input Sites ( 1---n) to a
            workflow specific execution directory on the staging site.</para>
          </listitem>

          <listitem>
            <para>Compute Job starts on a worker node in a local execution
            directory. Accesses the input data using pegasus transfer to
            transfer the data from the staging site to a local directory on
            the worker node</para>
          </listitem>

          <listitem>
            <para>The compute job executes in the worker node, and executes on
            the worker node.</para>
          </listitem>

          <listitem>
            <para>The compute Job writes out output data to the local
            directory on the worker node using Posix IO</para>
          </listitem>

          <listitem>
            <para>Output Data is pushed out to the staging site from the
            worker node using pegasus-transfer.</para>
          </listitem>

          <listitem>
            <para>Stageout Job executes ( either on Submit Host or staging
            site ) to stage out output data from the workflow specific
            execution directory to a directory on the final output
            site.</para>
          </listitem>
        </orderedlist>

        <para>In this case, the compute jobs are wrapped as <link
        linkend="pegasuslite">PegasusLite</link> instances.</para>

        <para>This mode is especially useful for running in the cloud
        environments where you don't want to setup a shared filesystem between
        the worker nodes. Running in that mode is explained in detail <link
        linkend="amazon_aws">here.</link></para>

        <tip>
          <para>Set p <emphasis
          role="bold">egasus.data.configuration</emphasis> to <emphasis
          role="bold">nonsharedfs</emphasis> to run in this configuration. The
          staging site can be specified using the <emphasis
          role="bold">--staging-site</emphasis> option to pegasus-plan.</para>
        </tip>

        <para>In this setup, Pegasus always stages the input files through the
        staging site i.e the stage-in job stages in data from the input site
        to the staging site. The PegasusLite jobs that start up on the worker
        nodes, then pull the input data from the staging site for each job. In
        some cases, it might be useful to setup the PegasusLite jobs to pull
        input data directly from the input site without going through the
        staging server. This is based on the assumption that the worker nodes
        can access the input site. Starting 4.3 release, users can enable
        this. However, you should be aware that the access to the input site
        is no longer throttled ( as in case of stage in jobs). If large number
        of compute jobs start at the same time in a workflow, the input server
        will see a connection from each job.</para>

        <tip>
          <para>Set <emphasis role="bold">
          pegasus.transfer.bypass.input.staging</emphasis> to <emphasis
          role="bold">true</emphasis>to enable the bypass of staging of input
          files via the staging server.</para>
        </tip>
      </section>

      <section>
        <title>Condor Pool Without a Shared Filesystem</title>

        <para>This setup applies to a condor pool where the worker nodes
        making up a condor pool don't share a filesystem. All data IO is
        achieved using Condor File IO. This is a special case of the non
        shared filesystem setup, where instead of using pegasus-transfer to
        transfer input and output data, Condor File IO is used.</para>

        <para><emphasis role="bold">Setup</emphasis> <itemizedlist>
            <listitem>
              <para>Submit Host and staging site are same</para>
            </listitem>

            <listitem>
              <para>head node and worker nodes of compute site don't share a
              filesystem</para>
            </listitem>

            <listitem>
              <para>Input Data is staged from remote sites.</para>
            </listitem>

            <listitem>
              <para>Remote Output Site i.e site other than compute site. Can
              be submit host.</para>
            </listitem>
          </itemizedlist></para>

        <figure>
          <title>Condor Pool Without a Shared Filesystem</title>

          <mediaobject id="Figure13">
            <imageobject role="html">
              <imagedata align="center" contentdepth="450px"
                         fileref="images/data-configuration-condorio.png"/>
            </imageobject>

            <imageobject role="fo">
              <imagedata align="center" contentdepth="4in"
                         fileref="images/data-configuration-condorio.png"/>
            </imageobject>
          </mediaobject>
        </figure>

        <para>The data flow is as follows in this case</para>

        <orderedlist>
          <listitem>
            <para>Stagein Job executeson the submit host to stage in input
            data from Input Sites ( 1---n) to a workflow specific execution
            directory on the submit host</para>
          </listitem>

          <listitem>
            <para>Compute Job starts on a worker node in a local execution
            directory. Before the compute job starts, Condor transfers the
            input data for the job from the workflow execution directory on
            thesubmit host to the local execution directory on the worker
            node.</para>
          </listitem>

          <listitem>
            <para>The compute job executes in the worker node, and executes on
            the worker node.</para>
          </listitem>

          <listitem>
            <para>The compute Job writes out output data to the local
            directory on the worker node using Posix IO</para>
          </listitem>

          <listitem>
            <para>When the compute job finishes, Condor transfers the output
            data for the job from the local execution directory on the worker
            node to the workflow execution directory on the submit
            host.</para>
          </listitem>

          <listitem>
            <para>Stageout Job executes ( either on Submit Host or staging
            site ) to stage out output data from the workflow specific
            execution directory to a directory on the final output
            site.</para>
          </listitem>
        </orderedlist>

        <para>In this case, the compute jobs are wrapped as <link
        linkend="pegasuslite">PegasusLite</link> instances.</para>

        <para>This mode is especially useful for running in the cloud
        environments where you don't want to setup a shared filesystem between
        the worker nodes. Running in that mode is explained in detail <link
        linkend="amazon_aws">here.</link></para>

        <tip>
          <para>Set p <emphasis
          role="bold">egasus.data.configuration</emphasis> to <emphasis
          role="bold">condorio</emphasis> to run in this configuration. In
          this mode, the staging site is automatically set to site <emphasis
          role="bold">local</emphasis></para>
        </tip>

        <para>In this setup, Pegasus always stages the input files through the
        submit host i.e the stage-in job stages in data from the input site to
        the submit host (local site). The input data is then transferred to
        remote worker nodes from the submit host using Condor file transfers.
        In the case, where the input data is locally accessible at the submit
        host i.e the input site and the submit host are the same, then it is
        possible to bypass the creation of separate stage in jobs that copy
        the data to the workflow specific directory on the submit host.
        Instead, Condor file transfers can be setup to transfer the input
        files directly from the locally accessible input locations ( file
        URL's with "<emphasis>site</emphasis>" attribute set to local)
        specified in the replica catalog. Starting 4.3 release, users can
        enable this.</para>

        <tip>
          <para>Set <emphasis role="bold">
          pegasus.transfer.bypass.input.staging</emphasis> to <emphasis
          role="bold">true</emphasis>to bypass the creation of separate stage
          in jobs.</para>
        </tip>
      </section>
    </section>

    <section id="local_vs_remote_transfers">
      <title>Local versus Remote Transfers</title>

      <para>As far as possible, Pegasus will ensure that the transfer jobs
      added to the executable workflow are executed on the submit host. By
      default, Pegasus will schedule a transfer to be executed on the remote
      staging site only if there is no way to execute it on the submit host.
      Some scenarios where transfer jobs are executed on remote sites are as
      follows:</para>

      <itemizedlist>
        <listitem>
          <para>the file server specified for the staging site/compute site is
          a file server. In that case, Pegasus will schedule all the stage in
          data movement jobs on the compute site to stage-in the input data
          for the workflow.</para>
        </listitem>

        <listitem>
          <para>a user has symlinking turned on. In that case, the transfer
          jobs that symlink against the input data on the compute site, will
          be executed remotely ( on the compute site ).</para>
        </listitem>
      </itemizedlist>

      <para>In certain execution environments, such a local campus cluster the
      compute site and the local share a filesystem ( i.e. compute site has
      file servers specified for the staging/compute site, and the scratch and
      storage directories mentioned for the compute site are locally mounted
      on the submit host), it is beneficial to have the remote transfer jobs
      run locally and hence bypass going through the local scheduler queue. In
      that case, users can set a boolean profile auxillary.local in pegasus
      namespace in the site catalog for the compute/staging site to
      true.</para>

      <para>Users can specify the property <emphasis role="bold">
      pegasus.transfer.*.remote.sites</emphasis> to change the default
      behaviour of Pegasus and force pegasus to run different types of
      transfer jobs for the sites specified on the remote site. The value of
      the property is a comma separated list of compute sites for which you
      want the transfer jobs to run remotely.</para>

      <para>The table below illustrates all the possible variations of the
      property.</para>

      <table>
        <title>Property Variations for pegasus.transfer.*.remote.sites</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry>Property Name</entry>

              <entry>Applies to</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>pegasus.transfer.stagein.remote.sites</entry>

              <entry>the stage in transfer jobs</entry>
            </row>

            <row>
              <entry>pegasus.transfer.stageout.remote.sites</entry>

              <entry>the stage out transfer jobs</entry>
            </row>

            <row>
              <entry>pegasus.transfer.inter.remote.sites</entry>

              <entry>the inter site transfer jobs</entry>
            </row>

            <row>
              <entry>pegasus.transfer.*.remote.sites</entry>

              <entry>all types of transfer jobs</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>The prefix for the transfer job name indicates whether the
      transfer job is to be executed locallly ( on the submit host ) or
      remotely ( on the compute site ). For example stage_in_local_ in a
      transfer job name stage_in_local_isi_viz_0 indicates that the transfer
      job is a stage in transfer job that is executed locally and is used to
      transfer input data to compute site isi_viz. The prefix naming scheme
      for the transfer jobs is <emphasis role="bold">
      [stage_in|stage_out|inter]_[local|remote]_</emphasis> .</para>
    </section>

    <section id="controlling_transfer_parallelism">
      <title>Controlling Transfer Parallelism</title>

      <para>When it comes to data transfers, Pegasus ships with a default
      configuration which is trying to strike a balance between performance
      and aggressiveness. We obviously want data transfers to be as quick as
      possibly, but we also do not want our transfers to overwhelm data
      services and systems. The default configuration consists of a
      combination of the maximum number of transfer jobs per level in the
      workflow, and how many threads such a pegasus-transfer job can
      spawn.</para>

      <para>Information on how to control the number of stagein and stageout
      jobs can be found in the <link linkend="data_movement_nodes"> Data
      Movement Nodes</link> section.</para>

      <para>How to control the number of threads pegasus-transfer can use
      depends on if you want to control standard transfer jobs, or
      PegasusLite. For the former, see the <link linkend="transfer_props">
      pegasus.transfer.threads</link> property, and for the latter the <link
      linkend="transfer_props"> pegasus.transfer.lite.threads</link>
      property.</para>
    </section>

    <section id="transfer_symlink">
      <title>Symlinking Against Input Data</title>

      <para>If input data for a job already exists on a compute site, then it
      is possible for Pegasus to symlink against that data. In this case, the
      remote stage in transfer jobs that Pegasus adds to the executable
      workflow will symlink instead of doing a copy of the data.</para>

      <para>Pegasus determines whether a file is on the same site as the
      compute site, by inspecting the <emphasis>"site</emphasis>" attribute
      associated with the URL in the Replica Catalog. If the
      <emphasis>"site"</emphasis> attribute of an input file location matches
      the compute site where the job is scheduled, then that particular input
      file is a candidate for symlinking.</para>

      <para>For Pegasus to symlink against existing input data on a compute
      site, following must be true</para>

      <orderedlist>
        <listitem>
          <para>Property <emphasis role="bold">
          pegasus.transfer.links</emphasis> is set to <emphasis role="bold">
          true</emphasis></para>
        </listitem>

        <listitem>
          <para>The input file location in the Replica Catalog has the
          <emphasis>"site"</emphasis> attribute matching the compute
          site.</para>
        </listitem>
      </orderedlist>

      <tip>
        <para>To confirm if a particular input file is symlinked instead of
        being copied, look for the destination URL for that file in
        stage_in_remote*.in file. The destination URL will start with
        symlink:// .</para>
      </tip>

      <para>In the symlinking case, Pegasus strips out URL prefix from a URL
      and replaces it with a file URL.</para>

      <para>For example if a user has the following URL catalogued in the
      Replica Catalog for an input file f.input</para>

      <programlisting>f.input   gsiftp://server.isi.edu/shared/storage/input/data/f.input site="isi"</programlisting>

      <para>and the compute job that requires this file executes on a compute
      site named isi , then if symlinking is turned on the data stage in job
      (stage_in_remote_viz_0 ) will have the following source and destination
      specified for the file</para>

      <programlisting>#viz viz
file:///shared/storage/input/data/f.input  symlink://shared-scratch/workflow-exec-dir/f.input
</programlisting>
    </section>

    <section id="data_movement_nodes">
      <title>Addition of Separate Data Movement Nodes to Executable
      Workflow</title>

      <para>Pegasus relies on a Transfer Refiner that comes up with the
      strategy on how many data movement nodes are added to the executable
      workflow. All the compute jobs scheduled to a site share the same
      workflow specific directory. The transfer refiners ensure that only one
      copy of the input data is transferred to the workflow execution
      directory. This is to prevent data clobbering . Data clobbering can
      occur when compute jobs of a workflow share some input files, and have
      different stage in transfer jobs associated with them that are staging
      the shared files to the same destination workflow execution
      directory.</para>

      <para>Pegasus supports three different transfer refiners that dictate
      how the stagein and stageout jobs are added for the workflow.The default
      Transfer Refiner used in Pegasus is the BalancedCluster Refiner.
      Starting 4.8.0 release, the default configuration of Pegasus now adds
      transfer jobs and cleanup jobs based on the number of jobs at a
      particular level of the workflow. For example, for every 10 compute jobs
      on a level of a workflow, one data transfer job( stage-in and stage-out)
      is created.</para>

      <para>The transfer refiners also allow the user to specify how many
      local|remote stagein|stageout jobs are created per execution
      site.</para>

      <para>The behavior of the refiners (BalancedCluster and Cluster) are
      controlled by specifying certain pegasus profiles</para>

      <orderedlist>
        <listitem>
          <para>either with the execution sites in the site catalog</para>
        </listitem>

        <listitem>
          <para>OR globally in the properties file</para>
        </listitem>
      </orderedlist>

      <table>
        <title>Pegasus Profile Keys For the Cluster Transfer Refiner</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry>Profile Key</entry>

              <entry>Description</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>stagein.clusters</entry>

              <entry>This key determines the maximum number of stage-in jobs
              that are can executed locally or remotely per compute site per
              workflow.</entry>
            </row>

            <row>
              <entry>stagein.local.clusters</entry>

              <entry>This key provides finer grained control in determining
              the number of stage-in jobs that are executed locally and are
              responsible for staging data to a particular remote
              site.</entry>
            </row>

            <row>
              <entry>stagein.remote.clusters</entry>

              <entry>This key provides finer grained control in determining
              the number of stage-in jobs that are executed remotely on the
              remote site and are responsible for staging data to it.</entry>
            </row>

            <row>
              <entry>stageout.clusters</entry>

              <entry>This key determines the maximum number of stage-out jobs
              that are can executed locally or remotely per compute site per
              workflow.</entry>
            </row>

            <row>
              <entry>stageout.local.clusters</entry>

              <entry>This key provides finer grained control in determining
              the number of stage-out jobs that are executed locally and are
              responsible for staging data from a particular remote
              site.</entry>
            </row>

            <row>
              <entry>stageout.remote.clusters</entry>

              <entry>This key provides finer grained control in determining
              the number of stage-out jobs that are executed remotely on the
              remote site and are responsible for staging data from
              it.</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <tip>
        <para>Which transfer refiner to use is controlled by property
        pegasus.transfer.refiner</para>
      </tip>

      <section id="transfer-refiner-balanced-cluster">
        <title>BalancedCluster</title>

        <para>This is a new transfer refiner that was introduced in Pegasus
        4.4.0 and is the default one used in Pegasus. It does a round robin
        distribution of the files amongst the stagein and stageout jobs per
        level of the workflow. The figure below illustrates the behavior of
        this transfer refiner.</para>

        <figure>
          <title>BalancedCluster Transfer Refiner : Input Data To Workflow
          Specific Directory on Shared File System</title>

          <mediaobject id="img-balanced-cluster-transfer-refiner">
            <imageobject role="html">
              <imagedata align="center" contentdepth="650px"
                         fileref="images/balanced-cluster-transfer-refiner.png"/>
            </imageobject>

            <imageobject role="fo">
              <imagedata align="center" contentdepth="4in"
                         fileref="images/balanced-cluster-transfer-refiner.png"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Cluster</title>

        <para>This transfer refiner is similar to BalancedCluster but differs
        in the way how distribution of files happen across stagein and
        stageout jobs per level of the workflow. In this refiner, all the
        input files for a job get associated with a single transfer job. As
        illustrated in the figure below each compute usually gets associated
        with one stagein transfer job. In contrast, for the BalancedCluster a
        compute job maybe associated with multiple data stagein jobs.</para>

        <figure>
          <title>Cluster Transfer Refiner : Input Data To Workflow Specific
          Directory on Shared File System</title>

          <mediaobject id="img-cluster-transfer-refiner">
            <imageobject role="html">
              <imagedata align="center" contentdepth="650px"
                         fileref="images/cluster-transfer-refiner.png"/>
            </imageobject>

            <imageobject role="fo">
              <imagedata align="center" contentdepth="4in"
                         fileref="images/cluster-transfer-refiner.png"/>
            </imageobject>
          </mediaobject>
        </figure>
      </section>

      <section>
        <title>Basic</title>

        <para>Pegasus also supports a basic Transfer Refiner that adds one
        stagein and stageout job per compute job of the workflow. This is not
        recommended to be used for large workflows as the number of data
        transfer nodes in the worst case are 2n where n is the number of
        compute jobs in the workflow.</para>
      </section>
    </section>

    <section id="transfer_client">
      <title>Executable Used for Transfer and Cleanup Jobs</title>

      <para>Pegasus refers to a python script called <emphasis role="bold">
      pegasus-transfer</emphasis> as the executable in the transfer jobs to
      transfer the data. pegasus-transfer looks at source and destination url
      and figures out automatically which underlying client to use.
      pegasus-transfer is distributed with the PEGASUS and can be found at
      $PEGASUS_HOME/bin/pegasus-transfer.</para>

      <para>Currently, pegasus-transfer interfaces with the following transfer
      clients</para>

      <table>
        <title>Transfer Clients interfaced to by pegasus-transfer</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry>Transfer Client</entry>

              <entry>Used For</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>gfal-copy</entry>

              <entry>staging file to and from GridFTP servers</entry>
            </row>

            <row>
              <entry>globus-url-copy</entry>

              <entry>staging files to and from GridFTP servers, only if gfal
              is not detected in the path.</entry>
            </row>

            <row>
              <entry>gfal-copy</entry>

              <entry>staging files to and from SRM or XRootD servers</entry>
            </row>

            <row>
              <entry>wget</entry>

              <entry>staging files from a HTTP server</entry>
            </row>

            <row>
              <entry>cp</entry>

              <entry>copying files from a POSIX filesystem</entry>
            </row>

            <row>
              <entry>ln</entry>

              <entry>symlinking against input files</entry>
            </row>

            <row>
              <entry>pegasus-s3</entry>

              <entry>staging files to and from S3 buckets in Amazon Web
              Services</entry>
            </row>

            <row>
              <entry>gsutil</entry>

              <entry>staging files to and from Google Storage buckets</entry>
            </row>

            <row>
              <entry>scp</entry>

              <entry>staging files using scp</entry>
            </row>

            <row>
              <entry>gsiscp</entry>

              <entry>staging files using gsiscp and X509</entry>
            </row>

            <row>
              <entry>iget</entry>

              <entry>staging files to and from iRODS servers</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>For remote sites, Pegasus constructs the default path to
      pegasus-transfer on the basis of PEGASUS_HOME env profile specified in
      the site catalog. To specify a different path to the pegasus-transfer
      client , users can add an entry into the transformation catalog with
      fully qualified logical name as <emphasis
      role="bold">pegasus::pegasus-transfer</emphasis></para>

      <section>
        <title>Preference of GFAL over GUC</title>

        <para>JGlobus is no longer actively supported and is not inÂ compliance
        with <ulink
        url="https://docs.globus.org/security-bulletins/2015-12-strict-mode">RFC
        2818</ulink> . As a result cleanup jobs using pegasus-gridftp client
        would fail against the servers supporting the strict mode. We have
        removed the pegasus-gridftp client and now use gfal clients as
        globus-url-copy does not support removes. If gfal is not available,
        globus-url-copy is used for cleanup by writing out zero bytes files
        instead of removing them.</para>
      </section>
    </section>

    <section>
      <title>Staging of Executables</title>

      <para>Users can get Pegasus to stage the user executables ( executables
      that the jobs in the DAX refer to ) as part of the transfer jobs to the
      workflow specific execution directory on the compute site. The URL
      locations of the executables need to be specified in the transformation
      catalog as the PFN and the type of executable needs to be set to
      <emphasis role="bold"> STAGEABLE</emphasis> .</para>

      <para>The location of a transformation can be specified either in</para>

      <itemizedlist>
        <listitem>
          <para>DAX in the executables section. More details <link
          linkend="dax_transformation_catalog">here</link> .</para>
        </listitem>

        <listitem>
          <para>Transformation Catalog. More details <link
          linkend="transformation">here</link> .</para>
        </listitem>
      </itemizedlist>

      <para>A particular transformation catalog entry of type STAGEABLE is
      compatible with a compute site only if all the System Information
      attributes associated with the entry match with the System Information
      attributes for the compute site in the Site Catalog. The following
      attributes make up the System Information attributes</para>

      <orderedlist>
        <listitem>
          <para>arch</para>
        </listitem>

        <listitem>
          <para>os</para>
        </listitem>

        <listitem>
          <para>osrelease</para>
        </listitem>

        <listitem>
          <para>osversion</para>
        </listitem>
      </orderedlist>

      <section>
        <title>Transformation Mappers</title>

        <para>Pegasus has a notion of transformation mappers that determines
        what type of executables are picked up when a job is executed on a
        remote compute site. For transfer of executables, Pegasus constructs a
        soft state map that resides on top of the transformation catalog, that
        helps in determining the locations from where an executable can be
        staged to the remote site.</para>

        <para>Users can specify the following property to pick up a specific
        transformation mapper</para>

        <programlisting><emphasis role="bold">pegasus.catalog.transformation.mapper</emphasis> </programlisting>

        <para>Currently, the following transformation mappers are
        supported.</para>

        <table>
          <title>Transformation Mappers Supported in Pegasus</title>

          <tgroup cols="2">
            <thead>
              <row>
                <entry>Transformation Mapper</entry>

                <entry>Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>Installed</entry>

                <entry>This mapper only relies on transformation catalog
                entries that are of type INSTALLED to construct the soft state
                map. This results in Pegasus never doing any transfer of
                executables as part of the workflow. It always prefers the
                installed executables at the remote sites</entry>
              </row>

              <row>
                <entry>Staged</entry>

                <entry>This mapper only relies on matching transformation
                catalog entries that are of type STAGEABLE to construct the
                soft state map. This results in the executable workflow
                referring only to the staged executables, irrespective of the
                fact that the executables are already installed at the remote
                end</entry>
              </row>

              <row>
                <entry>All</entry>

                <entry>This mapper relies on all matching transformation
                catalog entries of type STAGEABLE or INSTALLED for a
                particular transformation as valid sources for the transfer of
                executables. This the most general mode, and results in the
                constructing the map as a result of the cartesian product of
                the matches.</entry>
              </row>

              <row>
                <entry>Submit</entry>

                <entry>This mapper only on matching transformation catalog
                entries that are of type STAGEABLE and reside at the submit
                host (site local), are used while constructing the soft state
                map. This is especially helpful, when the user wants to use
                the latest compute code for his computations on the grid and
                that relies on his submit host.</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>
    </section>

    <section id="transfer_worker_package_staging">
      <title>Staging of Pegasus Worker Package</title>

      <para>Pegasus can optionally stage the pegasus worker package as part of
      the executable workflow to remote workflow specific execution directory.
      The pegasus worker package contains the pegasus auxillary executables
      that are required on the remote site. If the worker package is not
      staged as part of the executable workflow, then Pegasus relies on the
      installed version of the worker package on the remote site. To determine
      the location of the installed version of the worker package on a remote
      site, Pegasus looks for an environment profile PEGASUS_HOME for the site
      in the Site Catalog.</para>

      <para>Users can set the following property to true to turn on worker
      package staging</para>

      <programlisting><emphasis role="bold">pegasus.transfer.worker.package          true</emphasis> </programlisting>

      <para>Starting 4.6.0 release, the worker package used for a remote site
      ( if required ) is determined in the following order</para>

      <itemizedlist>
        <listitem>
          <para>there is an entry for pegasus::worker executable in the
          transformation catalog. Information on how to construct that entry
          is provided below.</para>
        </listitem>

        <listitem>
          <para>the planner at runtime creates a worker package out of the
          binary installation on the pegasus website, and puts it in the
          submit directory. It uses this worker package if the OS and
          architecture of the created worker package match with remote site,
          or there is an exact match with ( osrelease and osversion) if
          specified by the user in the site catalog for the remote
          site.</para>
        </listitem>

        <listitem>
          <para>the worker package compatible with the remote site is used
          from the Pegasus website.</para>
        </listitem>
      </itemizedlist>

      <para>For users to specify a particular worker package to use, they can
      specify the transformation <emphasis
      role="bold">pegasus::worker</emphasis> in the transformation catalog
      with</para>

      <itemizedlist>
        <listitem>
          <para>type set to STAGEABLE</para>
        </listitem>

        <listitem>
          <para>System Information attributes of the transformation catalog
          entry match the System Information attributes of the compute
          site.</para>
        </listitem>

        <listitem>
          <para>the PFN specified should be a remote URL that can be pulled to
          the compute site.</para>
        </listitem>
      </itemizedlist>

      <programlisting># example of specifying a worker package in the transformation catalog
tr pegasus::worker {
site corbusier {
    pfn "http://pegasus.isi.edu/pegasus/download?filename=4.5.2/pegasus-binary-4.5.2-x86_64_macos_10.tar.gz"
    arch "x86_64"
    os "MACOSX"
    type "INSTALLED"
  }
}

</programlisting>

      <section>
        <title>Worker Package Staging in Non Shared Filesystem setup</title>

        <para>Worker package staging is automatically set to true , when
        workflows are setup to run in a non shared filesystem setup i.e.
        <emphasis role="bold">pegasus.data.configuration</emphasis> is set to
        <emphasis role="bold">nonsharedfs</emphasis> or <emphasis
        role="bold">condorio</emphasis> . In these configurations, a
        stage_worker job is created that brings in the worker package to the
        submit directory of the workflow. For each job, the worker package is
        then transferred with the job using Condor File Transfers ( <emphasis
        role="bold">transfer_input_files</emphasis> ) . This transfer always
        happens unless, PEGASUS_HOME is specified in the site catalog for the
        site on which the job is scheduled to run.</para>

        <para>Users can explicitly set the following property to false, to
        turn off worker package staging by the Planner. This is applicable ,
        when running in the cloud and virtual machines / worker nodes already
        have the pegasus worker tools installed.</para>

        <programlisting><emphasis role="bold">pegasus.transfer.worker.package          false</emphasis> </programlisting>
      </section>
    </section>

    <xi:include href="containers.xml" xpointer="container-transfers"
                xmlns:xi="http://www.w3.org/2001/XInclude"/>

    <section id="staging_job_checkpoint_files">
      <title>Staging of Job Checkpoint Files</title>

      <para>Pegasus has support for transferring job checkpoint files back to
      the staging site, when a job exceeds it's advertised running time. In
      order to use this feature, you need to</para>

      <orderedlist>
        <listitem>
          <para>Associate a job checkpoint file ( that the job creates ) with
          the job in the DAX. A checkpoint file is specified by setting the
          link attribute to checkpoint for the uses tag.</para>
        </listitem>

        <listitem>
          <para>Associate a Pegasus profile key named <emphasis role="bold">
          checkpoint.time</emphasis> is the time in minutes after which a job
          is sent the TERM signal by pegasus-kickstart, telling it to create
          the checkpoint file.</para>
        </listitem>

        <listitem>
          <para>Associate a Pegasus profile key named <emphasis role="bold">
          maxwalltime</emphasis> with the job that specifies the max runtime
          in minutes before the job will be killed by the local resource
          manager ( such as PBS) deployed on the site. Usually, this value
          should be associated with the execution site in the site
          catalog.</para>
        </listitem>
      </orderedlist>

      <para>Pegasus planner uses the above mentioned profile keys to setup
      pegasus-kickstart such that the job is sent a TERM signal when the
      checkpoint time of job is reached. A KILL signal is sent at
      (checkpoint.time + (maxwalltime-checkpoint.time)/2) minutes. This
      ensures that there is enough time for pegasus-lite to transfer the
      checkpoint file before the job is killed by the underlying
      scheduler.</para>
    </section>

    <section>
      <title>Using Amazon S3 as a Staging Site</title>

      <para>Pegasus can be configured to use Amazon S3 as a staging site. In
      this mode, Pegasus transfers workflow inputs from the input site to S3.
      When a job runs, the inputs for that job are fetched from S3 to the
      worker node, the job is executed, then the output files are transferred
      from the worker node back to S3. When the jobs are complete, Pegasus
      transfers the output data from S3 to the output site.</para>

      <para>In order to use S3, it is necessary to create a config file for
      the S3 transfer client, <link linkend="cli-pegasus-s3">
      pegasus-s3</link>. See the <link linkend="cli-pegasus-s3">man
      page</link> for details on how to create the config file. You also need
      to specify <link linkend="non_shared_fs">S3 as a staging
      site</link>.</para>

      <para>Next, you need to modify your site catalog to tell the location of
      your s3cfg file. See <link linkend="cred_staging">the section on
      credential staging</link>.</para>

      <para>The following site catalog shows how to specify the location of
      the s3cfg file on the local site and how to specify an Amazon S3 staging
      site:</para>

      <programlisting>&lt;sitecatalog xmlns="http://pegasus.isi.edu/schema/sitecatalog"
             xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
             xsi:schemaLocation="http://pegasus.isi.edu/schema/sitecatalog
             http://pegasus.isi.edu/schema/sc-3.0.xsd" version="3.0"&gt;
    &lt;site handle="local" arch="x86_64" os="LINUX"&gt;
        &lt;head-fs&gt;
            &lt;scratch&gt;
                &lt;shared&gt;
                    &lt;file-server protocol="file" url="file://" mount-point="/tmp/wf/work"/&gt;
                    &lt;internal-mount-point mount-point="/tmp/wf/work"/&gt;
                &lt;/shared&gt;
            &lt;/scratch&gt;
            &lt;storage&gt;
                &lt;shared&gt;
                    &lt;file-server protocol="file" url="file://" mount-point="/tmp/wf/storage"/&gt;
                    &lt;internal-mount-point mount-point="/tmp/wf/storage"/&gt;
                &lt;/shared&gt;
            &lt;/storage&gt;
        &lt;/head-fs&gt;
        <emphasis role="bold">&lt;profile namespace="env" key="S3CFG"&gt;/home/username/.s3cfg&lt;/profile&gt;</emphasis>
    &lt;/site&gt;
    <emphasis role="bold">&lt;site handle="s3" arch="x86_64" os="LINUX"&gt;
        &lt;head-fs&gt;
            &lt;scratch&gt;
                &lt;shared&gt;
                    &lt;!-- wf-scratch is the name of the S3 bucket that will be used --&gt;
                    &lt;file-server protocol="s3" url="s3://user@amazon" mount-point="/wf-scratch"/&gt;
                    &lt;internal-mount-point mount-point="/wf-scratch"/&gt;
                &lt;/shared&gt;
            &lt;/scratch&gt;
        &lt;/head-fs&gt;
    &lt;/site&gt;</emphasis>
    &lt;site handle="condorpool" arch="x86_64" os="LINUX"&gt;
        &lt;head-fs&gt;
            &lt;scratch/&gt;
            &lt;storage/&gt;
        &lt;/head-fs&gt;
        &lt;profile namespace="pegasus" key="style"&gt;condor&lt;/profile&gt;
        &lt;profile namespace="condor" key="universe"&gt;vanilla&lt;/profile&gt;
        &lt;profile namespace="condor" key="requirements"&gt;(Target.Arch == "X86_64")&lt;/profile&gt;
    &lt;/site&gt;
&lt;/sitecatalog&gt;
</programlisting>
    </section>

    <section id="transfer_irods">
      <title>iRODS data access</title>

      <para>iRODS can be used as a input data location, a storage site for
      intermediate data during workflow execution, or a location for final
      output data. Pegasus uses a URL notation to identify iRODS files.
      Example:</para>

      <programlisting>irods://some-host.org/path/to/file.txt</programlisting>

      <para>The path to the file is <emphasis role="bold"> relative</emphasis>
      to the internal iRODS location. In the example above, the path used to
      refer to the file in iRODS is <emphasis> path/to/file.txt</emphasis> (no
      leading /).</para>

      <para>See <link linkend="cred_staging">the section on credential
      staging</link> for information on how to set up an irodsEnv file to be
      used by Pegasus.</para>
    </section>

    <section id="transfer_gridftp_ssh">
      <title>GridFTP over SSH (sshftp)</title>

      <para>Instead of using X.509 based security, newer version of Globus
      GridFTP can be configured to set up transfers over SSH. See the <ulink
      url="http://toolkit.globus.org/toolkit/docs/6.0/gridftp/admin/#gridftp-admin-config-security-sshftp">Globus
      Documentation</ulink> for details on installing and setting up this
      feature.</para>

      <para>Pegasus requires the ability to specify which SSH key to be used
      at runtime, and thus a small modification is necessary to the default
      Globus configuration. On the hosts where Pegasus initiates transfers
      (which depends on the data configuration of the workflow), please
      replace <emphasis>gridftp-ssh</emphasis>, usually located under
      <emphasis>/usr/share/globus/gridftp-ssh</emphasis>, with:</para>

      <programlisting>
#!/bin/bash

url_string=$1
remote_host=$2
port=$3
user=$4

port_str=""
if  [ "X" = "X$port" ]; then
    port_str=""
else
    port_str=" -p $port "
fi

if  [ "X" != "X$user" ]; then
    remote_host="$user@$remote_host"
fi

remote_default1=.globus/sshftp
remote_default2=/etc/grid-security/sshftp
remote_fail="echo -e 500 Server is not configured for SSHFTP connections.\\\r\\\n"
remote_program=$GLOBUS_REMOTE_SSHFTP
if  [ "X" = "X$remote_program" ]; then
    remote_program="(( test -f $remote_default1 &amp;&amp; $remote_default1 ) || ( test -f $remote_default2 &amp;&amp; $remote_default2 ) || $remote_fail )"
fi

if [ "X" != "X$GLOBUS_SSHFTP_PRINT_ON_CONNECT" ]; then
    echo "Connecting to $1 ..." &gt;/dev/tty
fi

# for pegasus-transfer
extra_opts=" -o StrictHostKeyChecking=no"
if [ "x$SSH_PRIVATE_KEY" != "x" ]; then
    extra_opts="$extra_opts -i $SSH_PRIVATE_KEY"
fi

exec /usr/bin/ssh $extra_opts $port_str $remote_host $remote_program
</programlisting>

      <para>Once configured, you should be able to use URLs such as
      <emphasis>sshftp://username@host/foo/bar.txt</emphasis> in your
      workflows.</para>
    </section>

    <section id="transfer_globus_online">
      <title>Globus Online</title>

      <para><ulink url="http://globus.org">Globus Online</ulink> is a transfer
      service with features such as policy based connection management and
      automatic failure detection and recovery. Pegasus has limited the
      support for Globus Online transfers.</para>

      <para>If you want to use Globus Online in your workflow, all data has to
      be accessible via a Globus Online endpoint. You can not mix Globus
      Online endpoints with other protocols. For most users, this means they
      will have to create an endpoint for their submit host and probably
      modify both the replica catalog and DAX generator so that all URLs in
      the workflow are for Globus Online endpoints.</para>

      <para>There are two levels of credentials required. One is for the
      workflow to use the Globus Online API, which is handled by a X.509
      proxy. The second level is for the endpoints, which the user will have
      have manage via the Globus Online web interface. The required steps
      are:</para>

      <orderedlist>
        <listitem>
          <para>Using the <ulink url="https://docs.globus.org/cli/">CLI
          interface</ulink>, update your user profile, and add your X.509
          DN.</para>
        </listitem>

        <listitem>
          <para>In the Globus Online web interface, under Endpoints, find the
          endpoints you need for the workflow, and activate them. Note that
          you should activate them for the whole duration of the workflow or
          you will have to regularly log in and re-activate the endpoints
          during workflow execution.</para>
        </listitem>
      </orderedlist>

      <para>URLs for Globus Online endpoint data follows the following scheme:
      <emphasis>go://[globus_username]@[endpoint]/[path]</emphasis>. For
      example, for a user with the Globus Online username
      <emphasis>bob</emphasis> and his private endpoint
      <emphasis>bob#researchdata</emphasis> and a file
      <emphasis>/home/bsmith/experiment/1.dat</emphasis>, the URL would be:
      <emphasis>go://bob@bob#researchdata/home/bsmith/experiment/1.dat</emphasis></para>
    </section>
  </section>

  <section id="cred_staging">
    <title>Credentials Management</title>

    <para>Pegasus tries to do data staging from localhost by default, but some
    data scenarios makes some <link linkend="local_vs_remote_transfers">remote
    jobs do data staging</link>. An example of such a case is when running in
    <link linkend="ref_data_staging_configuration">nonsharedfs</link> mode.
    Depending on the transfer protocols used, the job may have to carry
    credentials to enable these data transfers. To specify where which
    credential to use and where Pegasus can find it, use environment variable
    profiles in your site catalog. The supported credential types are X.509
    grid proxies, Amazon AWS S3 keys, Google Cloud Platform OAuth token (.boto
    file), iRods password and SSH keys.</para>

    <para>Credentials are usually associated per site in the site catalog.
    Users can associate the credentials either as a Pegasus profile or an
    environment profile with the site.</para>

    <orderedlist>
      <listitem>
        <para>A pegasus profile with the value pointing to the path to the
        credential on the local site or the submit host. If a pegasus
        credential profile associated with the site, then Pegasus
        automatically transfers it along with the remote jobs.</para>
      </listitem>

      <listitem>
        <para>A env profile with the value pointing to the path to the
        credential on the remote site. If an env profile is specified, then no
        credential is transferred along with the job. Instead the job's
        environment is set to ensure that the job picks up the path to the
        credential on the remote site.</para>
      </listitem>
    </orderedlist>

    <tip>
      <para>Specifying credentials as Pegasus profiles was introduced in 4.4.0
      release.</para>
    </tip>

    <para>In case of data transfer jobs, it is possible to associate different
    credentials for a single file transfer ( one for the source server and the
    other for the destination server) . For example, when leveraging GridFTP
    transfers between two sides that accept different grid credentials such as
    XSEDE Stampede site and NCSA Bluewaters. In that case, Pegasus picks up
    the associated credentials from the site catalog entries for the source
    and the destination sites associated with the transfer.</para>

    <section id="x509_cred">
      <title>X.509 Grid Proxies</title>

      <para>If the grid proxy is required by transfer jobs, and the proxy is
      in the standard location, Pegasus will pick the proxy up automatically.
      For non-standard proxy locations, you can use the
      <varname>X509_USER_PROXY</varname> environment variable. Site catalog
      example:</para>

      <programlisting>&lt;profile namespace="pegasus" key="X509_USER_PROXY" &gt;/some/location/x509up&lt;/profile&gt;</programlisting>
    </section>

    <section id="s3_cred">
      <title>Amazon AWS S3</title>

      <para>If a workflow is using s3 URLs, Pegasus has to be told where to
      find the .s3cfg file. This format of the file is described in the <link
      linkend="cli-pegasus-s3">pegaus-s3 command line client's man
      page</link>. For the file to be picked up by the workflow, set the
      <varname>S3CFG</varname> profile to the location of the file. Site
      catalog example:</para>

      <programlisting>&lt;profile namespace="pegasus" pegasus="S3CFG" &gt;/home/user/.s3cfg&lt;/profile&gt;</programlisting>
    </section>

    <section id="gs_cred">
      <title>Google Storage</title>

      <para>If a workflow is using gs:// URLs, Pegasus needs access to a
      Google Storage service account. First generate the credential by
      following the instructions at:</para>

      <para><ulink
      url="https://cloud.google.com/storage/docs/authentication#service_accounts">https://cloud.google.com/storage/docs/authentication#service_accounts</ulink></para>

      <para>Download the credential in PKCS12 format, and then use "gsutil
      config -e" to generate a .boto file. For example:</para>

      <programlisting>
$ gsutil config -e
This command will create a boto config file at /home/username/.boto
containing your credentials, based on your responses to the following
questions.
What is your service account email address? some-identifier@developer.gserviceaccount.com
What is the full path to your private key file? /home/username/my-cred.p12
What is the password for your service key file [if you haven't set one
explicitly, leave this line blank]? 

Please navigate your browser to https://cloud.google.com/console#/project,
then find the project you will use, and copy the Project ID string from the
second column. Older projects do not have Project ID strings. For such projects,
click the project and then copy the Project Number listed under that project.

What is your project-id? your-project-id

Boto config file "/home/username/.boto" created. If you need to use a
proxy to access the Internet please see the instructions in that file.
            </programlisting>

      <para>Pegasus has to be told where to find both the .boto file as well
      as the PKCS12 file. For the files to be picked up by the workflow, set
      the <varname>BOTO_CONFIG</varname> and <varname>GOOGLE_PKCS12</varname>
      profiles for the storage site. Site catalog example:</para>

      <programlisting>
&lt;profile namespace="pegasus" key="BOTO_CONFIG" &gt;/home/user/.boto&lt;/profile&gt;
&lt;profile namespace="pegasus" key="GOOGLE_PKCS12" &gt;/home/user/.google-service-account.p12&lt;/profile&gt;
</programlisting>
    </section>

    <section id="irods_cred">
      <title>iRods Password</title>

      <para>If a workflow is using iRods URLs, Pegasus has to be given an
      irodsEnv file. It is a standard file, with the addtion of an password
      attribute. Example when using iRods 3.X:</para>

      <programlisting># iRODS personal configuration file.
#
# iRODS server host name:
irodsHost 'some.host.edu'
# iRODS server port number:
irodsPort 1259

# Account name:
irodsUserName 'someuser'
# Zone:
irodsZone 'somezone' 

# this is used with Pegasus
irodsPassword 'somesecretpassword'</programlisting>

      <para>iRods 4.0 switched to a JSON based configuration file. Pegasus can
      handle either config file. JSON Example:</para>

      <programlisting>
{
    "irods_host": "some.host.edu",
    "irods_port": 1247,
    "irods_user_name": "someuser",
    "irods_zone_name": "somezone",
    "irodsPassword" : "somesecretpassword"
}
</programlisting>

      <para>The location of the file can be given to the workflow using the
      <varname>irodsEnvFile</varname> environment profile. Site catalog
      example:</para>

      <programlisting>&lt;profile namespace="pegasus" key="irodsEnvFile" &gt;/home/user/.irods/.irodsEnv&lt;/profile&gt;</programlisting>
    </section>

    <section id="ssh_cred">
      <title>SSH Keys</title>

      <para>New in Pegasus 4.0 is the support for data staging with scp using
      ssh public/private key authentication. In this mode, Pegasus transports
      a private key with the jobs. The storage machines will have to have the
      public part of the key listed in ~/.ssh/authorized_keys.</para>

      <warning>
        <para>SSH keys should be handled in a secure manner. In order to keep
        your personal ssh keys secure, It is recommended that a special set of
        keys are created for use with the workflow. Note that Pegasus will not
        pick up ssh keys automatically. The user will have to specify which
        key to use with <varname> SSH_PRIVATE_KEY</varname>.</para>
      </warning>

      <para>The location of the ssh private key can be specified with the
      <varname>SSH_PRIVATE_KEY</varname> environment profile. Site catalog
      example:</para>

      <programlisting>&lt;profile namespace="pegasus" key="SSH_PRIVATE_KEY" &gt;/home/user/wf/wfsshkey&lt;/profile&gt;</programlisting>
    </section>
  </section>

  <section id="ref_staging_mapper">
    <title>Staging Mappers</title>

    <para>Starting 4.7 release, Pegasus has support for staging mappers in the
    <emphasis role="bold">nonsharedfs</emphasis> data configuration. The
    staging mappers determine what sub directory on the staging site a job
    will be associated with. Before, the introduction of staging mappers, all
    files associated with the jobs scheduled for a particular site landed in
    the same directory on the staging site. As a result, for large workflows
    this could degrade filesystem performance on the staging servers.</para>

    <para>To configure the staging mapper, you need to specify the following
    property</para>

    <programlisting>pegasus.dir.staging.mapper  &lt;name of the mapper to use&gt;</programlisting>

    <para>The following mappers are supported currently, with Hashed being the
    default .</para>

    <orderedlist>
      <listitem>
        <para><emphasis role="bold">Flat</emphasis> : This mapper results in
        Pegasus placing all the job submit files in the staging site directory
        as determined from the Site Catalog and planner options. This can
        result in too many files in one directory for large workflows, and was
        the only option before Pegasus 4.7.0 release.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Hashed</emphasis> : This mapper results in
        the creation of a deep directory structure rooted at the staging site
        directory created by the create dir jobs. The binning is at the job
        level, and not at the file level i.e each job will push out it's
        outputs to the same directory on the staging site, independent of the
        number of output files. To control behavior of this mapper, users can
        specify the following properties</para>

        <screen>pegasus.dir.staging.mapper.hashed.levels     the number of directory levels used to accomodate the files. Defaults to 2.
pegasus.dir.staging.mapper.hashed.multiplier the number of files associated with a job in the submit directory. defaults to 5.
</screen>
      </listitem>
    </orderedlist>

    <note>
      <para>The staging mappers are only triggered if
      pegasus.data.configuration is set to nonsharedfs</para>
    </note>
  </section>

  <section id="ref_output_mapper">
    <title>Output Mappers</title>

    <para>Starting 4.3 release, Pegasus has support for output mappers, that
    allow users fine grained control over how the output files on the output
    site are laid out. By default, Pegasus stages output products to the
    storage directory specified in the site catalog for the output site.
    Output mappers allow users finer grained control over where the output
    files are placed on the output site.</para>

    <para>To configure the output mapper, you need to specify the following
    property</para>

    <programlisting>pegasus.dir.storage.mapper  &lt;name of the mapper to use&gt;</programlisting>

    <para>The following mappers are supported currently</para>

    <orderedlist>
      <listitem>
        <para><emphasis role="bold">Flat</emphasis> : By default, Pegasus will
        place the output files in the storage directory specified in the site
        catalog for the output site.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Fixed</emphasis> : This mapper allows
        users to specify an externally accesible url to the storage directory
        in their properties file. To use this mapper, the following property
        needs to be set.</para>

        <itemizedlist>
          <listitem>
            <para>pegasus.dir.storage.mapper.fixed.url an externally
            accessible URL to the storage directory on the output site e.g.
            gsiftp://outputs.isi.edu/shared/outputs</para>
          </listitem>
        </itemizedlist>

        <para>Note: For hierarchal workflows, the above property needs to be
        set separately for each dax job, if you want the sub workflow outputs
        to goto a different directory.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Hashed</emphasis> : This mapper results in
        the creation of a deep directory structure on the output site, while
        populating the results. The base directory on the remote end is
        determined from the site catalog. Depending on the number of files
        being staged to the remote site a Hashed File Structure is created
        that ensures that only 256 files reside in one directory. To create
        this directory structure on the storage site, Pegasus relies on the
        directory creation feature of the underlying file servers such as
        theGrid FTP server, which appeared in globus 4.0.x</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Replica:</emphasis> This mapper determines
        the path for an output file on the output site by querying an output
        replica catalog. The output site is one that is passed on the command
        line. The output replica catalog can be configured by specifying the
        following properties.</para>

        <itemizedlist>
          <listitem>
            <para>pegasus.dir.storage.mapper.replica Regex|File</para>
          </listitem>

          <listitem>
            <para>pegasus.dir.storage.mapper.replica.file the RC file at the
            backend to use</para>
          </listitem>
        </itemizedlist>

        <para>Please note that the output replica catalog ( even though the
        formats are the same) is logically different from the input replica
        catalog, where you specify the locations for the input files. You
        cannot specify the locations for the output files to be used by the
        mapper in the DAX. The format for the File based replica catalog is
        described <link linkend="rc-FILE">here</link>, while for the Regex it
        is <link linkend="rc-regex">here</link>.</para>
      </listitem>
    </orderedlist>

    <section>
      <title>Effect of pegasus.dir.storage.deep</title>

      <para>For Flat and Hashed output mappers, the base directory to which
      the add on component is added is determined by the property
      pegasus.dir.storage.deep . The output directory on the output site is
      determined from the site catalog.</para>

      <para>If pegasus.dir.storage.deep is set to true, then to this base
      directory, a relative directory is appended i.e. $storage_base = $base +
      $relative_directory. The relative directory is computed on the basis of
      the --relative-dir option. If that is not set, then defaults to the
      relative submit directory for the workflow ( usually
      $user/$vogroup/$label/runxxxx ).This is the base directory that is
      passed to the storage mappers.</para>
    </section>
  </section>

  <section id="data_cleanup">
    <title>Data Cleanup</title>

    <para>When executing large workflows, users often may run out of diskspace
    on the remote clusters / staging site. Pegasus provides a couple of ways
    of enabling automated data cleanup on the staging site ( i.e the scratch
    space used by the workflows). This is achieved by adding data cleanup jobs
    to the executable workflow that the Pegasus Mapper generates. These
    cleanup jobs are responsible for removing files and directories during the
    workflow execution. To enable data cleanup you can pass the --cleanup
    option to pegasus-plan . The value passed decides the cleanup strategy
    implemented</para>

    <orderedlist>
      <listitem>
        <para><emphasis role="bold">none </emphasis> disables cleanup
        altogether. The planner does not add any cleanup jobs in the
        executable workflow whatsoever.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">leaf</emphasis> the planner adds a leaf
        cleanup node per staging site that removes the directory created by
        the create dir job in the workflow</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">inplace</emphasis> the mapper adds cleanup
        nodes per level of the workflow in addition to leaf cleanup nodes. The
        nodes remove files no longer required during execution. For example,
        an added cleanup node will remove input files for a particular compute
        job after the job has finished successfully. Starting 4.8.0 release,
        the number of cleanup nodes created by this algorithm on a particular
        level, is dictated by the number of nodes it encounters on a level of
        the workflow.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">constraint</emphasis> the mapper adds
        cleanup nodes to constraint the amount of storage space used by a
        workflow, in addition to leaf cleanup nodes. The nodes remove files no
        longer required during execution. The added cleanup node guarantees
        limits on disk usage. File sizes are read from the <emphasis
        role="bold">size</emphasis> flag in the DAX, or from a CSV file (<link
        linkend="cleanup_props"><emphasis>
        pegasus.file.cleanup.constraint.csv</emphasis></link>).</para>
      </listitem>
    </orderedlist>

    <note>
      <para>For large workflows with lots of files, the inplace strategy may
      take a long time as the algorithm works at a per file level to figure
      out when it is safe to remove a file.</para>
    </note>

    <para>Behaviour of the cleanup strategies implemented in the Pegasus
    Mapper can be controlled by properties described <link
    linkend="cleanup_props">here</link>.</para>

    <section>
      <title>Data Cleanup in Hierarchal Workflows</title>

      <para>By default, for hierarchal workflows the inplace cleanup is always
      turned off. This is because the cleanup algorithm ( InPlace ) does not
      work across the sub workflows. For example, if you have two DAX jobs in
      your top level workflow and the child DAX job refers to a file generated
      during the execution of the parent DAX job, the InPlace cleanup
      algorithm when applied to the parent dax job will result in the file
      being deleted, when the sub workflow corresponding to parent DAX job is
      executed. This would result in failure of sub workflow corresponding to
      the child DAX job, as the file deleted is required to present during
      it's execution.</para>

      <para>In case there are no data dependencies across the dax jobs, then
      yes you can enable the InPlace algorithm for the sub daxâes . To do this
      you can set the property</para>

      <itemizedlist>
        <listitem>
          <para>pegasus.file.cleanup.scope deferred</para>
        </listitem>
      </itemizedlist>

      <para>This will result in cleanup option to be picked up from the
      arguments for the DAX job in the top level DAX .</para>
    </section>
  </section>

  <section id="metadata">
    <title>Metadata</title>

    <para>Pegasus allows users to associate metadata at</para>

    <itemizedlist>
      <listitem>
        <para>Workflow Level in the DAX</para>
      </listitem>

      <listitem>
        <para>Task level in the DAX and the Transformation Catalog</para>
      </listitem>

      <listitem>
        <para>File level in the DAX and Replica Catalog</para>
      </listitem>
    </itemizedlist>

    <para>Metadata is specified as a key value tuple, where both key and
    values are of type String.</para>

    <para>All the metadata ( user specified and auto-generated) gets populated
    into the workflow database ( usually in the workflow submit directory) by
    pegasus-monitord. The metadata in this database can be be queried for
    using the <emphasis role="bold"><link
    linkend="cli-pegasus-metadata">pegasus-metadata</link></emphasis> command
    line tool, or is also shown in the <link linkend="dashboard">Pegasus
    Dashboard</link>.</para>

    <section id="metadata_dax">
      <title>Metadata in the DAX</title>

      <para>In the DAX, metadata can be associated with the workflow, tasks,
      files and executables. For details on how to associate metadata in the
      DAX using the DAX API refer to the DAX API <link
      linkend="dax_generator_api">chapter</link>. Below is an example DAX that
      illustrates metadata associations at workflow, task and file
      level.</para>

      <programlisting>
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!-- generated on: 2016-01-21T10:36:39-08:00 --&gt;
&lt;!-- generated by: vahi [ ?? ] --&gt;
&lt;adag xmlns="http://pegasus.isi.edu/schema/DAX" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://pegasus.isi.edu/schema/DAX http://pegasus.isi.edu/schema/dax-3.6.xsd" version="3.6" name="diamond" index="0" count="1"&gt;

&lt;!-- Section 1: Metadata attributes for the workflow (can be empty)  --&gt;

   &lt;metadata key="name"&gt;diamond&lt;/metadata&gt;
   &lt;metadata key="createdBy"&gt;Karan Vahi&lt;/metadata&gt;

&lt;!-- Section 2: Invokes - Adds notifications for a workflow (can be empty) --&gt;

   &lt;invoke when="start"&gt;/pegasus/libexec/notification/email -t notify@example.com&lt;/invoke&gt;
   &lt;invoke when="at_end"&gt;/pegasus/libexec/notification/email -t notify@example.com&lt;/invoke&gt;

&lt;!-- Section 3: Files - Acts as a Replica Catalog (can be empty) --&gt;

   &lt;file name="f.a"&gt;
      &lt;metadata key="size"&gt;1024&lt;/metadata&gt;
      &lt;pfn url="file:///Volumes/Work/lfs1/work/pegasus-features/PM-902/f.a" site="local"/&gt;
   &lt;/file&gt;

&lt;!-- Section 4: Executables - Acts as a Transformaton Catalog (can be empty) --&gt;

   &lt;executable namespace="pegasus" name="preprocess" version="4.0" installed="true" arch="x86" os="linux"&gt;
      &lt;metadata key="size"&gt;2048&lt;/metadata&gt;
      &lt;pfn url="file:///usr/bin/keg" site="TestCluster"/&gt;
   &lt;/executable&gt;
   &lt;executable namespace="pegasus" name="findrange" version="4.0" installed="true" arch="x86" os="linux"&gt;
      &lt;pfn url="file:///usr/bin/keg" site="TestCluster"/&gt;
   &lt;/executable&gt;
   &lt;executable namespace="pegasus" name="analyze" version="4.0" installed="true" arch="x86" os="linux"&gt;
      &lt;pfn url="file:///usr/bin/keg" site="TestCluster"/&gt;
   &lt;/executable&gt;

&lt;!-- Section 5: Transformations - Aggregates executables and Files (can be empty) --&gt;


&lt;!-- Section 6: Job's, DAX's or Dag's - Defines a JOB or DAX or DAG (Atleast 1 required) --&gt;

   &lt;job id="j1" namespace="pegasus" name="preprocess" version="4.0"&gt;
      &lt;metadata key="time"&gt;60&lt;/metadata&gt;
      &lt;argument&gt;-a preprocess -T 60 -i  &lt;file name="f.a"/&gt; -o  &lt;file name="f.b1"/&gt;   &lt;file name="f.b2"/&gt;&lt;/argument&gt;
      &lt;uses name="f.a" link="input"&gt;
         &lt;metadata key="size"&gt;1024&lt;/metadata&gt;
      &lt;/uses&gt;
      &lt;uses name="f.b1" link="output" transfer="true" register="true"/&gt;
      &lt;uses name="f.b2" link="output" transfer="true" register="true"/&gt;
      &lt;invoke when="start"&gt;/pegasus/libexec/notification/email -t notify@example.com&lt;/invoke&gt;
      &lt;invoke when="at_end"&gt;/pegasus/libexec/notification/email -t notify@example.com&lt;/invoke&gt;
   &lt;/job&gt;
   &lt;job id="j2" namespace="pegasus" name="findrange" version="4.0"&gt;
      &lt;metadata key="time"&gt;60&lt;/metadata&gt;
      &lt;argument&gt;-a findrange -T 60 -i  &lt;file name="f.b1"/&gt; -o  &lt;file name="f.c1"/&gt;&lt;/argument&gt;
      &lt;uses name="f.b1" link="input"/&gt;
      &lt;uses name="f.c1" link="output" transfer="true" register="true"/&gt;
      &lt;invoke when="start"&gt;/pegasus/libexec/notification/email -t notify@example.com&lt;/invoke&gt;
      &lt;invoke when="at_end"&gt;/pegasus/libexec/notification/email -t notify@example.com&lt;/invoke&gt;
   &lt;/job&gt;
   &lt;job id="j3" namespace="pegasus" name="findrange" version="4.0"&gt;
      &lt;metadata key="time"&gt;60&lt;/metadata&gt;
      &lt;argument&gt;-a findrange -T 60 -i  &lt;file name="f.b2"/&gt; -o  &lt;file name="f.c2"/&gt;&lt;/argument&gt;
      &lt;uses name="f.b2" link="input"/&gt;
      &lt;uses name="f.c2" link="output" transfer="true" register="true"/&gt;
      &lt;invoke when="start"&gt;/pegasus/libexec/notification/email -t notify@example.com&lt;/invoke&gt;
      &lt;invoke when="at_end"&gt;/pegasus/libexec/notification/email -t notify@example.com&lt;/invoke&gt;
   &lt;/job&gt;
   &lt;job id="j4" namespace="pegasus" name="analyze" version="4.0"&gt;
      &lt;metadata key="time"&gt;60&lt;/metadata&gt;
      &lt;argument&gt;-a analyze -T 60 -i  &lt;file name="f.c1"/&gt;   &lt;file name="f.c2"/&gt; -o  &lt;file name="f.d"/&gt;&lt;/argument&gt;
      &lt;uses name="f.c1" link="input"/&gt;
      &lt;uses name="f.c2" link="input"/&gt;
      &lt;uses name="f.d" link="output" transfer="true" register="true"/&gt;
      &lt;invoke when="start"&gt;/pegasus/libexec/notification/email -t notify@example.com&lt;/invoke&gt;
      &lt;invoke when="at_end"&gt;/pegasus/libexec/notification/email -t notify@example.com&lt;/invoke&gt;
   &lt;/job&gt;

&lt;!-- Section 7: Dependencies - Parent Child relationships (can be empty) --&gt;

   &lt;child ref="j2"&gt;
      &lt;parent ref="j1"/&gt;
   &lt;/child&gt;
   &lt;child ref="j3"&gt;
      &lt;parent ref="j1"/&gt;
   &lt;/child&gt;
   &lt;child ref="j4"&gt;
      &lt;parent ref="j2"/&gt;
      &lt;parent ref="j3"/&gt;
   &lt;/child&gt;
&lt;/adag&gt;</programlisting>
    </section>

    <section id="metadata_wf">
      <title>Workflow Level Metadata</title>

      <para>Workflow level metadata can be associated only in the DAX under
      the root element adag. Below is a snippet that illustrates this</para>

      <programlisting>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!-- generated on: 2016-01-21T10:36:39-08:00 --&gt;
&lt;!-- generated by: vahi [ ?? ] --&gt;
&lt;adag xmlns="http://pegasus.isi.edu/schema/DAX" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://pegasus.isi.edu/schema/DAX http://pegasus.isi.edu/schema/dax-3.6.xsd" version="3.6" name="diamond" index="0" count="1"&gt;

&lt;!-- Section 1: Metadata attributes for the workflow (can be empty)  --&gt;

   &lt;metadata key="name"&gt;diamond&lt;/metadata&gt;
   &lt;metadata key="createdBy"&gt;Karan Vahi&lt;/metadata&gt;

...

&lt;/adag&gt;</programlisting>
    </section>

    <section id="metadata_task">
      <title>Task Level Metadata</title>

      <para>Metadata for the tasks is picked up from</para>

      <itemizedlist>
        <listitem>
          <para>metadata associated with the job element in the DAX</para>
        </listitem>

        <listitem>
          <para>metadata associated with the corresponding transformation. The
          transformation for a task is picked up from either a matching
          executable entry in the DAX ( if exists ) or the Transformation
          Catalog.</para>
        </listitem>
      </itemizedlist>

      <para>Below is a snippet that illustrates metadata for a task specified
      in the job element in the DAX</para>

      <programlisting>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!-- generated on: 2016-01-21T10:36:39-08:00 --&gt;
&lt;!-- generated by: vahi [ ?? ] --&gt;
&lt;adag xmlns="http://pegasus.isi.edu/schema/DAX" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://pegasus.isi.edu/schema/DAX http://pegasus.isi.edu/schema/dax-3.6.xsd" version="3.6" name="diamond" index="0" count="1"&gt;

...
    &lt;job id="j2" namespace="pegasus" name="findrange" version="4.0"&gt;
      &lt;metadata key="time"&gt;60&lt;/metadata&gt;
      &lt;argument&gt;-a findrange -T 60 -i  &lt;file name="f.b1"/&gt; -o  &lt;file name="f.c1"/&gt;&lt;/argument&gt;
      &lt;uses name="f.b1" link="input"/&gt;
      &lt;uses name="f.c1" link="output" transfer="true" register="true"/&gt;
      &lt;invoke when="start"&gt;/pegasus/libexec/notification/email -t notify@example.com&lt;/invoke&gt;
      &lt;invoke when="at_end"&gt;/pegasus/libexec/notification/email -t notify@example.com&lt;/invoke&gt;
   &lt;/job&gt;

...

&lt;/adag&gt;</programlisting>

      <para>Below is a snippet that illustrates metadata for a task specified
      in the executable element in the DAX</para>

      <programlisting>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!-- generated on: 2016-01-21T10:36:39-08:00 --&gt;
&lt;!-- generated by: vahi [ ?? ] --&gt;
&lt;adag xmlns="http://pegasus.isi.edu/schema/DAX" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://pegasus.isi.edu/schema/DAX http://pegasus.isi.edu/schema/dax-3.6.xsd" version="3.6" name="diamond" index="0" count="1"&gt;

...
    &lt;!-- Section 4: Executables - Acts as a Transformaton Catalog (can be empty) --&gt;

   &lt;executable namespace="pegasus" name="findrange" version="4.0" installed="true" arch="x86" os="linux"&gt;
      &lt;metadata key="size"&gt;2048&lt;/metadata&gt;
      &lt;pfn url="file:///usr/bin/keg" site="TestCluster"/&gt;
   &lt;/executable&gt;

...

&lt;/adag&gt;</programlisting>

      <para>Metadata can be associated with the transformation in the
      transformation catalog. The metadata specified in the transformation
      catalog gets automatically associated with the task level metadata for
      the corresponding task ( that uses that executable). This resolution is
      similar to how profiles associated in the Transformation Catalog get
      associated with the tasks. Below is an example Transformation Catalog
      that illustrates metadata associated with the executables.</para>

      <programlisting>tr pegasus::findrange:4.0 { 
    site TestCluster { 
        pfn "/usr/bin/pegasus-keg" 
        arch "x86_64" 
        os "linux" 
        type "INSTALLED" 
        profile pegasus "clusters.size" "20" 
        metadata "key" "value" 
        metadata "appmodel" "myxform.aspen" 
        metadata "version" "3.0" 
    } 
}</programlisting>
    </section>

    <section id="metadata_file">
      <title>File Level Metadata</title>

      <para>Metadata for the files is picked up from</para>

      <itemizedlist>
        <listitem>
          <para>metadata associated with the file element in the DAX. File
          elements are optionally used to record the locations of input files
          for the workflow in the DAX.</para>
        </listitem>

        <listitem>
          <para>metadata associated with the files in the uses section of the
          job element in the DAX</para>
        </listitem>

        <listitem>
          <para>metadata associated with the file in the Replica
          Catalog.</para>
        </listitem>
      </itemizedlist>

      <para>Below is a snippet that illustrates metadata for a file specified
      in the file element in the DAX</para>

      <programlisting>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!-- generated on: 2016-01-21T10:36:39-08:00 --&gt;
&lt;!-- generated by: vahi [ ?? ] --&gt;
&lt;adag xmlns="http://pegasus.isi.edu/schema/DAX" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://pegasus.isi.edu/schema/DAX http://pegasus.isi.edu/schema/dax-3.6.xsd" version="3.6" name="diamond" index="0" count="1"&gt;

...
    &lt;!-- Section 3: Files - Acts as a Replica Catalog (can be empty) --&gt;

   &lt;file name="f.a"&gt;
      &lt;metadata key="size"&gt;1024&lt;/metadata&gt;
      &lt;pfn url="file:///Volumes/Work/lfs1/work/pegasus-features/PM-902/f.a" site="local"/&gt;
   &lt;/file&gt;


...

&lt;/adag&gt;</programlisting>

      <para>Below is a snippet that illustrates metadata for a file in the
      uses section of the job element</para>

      <programlisting>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!-- generated on: 2016-01-21T10:36:39-08:00 --&gt;
&lt;!-- generated by: vahi [ ?? ] --&gt;
&lt;adag xmlns="http://pegasus.isi.edu/schema/DAX" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://pegasus.isi.edu/schema/DAX http://pegasus.isi.edu/schema/dax-3.6.xsd" version="3.6" name="diamond" index="0" count="1"&gt;

...
    &lt;job id="j1" namespace="pegasus" name="preprocess" version="4.0"&gt;
      &lt;argument&gt;-a preprocess -T 60 -i  &lt;file name="f.a"/&gt; -o  &lt;file name="f.b1"/&gt;   &lt;file name="f.b2"/&gt;&lt;/argument&gt;
      &lt;uses name="f.a" link="input"&gt;
         &lt;metadata key="size"&gt;1024&lt;/metadata&gt;
         &lt;metadata key="source"&gt;DAX&lt;/metadata&gt;
      &lt;/uses&gt;
      &lt;uses name="f.b1" link="output" transfer="true" register="true"/&gt;
      &lt;uses name="f.b2" link="output" transfer="true" register="true"/&gt;
   &lt;/job&gt;

...

&lt;/adag&gt;</programlisting>

      <para>Below is a snippet that illustrates metadata for an input file in
      the Replica Catalog entry for the file</para>

      <programlisting># File Based Replica Catalog
f.a file://$PWD/production_200.conf site="local" source="replica_catalog"</programlisting>
    </section>

    <section id="metadata_auto">
      <title>Automatically Generated Metadata attributes</title>

      <para>Pegasus captures certain metadata attributes as output files are
      generated and associates them at the file level in the database.
      Currently, the following attributes for the output files are
      automatically captured from the kickstart record and stored in the
      workflow database.</para>

      <itemizedlist>
        <listitem>
          <para>pfn - the physical file location</para>
        </listitem>

        <listitem>
          <para>ctime - creation time</para>
        </listitem>

        <listitem>
          <para>size - size of the file in bytes</para>
        </listitem>

        <listitem>
          <para>user - the linux user as who the process ran that generated
          the output file.</para>
        </listitem>
      </itemizedlist>

      <note>
        <para>The automatic collection of the metadata attributes for output
        files is only triggered if the output file is marked to be registered
        in the replica catalog, and --output-site option to pegasus-plan is
        specified.</para>
      </note>
    </section>

    <section id="metadata_trace">
      <title>Tracing Metadata for an output file</title>

      <para>The command line client <link
      linkend="cli-pegasus-metadata">pegasus-metadata</link> allows a user to
      trace all the metadata associated with the file. The client will display
      metadata for the output file, the task that generated the file, the
      workflow which contains the task, and the root workflow which contains
      the task. Below is a sample illustration of it.</para>

      <programlisting><emphasis role="bold">$ pegasus-metadata file --file-name f.d --trace /path/to/submit-dir</emphasis>

Workflow 493dda63-c6d0-4e62-bc36-26e5629449ad
    createdby : Test user
    name      : diamond

Task ID0000004
    size           : 2048
    time           : 60
    transformation : analyze

File f.d
    ctime        : 2016-01-20T19:02:14-08:00
    final_output : true
    size         : 582
    user         : bamboo
</programlisting>
    </section>
  </section>
</chapter>
