<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE section PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<section>
  <title>Command Line Tools</title>

  <para>This chapter contains reference material for all the command-line
  tools distributed with Pegasus.</para>

  <section>
    <title>pegasus-version</title>

    <para>pegasus-version is a simple command-line tool that reports the
    version number of the Pegasus distribution being used.</para>

    <para>In its most basic invocation, it will show the current version of
    the Pegasus software you have installed:</para>

    <para><screen>$ <command>pegasus-version</command>
3.1.0cvs</screen>If you want to know more details about the installed version,
    i.e. which system it was compiled for and when, use the
    <emphasis>long</emphasis> or <emphasis>full</emphasis> mode:</para>

    <para><screen>$ <command>pegasus-version</command> <command><replaceable>-f</replaceable></command>
3.1.0cvs-x86_64_cent_5.6-20110706191019Z</screen>The reported version may
    sometimes not be the version you would expect, if the Pegasus jar file got
    updated but not the remainder of the installation environment. To check
    this case, you use <emphasis>match</emphasis> mode:</para>

    <para><screen>$ <command>pegasus-version</command> <command><replaceable>-m</replaceable></command>
Compiled into PEGASUS: 20110706191019Z x86_64_cent_5.6
Installation provides: 20110706191019Z x86_64_cent_5.6
OK: Internal version matches installation.
Complete version info: 3.1.0cvs-x86_64_cent_5.6-20110706191019Z</screen></para>

    <para>The <emphasis>match</emphasis> mode implies
    <emphasis>long</emphasis> mode, and will thus show the full version as
    part of its output.</para>

    <para>In <emphasis>quiet</emphasis> mode, which can only be applied to
    <emphasis>match</emphasis> mode, no output is written unless there is an
    error. You would use <emphasis>quiet</emphasis> mode to check the exit
    code of <literal>pegasus-version</literal> to determine a problem while
    being in a scripted environment.</para>

    <para><screen>$ <command>pegasus-version</command> <command><replaceable>-mq</replaceable></command>
$ <command>echo $?</command>
0</screen>The manual page for <literal>pegasus-version</literal> will show
    more details, and long options availble to the user.</para>
  </section>

  <section>
    <title>pegasus-plan</title>

    <para>pegasus-plan generates an executable workflow from an abstract
    workflow description (DAX).</para>

    <section>
      <title>SYNOPSIS</title>

      <para><emphasis role="bold">pegasus-plan</emphasis> -h|--help</para>

      <para><emphasis role="bold">pegasus-plan</emphasis> -V|--version</para>

      <para><emphasis role="bold">pegasus-plan</emphasis> [-Dprop [..]] -d
      &lt;dax file&gt; [-b prefix] [--conf &lt;path to properties file&gt;]
      [-c f1[,f2[..]]] [-C &lt;clustering technique&gt;] [--dir &lt;base
      directory for o/p files&gt;] [-f] [--force-replan] [
      --inherited-rc-files] [-j job-prefix] [-n] [-o &lt;out- put site&gt;]
      [-r[directoryname]] [--relative-dir &lt;relative directory to base
      directory&gt;] [--relative-submit-dir &lt;relative sub- mit directory to
      the base directory&gt;] [-s site1[,site2[..]]] [-v] [-q] [-V]
      [-h]</para>
    </section>

    <section>
      <title>DESCRIPTION</title>

      <para>The pegasus-plan command takes in as input the DAX and generates
      an executable workflow usually in form of condor submit files, which can
      be submitted to an execution site for execution.</para>

      <para>As part of generating the executable workflow, the planner needs
      to discover</para>

      <itemizedlist>
        <listitem>
          <para>data</para>

          <para>The Pegasus Workflow Planner ensures that all the data
          required for the execution of the executable workflow is transferred
          to the execution site by adding transfer nodes at appropriate points
          in the DAG. This is done by looking up an appropriate Replica
          Catalog to determine the locations of the input files for the
          various jobs. At present the default replica mechanism used is RLS
          .</para>

          <para>The Pegasus Workflow Planner also tries to reduce the
          workflow, unless specified otherwise. This is done by deleting the
          jobs whose output files have been found in some location in the
          Replica Catalog . At present no cost metrics are used. However
          preference is given to a location corresponding to the execution
          site.</para>

          <para>The planner can also add nodes to transfer all the
          materialized files to an output site. The location on the output
          site is determined by looking up the site catalog file, the path to
          which is picked up from the <emphasis
          role="bold">pegasus.catalog.site.file</emphasis> property
          value.</para>
        </listitem>

        <listitem>
          <para>executables</para>

          <para>The planner looks up a Transformation Catalog to discover
          locations of the executables referred to in the executable workflow.
          Users can specify INSTALLED or STAGEABLE executables in the catalog.
          Stageable executables can be used by Pegasus to stage executables to
          resources where they are not pre-installed.</para>
        </listitem>

        <listitem>
          <para>resources</para>

          <para>The layout of the sites , where Pegasus can schedule jobs of a
          workflow are described in the Site Catalog. The planner looks up the
          site catalog to determine for a site what directories a job can be
          executed in, what servers to use for staging in and out data and
          what jobmanagers ( if applicable) can be used for submitting
          jobs.</para>
        </listitem>
      </itemizedlist>

      <para>The data and executable locations can now be specified in DAX'es
      conforming to DAX schema version 3.2 or higher.</para>
    </section>

    <section>
      <title>ARGUMENTS</title>

      <para>Any option will be displayed with its long options
      synonym(s).</para>

      <itemizedlist>
        <listitem>
          <para>-<emphasis role="bold">Dprop</emphasis></para>

          <para>The -D options allows an experienced user to override certain
          properties which influence the program execution, among them the
          default location of the user's properties file and the PEGASUS home
          location. One may set several CLI properties by giving this option
          multiple times. The -D option(s) must be the first option on the
          command line. A CLI property take precedence over the properties
          file property of the same key.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-d filename</emphasis></para>

          <para><emphasis role="bold">--dax filename</emphasis></para>

          <para>The DAX is the XML input file that describes an abstract
          workflow.</para>

          <para>This is a mandatory option, which has to be used.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-b prefix</emphasis></para>

          <para><emphasis role="bold">--basename prefix</emphasis></para>

          <para>The basename prefix to be used while constructing per workflow
          files like the dagman file (.dag file) and other work- flow specific
          files that are created by Condor. Usually this prefix, is taken from
          the name attribute specified in the root element of the dax
          files.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-c list of cache files</emphasis></para>

          <para><emphasis role="bold">--cache list of cache
          files</emphasis></para>

          <para>A comma separated list of paths to replica cache files that
          override the results from the replica catalog for a particular
          lfn.</para>

          <para>Each entry in the cache file describes a LFN , the
          corresponding PFN and the associated attributes. The pool attribute
          should be specified for each entry.</para>

          <programlisting>LFN_1 PFN_1 pool=[site handle 1]
LFN_2 PFN_2 pool=[site handle 2]
...
LFN_N PFN_N [site handle N]</programlisting>

          <para>To treat the cache files as supplemental replica catalogs set
          the property <emphasis
          role="bold">pegasus.catalog.replica.cache.asrc </emphasis>to true.
          This results in the mapping in the cache files to be merged with the
          mappings in the replica catalog. Thus, for a particular lfn both the
          entries in the cache file and replica catalog are available for
          replica selection.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-C comma separated list of clustering
          styles</emphasis></para>

          <para><emphasis role="bold">--cluster comma separated list of
          clustering styles</emphasis></para>

          <para>This mode of operation results in clustering of n compute jobs
          into a larger jobs to reduce remote scheduling overhead. You can
          specify a list of clustering techniques to recursively apply them to
          the workflow. For example, this allows you to cluster some jobs in
          the workflow using horizontal clustering and then use label based
          clustering on the intermediate workflow to do vertical
          clustering.</para>

          <para>The clustered jobs can be run at the remote site, either
          sequentially or by using mpi. This can be specified by setting the
          property pegasus.job.aggregator. The property can be overridden by
          associating the PEGASUS profile key <emphasis
          role="bold">collapser</emphasis> either with the transformation in
          the transformation catalog or the execution site in the site
          catalog. The value specified (to the property or the profile), is
          the logical name of the transformation that is to be used for
          clustering jobs. Note that clustering will only happen if the
          corresponding transformations are catalogued in the transformation
          catalog.</para>

          <para>PEGASUS ships with a clustering executable <emphasis
          role="bold">seqexec</emphasis> that can be found in
          $PEGASUS_HOME/bin directory. It runs the jobs in the clustered job
          sequentially on the same node at the remote site.</para>

          <para>In addition, a mpi wrapper <emphasis
          role="bold">mpiexec</emphasis> is distributed as source with the
          PEGASUS. It can be found in $PEGASUS_HOME/src/tools/cluster
          directory. The wrapper is run on every mpi node, with the first one
          being the master and the rest of the ones as workers. The number of
          instances of mpiexec that are invoked is equal to the value of the
          globus rsl key nodecount. The master distributes the smaller
          constituent jobs to the workers. For e.g. If there were 10 jobs in
          the clustered job and nodecount was 5, then one node acts as master,
          and the 10 jobs are distributed amongst the 4 slaves on demand. The
          master hands off a job to the slave node as and when it gets free.
          So initially all the 4 nodes are given a single job each, and then
          as and when they get done are handed more jobs till all the 10 jobs
          have been executed.</para>

          <para>By default, seqexec is used for clustering jobs unless
          overridden in the properties or by the <emphasis role="bold">pegasus
          profile key collapser</emphasis>.</para>

          <para>The following type of clustering styles are currently
          supported</para>

          <orderedlist>
            <listitem>
              <para><emphasis role="bold">horizontal</emphasis></para>

              <para>is the style of clustering in which jobs on the same level
              are aggregated into larger jobs. A level of the workflow is
              defined as the greatest distance of a node, from the root of the
              workflow. Clustering occurs only on jobs of the same type i.e
              they refer to the same logical transformation in the
              transformation catalog.</para>

              <para>The granularity of clustering can be specified by
              associating either the PEGASUS profile key <emphasis
              role="bold">clusters.size</emphasis> or the PEGASUS profile key
              <emphasis role="bold">clusters.num</emphasis> with the
              transformation. The clusters.size key indicates how many jobs
              need to be clustered into the larger clustered job. The
              clusters.num key indicates how many clustered jobs are to be
              created for a particular level at a particular execution site.
              If both keys are specified for a particular transformation, then
              the clusters.num key value is used to determine the clustering
              granularity.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">label</emphasis></para>

              <para>is the style of clustering in which you can label the jobs
              in your workflow. The jobs with the same level are put in the
              same clustered job. This allows you to aggregate jobs across
              levels, or in a manner that is best suited to your
              application.</para>

              <para>To label the workflow, you need to associate PEGASUS
              profiles with the jobs in the DAX. The profile key to use for
              labelling the workflow can be set by the property <emphasis
              role="bold">pegasus.clusterer.label.key</emphasis> . It defaults
              to label, meaning if you have a PEGASUS profile key label with
              jobs, the jobs with the same value for the pegasus profile key
              label will go into the same clustered job.</para>
            </listitem>
          </orderedlist>
        </listitem>

        <listitem>
          <para><emphasis role="bold">--conf path to properties
          file</emphasis></para>

          <para>The path to properties file that contains the properties
          planner needs to use while planning the workflow.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">--dir dir name</emphasis></para>

          <para>The base directory where you want the output of the Pegasus
          Workflow Planner usually condor submit files, to be generated.
          Pegasus creates a directory structure in this base directory on the
          basis of username, VO Group and the label of the workflow in the
          DAX.</para>

          <para>By default the base directory is the directory from which one
          runs the pegasus-plan command.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-f</emphasis></para>

          <para><emphasis role="bold">--force</emphasis></para>

          <para>This bypasses the reduction phase in which the abstract DAG is
          reduced, on the basis of the locations of the output files returned
          by the replica catalog. This is analogous to a make style generation
          of the executable workflow.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">--force-replan</emphasis></para>

          <para>By default, for hierarchal workflows if a dax job fails, then
          on job retry the rescue dag of the associated workflow is submitted.
          This option causes Pegasus to replan the dax job in case of failure
          instead.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-g</emphasis></para>

          <para><emphasis role="bold">--group</emphasis></para>

          <para>The VO Group to which the user belongs to.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-h</emphasis></para>

          <para><emphasis role="bold">--help</emphasis> displays the options
          to pegasus-plan command.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">--inherited-rc-files</emphasis> comma
          separated list of replica catalog files</para>

          <para>A comma separated list of paths to replica files. Locations
          mentioned in these have a lower priority than the locations in the
          DAX file. This option is usually used internally for hierarichal
          workflows, where the file locations mentioned in the parent (
          encompassing) workflow DAX, passed to the sub workflows (
          corresponding) to the dax jobs.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-j</emphasis></para>

          <para><emphasis role="bold">--j</emphasis></para>

          <para>The job prefix to be applied for constructing the filenames
          for the job submit files.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-n</emphasis></para>

          <para><emphasis role="bold">--no-cleanup</emphasis></para>

          <para>This results in the generation of the separate cleanup
          workflow that removes the directories created during the execution
          of the executable workflow. The cleanup workflow is to be submitted
          after the executable workflow has finished. If this option is not
          specified, then Pegasus adds cleanup nodes to the executable
          workflow itself that cleanup files on the remote sites when they are
          no longer required.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-o</emphasis> output site</para>

          <para><emphasis role="bold">--output </emphasis>output site</para>

          <para>The output site where all the materialized data is transferred
          to.</para>

          <para>By default the materialized data remains in the working
          directory on the execution site where it was created. Only those
          output files are transferred to an output site for which transfer
          attribute is set to true in the DAX.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-q</emphasis></para>

          <para><emphasis role="bold">--quiet</emphasis></para>

          <para>decreases the logging level</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">--relative-dir</emphasis> dir
          name</para>

          <para>The directory relative to the base directory where the
          executable workflow it to be generated and executed. This over-
          rides the default directory structure that Pegasus creates based on
          username, VO Group and the DAX label.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">--relative-submit-dir</emphasis> dir
          name</para>

          <para>The directory relative to the base directory where the
          executable workflow it to be generated. This overrides the default
          directory structure that Pegasus creates based on username, VO Group
          and the DAX label. By specifying --relative-dir and
          --relative-submit-dir you can have different relative execution
          directory on the remote site and different relative sub- mit
          directory on the submit host.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-s</emphasis> list of execution
          sites</para>

          <para><emphasis role="bold">--sites</emphasis> list of execution
          sites</para>

          <para>A comma separated list of execution sites on which the
          workflow is to be executed. Each of the sites should have an entry
          in the site catalog, that is being used. To run on the submit host,
          specify the execution site as<emphasis role="bold"> local
          .</emphasis></para>

          <para>In case this option is not specified, all the sites in the
          site catalog are picked up as candidates for running the work-
          flow.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-s</emphasis></para>

          <para><emphasis role="bold">--submit</emphasis></para>

          <para>Submits the generated executable workflow using pegasus-run
          script in $PEGASUS_HOME/bin directory.</para>

          <para>By default, the Pegasus Workflow Planner only generates the
          Condor submit files and does not submit them.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-v</emphasis></para>

          <para><emphasis role="bold">--verbose</emphasis></para>

          <para>increases the verbosity of messages about what is going
          on.</para>

          <para>By default, all FATAL, ERROR, CONSOLE and WARN messages are
          logged.</para>

          <para>The logging hierarchy is as follows</para>

          <itemizedlist>
            <listitem>
              <para>FATAL</para>
            </listitem>

            <listitem>
              <para>ERROR</para>
            </listitem>

            <listitem>
              <para>CONSOLE</para>
            </listitem>

            <listitem>
              <para>WARN</para>
            </listitem>

            <listitem>
              <para>INFO</para>
            </listitem>

            <listitem>
              <para>CONFIG</para>
            </listitem>

            <listitem>
              <para>DEBUG</para>
            </listitem>

            <listitem>
              <para>TRACE</para>
            </listitem>
          </itemizedlist>

          <para>For example, to see the INFO, CONFIG and DEBUG messages
          additionally, set -vvv.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-V</emphasis></para>

          <para><emphasis role="bold">--version</emphasis></para>

          <para>Displays the current version number of the Pegasus Workflow
          Management System.</para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>RETURN VALUE</title>

      <para>If the Pegasus Workflow Planner is able to generate an executable
      workflow successfully, the exitcode will be 0. All runtime errors result
      in an exitcode of 1. This is usually in the case when you have
      mis-configured your catalogs etc. In the case of an error occurring
      while loading a specific module implementation at run time, the exitcode
      will be 2. This is usually due to factory methods failing while loading
      a module. In case of any other error occurring during the running of the
      command, the exit- code will be 1. In most cases, the error message
      logged should give a clear indication as to where things went
      wrong.</para>
    </section>

    <section>
      <title>PEGASUS PROPERTIES</title>

      <para>This is not an exhaustive list of properties used. For the
      complete description and list of properties refer to <emphasis
      role="bold">$PEGASUS_HOME/doc/advanced-properties.pdf</emphasis> or the
      properties chapter.</para>

      <itemizedlist>
        <listitem>
          <para><emphasis
          role="bold">pegasus.catalog.replica</emphasis></para>

          <para>Specifies the type of replica catalog to be used. If not
          specified, then RLS is used as a Replica Catalog Backend.</para>
        </listitem>

        <listitem>
          <para><emphasis
          role="bold">pegasus.catalog.replica.url</emphasis></para>

          <para>Contact string to access the replica catalog. In case of RLS
          it is the RLI url. I</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">pegasus.dir.exec</emphasis></para>

          <para>A suffix to the workdir in the site catalog to determine the
          current working directory. If relative, the value will be appended
          to the working directory from the site.config file. If absolute it
          constitutes the working directory.</para>
        </listitem>

        <listitem>
          <para><emphasis
          role="bold">pegasus.catalog.transformation</emphasis></para>

          <para>Specifies the type of transformation catalog to be used. One
          can use either a file based or a database based transforma- tion
          catalog. At present the default is Text .</para>
        </listitem>

        <listitem>
          <para><emphasis
          role="bold">pegasus.catalog.transformation.file</emphasis></para>

          <para>The location of file to use as transformation catalog.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">pegasus.catalog.site</emphasis></para>

          <para>Specifies the type of site catalog to be used. At present the
          default is <emphasis role="bold">XML3</emphasis> .</para>
        </listitem>

        <listitem>
          <para><emphasis
          role="bold">pegasus.catalog.site.file</emphasis></para>

          <para>The location of file to use as a site catalog. If not
          specified, then default value of $PEGASUS_HOME/etc/sites.xml is used
          in case of the xml based site catalog and
          $PEGASUS_HOME/etc/sites.txt in case of the text based site
          catalog.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">pegasus.code.generator</emphasis></para>

          <para>The code generator to use. By default, Condor submit files are
          generated for the executable workflow. Setting to <emphasis
          role="bold">Shell</emphasis> results in Pegasus generating a shell
          script that can be executed on the submit host.</para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>FILES</title>

      <para></para>

      <itemizedlist>
        <listitem>
          <para><emphasis
          role="bold">$PEGASUS_HOME/etc/dax-3.2.xsd</emphasis></para>

          <para>It is the suggested location of the latest DAX schema to
          produce DAX output.</para>
        </listitem>

        <listitem>
          <para><emphasis
          role="bold">$PEGASUS_HOME/etc/tc.data.text</emphasis></para>

          <para>is the suggested location for the file corresponding to the
          Transformation Catalog.</para>
        </listitem>

        <listitem>
          <para><emphasis
          role="bold">$PEGASUS_HOME/etc/sc-3.0.xsd</emphasis></para>

          <para>is the suggested location of the latest Site Catalog schema
          that is used to create the XML3 version of the site catalog</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">$PEGASUS_HOME/etc/sites.xml3 |
          $PEGASUS_HOME/etc/sites.xml</emphasis></para>

          <para>is the suggested location for the file containing the site
          information.</para>
        </listitem>

        <listitem>
          <para><emphasis
          role="bold">$PEGASUS_HOME/lib/pegasus.jar</emphasis></para>

          <para>contains all compiled Java bytecode to run the Pegasus
          Workflow Planner.</para>
        </listitem>
      </itemizedlist>
    </section>
  </section>

  <section>
    <title>pegasus-run</title>

    <para>pegasus-run executes a workflow that has been planned using
    pegasus-plan.</para>
  </section>

  <section>
    <title>pegasus-remove</title>

    <para>pegasus-remove is used to abort a running workflow.</para>
  </section>

  <section>
    <title>pegasus-status</title>

    <para>pegasus-status reports on the status of a workflow.</para>
  </section>

  <section>
    <title>pegasus-analyzer</title>

    <para>pegasus-analyzer is used to debug failed workflows.</para>
  </section>

  <section>
    <title>pegasus-statistics</title>

    <para>pegasus-statistics reports statistics about a workflow.</para>
  </section>

  <section>
    <title>pegasus-plots</title>

    <para>pegasus-plots generates charts and graphs that illustrate the
    statistics and execution of a workflow.</para>
  </section>

  <section>
    <title>pegasus-transfer</title>

    <para>pegasus-transfer is a wrapper for several file transfer
    clients.</para>
  </section>

  <section>
    <title>pegasus-sc-client</title>

    <para>pegasus-sc-client is used to generate and modify site
    catalogs.</para>
  </section>

  <section>
    <title>pegasus-rc-client</title>

    <para>pegasus-rc-client - shell client for replica implementations</para>

    <section>
      <title>SYNOPSIS</title>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">pegasus-rc-client</emphasis> [-Dprop
          [...]] [-c fn] [-p k=v] [[-f fn]|[-i|-d fn]|[cmd [args]]</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">pegasus-rc-client</emphasis> [-c fn]
          -V</para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>DESCRIPTION</title>

      <para>The shell interface to replica catalog implementations is a
      prototype. It determines from various property setting which class
      implements the replica manager interface, and loads that driver at
      run-time. Some commands depend on the implementation.</para>
    </section>

    <section>
      <title>ARGUMENTS</title>

      <para>Any option will be displayed with its long options
      synonym(s).</para>

      <para></para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">-h, --help</emphasis></para>

          <para>print this help text</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-V, --version</emphasis></para>

          <para>print some version identification string and exit</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-Dprop</emphasis></para>

          <para>The -D options allows an experienced user to override certain
          properties which influence the program execution, among them the
          default location of the user's properties file and the PEGASUS home
          location. One may set several CLI properties by giving this option
          multiple times. The -D option(s) must be the first option on the
          command line. A CLI property take precedence over the properties
          file property of the same key.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-f, --file fn</emphasis></para>

          <para>uses non-interactive mode, reading from file fn. The special
          filename hyphen reads from pipes</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-c, --conf fn</emphasis></para>

          <para>path to the property file</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-v, --verbose</emphasis></para>

          <para>increases the verbosity level</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-p, --pref k=v</emphasis></para>

          <para>enters the specified mapping into preferences (multi-use).
          remember quoting, e.g. -p 'format=%l %p %a'</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-i, --insert fn</emphasis></para>

          <para>the path to the file containing the mappings to be inserted.
          Each line in the file denotes one mapping of format &lt;LFN&gt;
          &lt;PFN&gt; [k=v [..]]</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-d, --delete fn</emphasis></para>

          <para>the path to the file containing the mappings to be deleted.
          Each line in the file denotes one mapping of format &lt;LFN&gt;
          &lt;PFN&gt; [k=v [..]].</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-l, --lookup fn</emphasis></para>

          <para>the path to the file containing the LFN's to be looked up.
          Each line in the file denotes one LFN</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">cmd [args]</emphasis></para>

          <para>If not in file-driven mode, a single command can be specified
          with its arguments.</para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>RETURN VALUE</title>

      <para>Regular and planned program terminations will result in an exit
      code of 0. Abnormal termination will result in a non-zero exit
      code.</para>
    </section>

    <section>
      <title>FILES</title>

      <itemizedlist>
        <listitem>
          <para><emphasis
          role="bold">$PEGASUS_HOME/etc/properties</emphasis></para>

          <para>contains the basic properties with all configurable
          options.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">$HOME/.pegasusrc</emphasis></para>

          <para>contains the basic properties with all configurable
          options.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">pegasus.jar</emphasis></para>

          <para>contains all compiled Java bytecode to run the replica
          manager.</para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>ENVIRONMENT VARIABLES</title>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">$PEGASUS_HOME</emphasis></para>

          <para>is the suggested base directory of your the execution
          environment.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">$JAVA_HOME</emphasis></para>

          <para>should be set and point to a valid location to start the
          intended Java virtual machine as $JAVA_HOME/bin/java.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">$CLASSPATH</emphasis></para>

          <para>should be set to contain all necessary files for the execution
          environment. Please make sure that your CLASSPATH includes pointer
          to the replica implementation required jar files.</para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>PROPERTIES</title>

      <para>The complete branch of properties pegasus.catalog.replica
      including itself are interpreted by the prototype. While the
      pegasus.catalog.replica property itself steers the backend to connect
      to, any meaning of branched keys is dependent on the backend. The same
      key may have different meanings for different backends.</para>

      <itemizedlist>
        <listitem>
          <para><emphasis
          role="bold">pegasus.catalog.replica</emphasis></para>

          <para>determines the name of the implementing class to load at
          run-time. If the class resides in org.griphyn.common.catalog.replica
          no prefix is required. Otherwise, the fully qualified class name
          must be specified.</para>
        </listitem>

        <listitem>
          <para><emphasis
          role="bold">pegasus.catalog.replica.url</emphasis></para>

          <para>is used by the RLS|LRC implementations. It determines the RLI
          / LRC url to use.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">p<emphasis
          role="bold">egasus.catalog.replica.file</emphasis></emphasis></para>

          <para>is used by the SimpleFile implementation. It specifies the
          path to the file to use as the backend for the catalog.</para>
        </listitem>

        <listitem>
          <para><emphasis
          role="bold">pegasus.catalog.replica.db.driver</emphasis></para>

          <para>is used by a simple rDBMs implementation. The string is the
          fully-qualified class name of the JDBC driver used by the rDBMS
          implementer.</para>
        </listitem>

        <listitem>
          <para><emphasis
          role="bold">pegasus.catalog.replica.db.url</emphasis></para>

          <para>is the jdbc url to use to connect to the database.</para>
        </listitem>

        <listitem>
          <para><emphasis
          role="bold">pegasus.catalog.replica.db.user</emphasis></para>

          <para>is used by a simple rDBMS implementation. It constitutes the
          database user account that contains the RC_LFN and RC_ATTR
          tables.</para>
        </listitem>

        <listitem>
          <para><emphasis
          role="bold">pegasus.catalog.replica.db.password</emphasis></para>

          <para>is used by a simple rDBMS implementation. It constitutes the
          database user account that contains the RC_LFN and RC_ATTR
          tables.</para>
        </listitem>

        <listitem>
          <para><emphasis
          role="bold">pegasus.catalog.replica.chunk.size</emphasis></para>

          <para>is used by the pegasus-rc-client for the bulk insert and
          delete operations. The value determines the number of lines that are
          read in at a time, and worked upon at together.</para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>COMMANDS</title>

      <para>The commandline tool provides a simplified shell-wrappable
      interface to manage a replica catalog backend. The commands can either
      be specified in a file in bulk mode, in a pipe, or as additional
      arguments to the invocation.</para>

      <para></para>

      <para>Note that you must escape special characters from the
      shell.</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">help</emphasis></para>

          <para>displays a small resume of the commands.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">exit, quit</emphasis></para>

          <para><emphasis role="bold"><emphasis>should only be used in
          interactive mode to exit the interactive
          mode.</emphasis></emphasis></para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">clear</emphasis></para>

          <para>drops all contents from the backend. Use with special
          care!</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">insert &lt;lfn&gt; &lt;pfn&gt; [k=v
          [..]]</emphasis></para>

          <para>inserts a given lfn and pfn, and an optional site string into
          the backend. If the site is not specified, a null value is inserted
          for the site.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">delete &lt;lfn&gt; &lt;pfn&gt; [k=v
          [..]]</emphasis></para>

          <para>removes a triple of lfn, pfn and, optionally, site from the
          replica backend. If the site was not specified, all matches of the
          lfn pfn pairs will be removed, regardless of the site.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">lookup &lt;lfn&gt; [&lt;lfn&gt;
          [..]]</emphasis></para>

          <para>retrieves one or more mappings for a given lfn from the
          replica backend.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">remove &lt;lfn&gt; [&lt;lfn&gt;
          [..]]</emphasis></para>

          <para>removes all mappings for each lfn from the replica
          backend.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">list [lfn &lt;pat&gt;] [pfn &lt;pat&gt;]
          [&lt;name&gt; &lt;pat&gt;]</emphasis></para>

          <para>obtains all matches from the replica backend. If no arguments
          were specified, all contents of the replica backend are matched. You
          must use the word lfn, pfn or &lt;name&gt; before specifying a
          pattern. The pattern is meaningful only to the implementation. Thus,
          a SQL implementation may chose to permit SQL wild-card characters. A
          memory-resident service may chose to interpret the pattern as
          regular expression.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">set [var [value]]</emphasis></para>

          <para>sets an internal variable that controls the behavior of the
          front-end. With no arguments, all possible behaviors are displayed.
          With one argument, just the matching behavior is listed. With two
          arguments, the matching behavior is set to the value.</para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>DATABASE SCHEMA</title>

      <para>The tables are set up as part of the PEGASUS database setup. The
      files concerned with the database have a suffix -rc.sql.</para>
    </section>
  </section>

  <section>
    <title>pegasus-tc-client</title>

    <para>A full featured generic client to handle adds, delete and queries to
    the Transformation Catalog (TC).</para>

    <section>
      <title>SYNOPSIS</title>

      <para><emphasis role="bold">pegasus-tc-client</emphasis> [-Dprop [...]]
      OPERATION TRIGGERS [OPTIONS] [-h] [-v] [-V]</para>
    </section>

    <section>
      <title>DESCRIPTION</title>

      <para>The tc-client command is a generic client that performs the three
      basic operation of adding, deleting and querying of any Transformation
      Catalog impemented to the TC API. The client implements all the
      operations supported by the TC Api. It is upto the TC implementation
      whether they support all operations or modes.</para>

      <para></para>

      <para>The following 3 operations are supported by the tc-client. One of
      these operations have to be specified to run the client.</para>

      <orderedlist>
        <listitem>
          <para><emphasis role="bold">ADD</emphasis></para>

          <para>This operation allows the client to add or update entries in
          the Transformation Catalog. Entries can be added one by one on the
          command line or in bulk by using the BULK Trigger and pro‚Äê viding a
          file with the necessary entries. Also Profiles can be added to
          either the logical transformation or the physical
          transformation.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">DELETE</emphasis></para>

          <para>This operation allows the client to delete entries from the
          Transformation Catalog. Entries can be deleted based on logical
          transformation, by resource, by transformation type as well as the
          transformation system information. Also Profiles associated with the
          logical or physical transformation can be deleted.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">QUERY</emphasis></para>

          <para>This opeartion allows the client to query for entries from the
          Transformation Catalog. Queries can be made for printing all the
          contents of the Catalog or for specific entries, for all the logical
          transformations or resources etc. See the TRIGGERS and VALID
          COMBINATIONS section for more details.</para>
        </listitem>
      </orderedlist>
    </section>

    <section>
      <title>OPERATIONS</title>

      <para>To select one of the 3 operations.</para>

      <para></para>

      <orderedlist>
        <listitem>
          <para><emphasis role="bold">-a , --add</emphasis></para>

          <para>Perform addition operations on the TC.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-d , --delete </emphasis></para>

          <para>Perform delete operations on the TC.</para>
        </listitem>

        <listitem>
          <para><emphasis>-q , --query</emphasis></para>

          <para>Perform query operations on the TC.</para>
        </listitem>
      </orderedlist>
    </section>

    <section>
      <title>TRIGGERS</title>

      <para>Triggers modify an OPERATIONS behaviour. E.g. if you want to
      perform a bulk operation you would use a BULK Trigger or if you want to
      perform an operation on a Logical Transformation then you would use the
      LFN Trigger.</para>

      <para></para>

      <para>The following 7 Triggers are available. See the VALID COMBINATIONS
      section for the correct grouping and usage.</para>

      <para></para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">-B </emphasis></para>

          <para>Triggers a bulk operation.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-L</emphasis></para>

          <para><emphasis role="bold"><emphasis>Triggers an operation on a
          logical transformation.</emphasis></emphasis></para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-P</emphasis></para>

          <para><emphasis role="bold"><emphasis>Triggers an operation on a
          physical transformation.</emphasis></emphasis></para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-R</emphasis></para>

          <para><emphasis role="bold"><emphasis>Triggers an operation on a
          resource.</emphasis></emphasis></para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-E</emphasis></para>

          <para><emphasis role="bold"><emphasis>Triggers an operation on a
          Profile.</emphasis></emphasis></para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-T</emphasis></para>

          <para><emphasis role="bold"><emphasis>Triggers an operation on a
          Type.</emphasis></emphasis></para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-S</emphasis></para>

          <para><emphasis role="bold"><emphasis>Triggers an operation on a
          System information.</emphasis></emphasis></para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>OPTIONS</title>

      <para>The following options are applicable for all the
      operations.</para>

      <para></para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">-l, --lfn logicalTR</emphasis></para>

          <para>The logical transformation to be added. The format is
          NAMESPACE::NAME:VERSION. The name is always required, namespace and
          version are optional.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-p, --pfn physical TR</emphasis></para>

          <para>The physical transfromation to be added. For INSTALLED
          executables its a local file path, for all others its a url.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-t , --type type</emphasis></para>

          <para>The type of physical transformation. Valid values are :
          INSTALLED, STATIC_BINARY, DYNAMIC_BINARY, SCRIPT, SOURCE,
          PACMAN_PACKAGE.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-r , --resource
          resourceID</emphasis></para>

          <para>The resourceID where the transformation is located.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-e , --profile
          profiles</emphasis></para>

          <para>The profiles for the transformation. Mulitple profiles of same
          namespace can be added simultaneously by seperating them with a
          comma ",". Each profile section is written as
          NAMESPACE::KEY=VALUE,KEY2=VALUE2 e.g.
          ENV::JAVA_HOME=/usr/bin/java2,PEGASUS_HOME=/usr/local/pegasus. To
          add muliple namespaces you need to repeat the -e option for each
          namespace.</para>

          <para>e.g. -e ENV::JAVA_HOME=/usr/bin/java -e
          GLOBUS::JobType=MPI,COUNT=10</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">-s , --system
          systeminfo</emphasis></para>

          <para>The architecture, os, osversion and glibc if any for the
          executable. Each system info is written in the form
          ARCH::OS:OSVER:GLIBC</para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>OTHER OPTIONS</title>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">-Dprop</emphasis></para>

          <para>The -D options allows an experienced user to override certain
          properties which influence the program execution, among them the
          default location of the user's properties file and the PEGASUS home
          location. One may set several CLI properties by giving this option
          multiple times. The -D option(s) must be the first option on the
          command line. A CLI property take precedence over the properties
          file property of the same key.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">--oldformat, -o</emphasis></para>

          <para>Generates the output in the old single line format</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">--conf, -c</emphasis></para>

          <para>path to property file</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">--verbose, -v</emphasis></para>

          <para>increases the verbosity level</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">--version, -V</emphasis></para>

          <para>Displays the version number of the Griphyn Virtual Data System
          software</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">--help, -h</emphasis></para>

          <para>Generates this help</para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>VALID COMBINATIONS</title>

      <para>The following are valid combinations of OPERATIONS, TRIGGERS,
      OPTIONS for the tc-client</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">ADD</emphasis></para>

          <itemizedlist>
            <listitem>
              <para><emphasis>ADD TC ENTRY</emphasis></para>

              <para><emphasis><emphasis>-a -l lfn -p pfn -t type -r resource
              -s system [-e profiles..] </emphasis></emphasis></para>

              <para><emphasis><emphasis>Adds a single entry into the
              transformation catalog.</emphasis></emphasis></para>
            </listitem>

            <listitem>
              <para><emphasis>ADD PFN PROFILE</emphasis></para>

              <para>-a -P -E -p pfn -t type -r resource -e profiles
              ....</para>

              <para>Adds profiles to a specified physical transformation on a
              given resource and of a given type.</para>
            </listitem>

            <listitem>
              <para><emphasis>ADD LFN PROFILE</emphasis></para>

              <para>-a -L -E -l lfn -e profiles ....</para>

              <para>Adds profiles to a specified logical
              transformation.</para>
            </listitem>

            <listitem>
              <para><emphasis>Add Bulk Entries</emphasis></para>

              <para>-a -B -f file</para>

              <para>Adds entries in bulk mode by supplying a file containg the
              entries.</para>

              <para>The format of the file cotnains 6 columns. E.g.</para>

              <para>#RESOURCE LFN PFN TYPE SYSINFO PROFILES # isi NS::NAME:VER
              /bin/date INSTALLED ARCH::OS:OSVERS:GLIBC
              NS::KEY=VALUE,KEY=VALUE;NS2::KEY=VALUE,KEY=VALUE</para>
            </listitem>
          </itemizedlist>
        </listitem>

        <listitem>
          <para><emphasis role="bold">DELETE</emphasis></para>

          <itemizedlist>
            <listitem>
              <para><emphasis>Delete all TC </emphasis></para>

              <para>-d -BPRELST</para>

              <para>Deletes the entire contents of the TC. WARNING : USE WITH
              CAUTION.</para>
            </listitem>

            <listitem>
              <para><emphasis>Delete by LFN </emphasis></para>

              <para>-d -L -l lfn [-r resource] [-t type]</para>

              <para>Deletes entries from the TC for a particular logical
              transformation and additionaly a resource and or type.</para>
            </listitem>

            <listitem>
              <para><emphasis>Delete by PFN </emphasis></para>

              <para>-d -P -l lfn -p pfn [-r resource] [-t type]</para>

              <para>Deletes entries from the TC for a given logical and
              physical transformation and additionaly on a particular resource
              and or of a particular type.</para>
            </listitem>

            <listitem>
              <para><emphasis>Delete by Type </emphasis></para>

              <para>-d -T -t type [-r resource]</para>

              <para>Deletes entries from TC of a specific type and/or on a
              specific resource.</para>
            </listitem>

            <listitem>
              <para><emphasis>Delete by Resource </emphasis></para>

              <para>-d -R -r resource</para>

              <para>Deletes the entries from the TC on a particular
              resource.</para>
            </listitem>

            <listitem>
              <para><emphasis>Delete by SysInfo </emphasis></para>

              <para>-d -S -s sysinfo</para>

              <para>Deletes the entries from the TC for a particular system
              information type.</para>
            </listitem>

            <listitem>
              <para><emphasis>Delete Pfn Profile </emphasis></para>

              <para>-d -P -E -p pfn -r resource -t type [-e profiles
              ..]</para>

              <para>Deletes all or specific profiles associated with a
              physical transformation.</para>
            </listitem>

            <listitem>
              <para><emphasis>Delete Lfn Profile </emphasis></para>

              <para>-d -L -E -l lfn -e profiles ....</para>

              <para>Deletes all or specific profiles associated with a logical
              transformation.</para>
            </listitem>
          </itemizedlist>
        </listitem>

        <listitem>
          <para><emphasis role="bold">QUERY</emphasis></para>

          <itemizedlist>
            <listitem>
              <para><emphasis>Query Bulk </emphasis></para>

              <para>-q -B Queries for all the contents of the TC.</para>

              <para>It produces a file format TC which can be added to another
              TC using the bulk option.</para>
            </listitem>

            <listitem>
              <para><emphasis>Query LFN </emphasis></para>

              <para>-q -L [-r resource] [-t type]</para>

              <para>Queries the TC for logical transformation and/or on a
              particular resource and/or of a particular type.</para>
            </listitem>

            <listitem>
              <para><emphasis> Query PFN </emphasis></para>

              <para>-q -P -l lfn [-r resource] [-t type]</para>

              <para>Queries the TC for physical transformations for a give
              logical transformation and/or on a particular resource and/or of
              a particular type.</para>
            </listitem>

            <listitem>
              <para><emphasis>Query Resource </emphasis></para>

              <para>-q -R -l lfn [-t type]</para>

              <para>Queries the TC for resources that are registered and/or
              resources registered for a specific type of
              transformation.</para>
            </listitem>

            <listitem>
              <para><emphasis>Query Lfn Profile </emphasis></para>

              <para>-q -L -E -l lfn</para>

              <para>Queries for profiles associated with a particular logical
              transformation</para>
            </listitem>

            <listitem>
              <para><emphasis>Query Pfn Profile </emphasis></para>

              <para>-q -P -E -p pfn -r resource -t type</para>

              <para>Queries for profiles associated with a particular physical
              transformation</para>
            </listitem>
          </itemizedlist>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>PROPERTIES</title>

      <para>This are the properties you will need to set to use either the
      File or Text TC. For more details please check the
      $PEGASUS_HOME/etc/sample.properties file.</para>

      <itemizedlist>
        <listitem>
          <para><emphasis
          role="bold">pegasus.catalog.transformation</emphasis></para>

          <para>Identifies what impelemntation of TC will be used. If relative
          name is used then the path org.griphyn.cPlanner.tc is prefixed to
          the name and used as the class name to load. The default value is
          Text. Other supported mode is File</para>
        </listitem>

        <listitem>
          <para><emphasis
          role="bold">pegasus.catalog.transformation.file</emphasis></para>

          <para>The file path where the text based TC is located. By default
          the path $PEGASUS_HOME/var/tc.data is used.</para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>FILES</title>

      <itemizedlist>
        <listitem>
          <para><emphasis
          role="bold">$PEGASUS_HOME/var/tc.data</emphasis></para>

          <para>is the suggested location for the file corresponding to the
          Transformation Catalog</para>
        </listitem>

        <listitem>
          <para><emphasis
          role="bold">$PEGASUS_HOME/etc/properties</emphasis></para>

          <para>is the location to specify properties to change what
          Tranformation Catalog Implementation to use and the implementation
          related PROPERTIES.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">pegasus.jar</emphasis></para>

          <para>contains all compiled Java bytecode to run the PEGASUS
          Planner.</para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>ENVIRONMENT VARIABLES</title>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">$PEGASUS_HOME </emphasis></para>

          <para>Path to the PEGASUS installation directory.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">$JAVA_HOME </emphasis></para>

          <para>Path to the JAVA 1.4.x installation directory.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">$CLASSPATH </emphasis></para>

          <para>The classpath should be set to contain all necessary PEGASUS
          files for the execution environment. To automatically add the
          CLASSPATH to you environment, in the $PEGASUS_HOME directory run the
          script source setup-user-env.csh or source setup-user-env.sh.</para>
        </listitem>
      </itemizedlist>
    </section>
  </section>

  <section>
    <title>pegasus-s3</title>

    <para>pegasus-s3 is a client for the Amazon S3 object storage service and
    any other storage services that conform to the Amazon S3 API, such as
    Eucalyptus Walrus.</para>

    <section>
      <title>URL Format</title>

      <para>All URLs for objects stored in S3 should be specified in the
      following format:</para>

      <programlisting>s3[s]://USER@SITE[/BUCKET[/KEY]]</programlisting>

      <para>The protocol part can be s3:// or s3s://. If s3s:// is used, then
      pegasus-s3 will force the connection to use SSL and override the setting
      in the configuration file. If s3:// is used, then whether the connection
      uses SSL or not is determined by the value of the 'endpoint' variable in
      the configuration for the site.</para>

      <para>The <emphasis>USER@SITE</emphasis> part is required, but the
      <emphasis>BUCKET</emphasis> and <emphasis>KEY</emphasis> parts may be
      optional depending on the context.</para>

      <para>The <emphasis>USER@SITE</emphasis> portion is referred to as the
      'identity', and the <emphasis>SITE</emphasis> portion is referred to as
      the site. Both the identity and the site are looked up in the
      configuration file (see pegasus-s3 Configuration) to determine the
      parameters to use when establishing a connection to the service. The
      site portion is used to find the host and port, whether to use SSL, and
      other things. The identity portion is used to determine which
      authentication tokens to use. This format is designed to enable users to
      easily use multiple services with multiple authentication tokens. Note
      that neither the <emphasis>USER</emphasis> nor the
      <emphasis>SITE</emphasis> portion of the URL have any meaning outside of
      pegasus-s3. They do not refer to real usernames or hostnames, but are
      rather handles used to look up configuration values in the configuration
      file.</para>

      <para>The <emphasis>BUCKET</emphasis> portion of the URL is the part
      between the 3rd and 4th slashes. Buckets are part of a global namespace
      that is shared with other users of the storage service. As such, they
      should be unique.</para>

      <para>The <emphasis>KEY</emphasis> portion of the URL is anything after
      the 4th slash. Keys can include slashes, but S3-like storage services do
      not have the concept of a directory like regular file systems. Instead,
      keys are treated like opaque identifiers for individual objects. So, for
      example, the keys 'a/b' and 'a/c' have a common prefix, but cannot be
      said to be in the same 'directory'.</para>

      <para>Some example URLs are:</para>

      <programlisting>s3://ewa@amazon
s3://juve@skynet/gideon.isi.edu
s3://juve@magellan/pegasus-images/centos-5.5-x86_64-20101101.part.1
s3s://ewa@amazon/pegasus-images/data.tar.gz</programlisting>
    </section>

    <section>
      <title>Subcommands</title>

      <para>pegasus-s3 has several subcommands for different storage service
      operations.</para>

      <section>
        <title>help</title>

        <para><emphasis role="bold">pegasus-s3 help</emphasis></para>

        <para>The <emphasis role="bold">help</emphasis> subcommand lists all
        available subcommands.</para>
      </section>

      <section>
        <title>ls</title>

        <para><emphasis role="bold">pegasus-s3 ls [options]
        URL...</emphasis></para>

        <para>The <emphasis role="bold">ls</emphasis> subcommand lists the
        contents of a URL. If the URL does not contain a bucket, then all the
        buckets owned by the user are listed. If the URL contains a bucket,
        but no key, then all the keys in the bucket are listed. If the URL
        contains a bucket and a key, then all keys in the bucket that begin
        with the specified key are listed.</para>
      </section>

      <section>
        <title>mkdir</title>

        <para><emphasis role="bold">pegasus-s3 mkdir [options]
        URL...</emphasis></para>

        <para>The <emphasis role="bold">mkdir</emphasis> subcommand creates
        one or more buckets.</para>
      </section>

      <section>
        <title>rmdir</title>

        <para><emphasis role="bold">pegasus-s3 rmdir [options]
        URL...</emphasis></para>

        <para>The <emphasis role="bold">rmdir</emphasis> subcommand deletes
        one or more buckets from the storage service. In order to delete a
        bucket, the bucket must be empty.</para>
      </section>

      <section>
        <title>rm</title>

        <para><emphasis role="bold">pegasus-s3 rm [options]
        URL...</emphasis></para>

        <para>The <emphasis role="bold">rm</emphasis> subcommand deletes one
        or more keys from the storage service.</para>
      </section>

      <section>
        <title>put</title>

        <para><emphasis role="bold">pegasus-s3 put [options] FILE
        URL</emphasis></para>

        <para>The <emphasis role="bold">put</emphasis> subcommand stores the
        file specified by <emphasis>FILE</emphasis> in the storage service
        under the bucket and key specified by <emphasis>URL</emphasis>. If the
        URL contains a bucket, but not a key, then the file name is used as
        the key.</para>

        <para>If a transient failure occurs, then the upload will be retried
        several times before pegasus-s3 gives up and fails.</para>

        <para>The put subcommand can do both chunked and parallel uploads if
        the service supports multipart uploads (see multipart_uploads in the
        configuration). Currently only Amazon S3 supports multipart
        uploads.</para>

        <para>This subcommand will check the size of the file to make sure it
        can be stored before attempting to store it.</para>

        <para>Chunked uploads are useful to reduce the probability of an
        upload failing. If an upload is chunked, then pegasus-s3 issues
        separate PUT requests for each chunk of the file. Specifying smaller
        chunks (using --chunksize) will reduce the chances of an upload
        failing due to a transient error. Chunksizes can range from 5 MB to
        1GB (chunk sizes smaller than 5 MB produced incomplete uploads on
        Amazon S3). The maximum number of chunks for any single file is
        10,000, so if a large file is being uploaded with a small chunksize,
        then the chunksize will be increased to fit within the 10,000 chunk
        limit. By default, the file will be split into 10 MB chunks if the
        storage service supports multipart uploads. Chunked uploads can be
        disabled by specifying a chunksize of 0. If the upload is chunked,
        then each chunk is retried independently under transient failures. If
        any chunk fails permanently, then the upload is aborted.</para>

        <para>Parallel uploads can increase performance for services that
        support multipart uploads. In a parallel upload the file is split into
        N chunks and each chunk is uploaded concurrently by one of M threads
        in first-come, first-served fashion. If the chunksize is set to 0,
        then parallel uploads are disabled. If M &gt; N, then the actual
        number of threads used will be reduced to N. The number of threads can
        be specified using the --parallel argument. If --parallel is 0 or 1,
        then only a single thread is used. The default value is 0. There is no
        maximum number of threads, but it is likely that the link will be
        saturated by ~4 threads. Very high-bandwidth, long-delay links may get
        better results with up to ~8 threads.</para>

        <note>
          <para>Under certain circumstances, when a multipart upload fails it
          could leave behind data on the server. When a failure occurs the put
          subcommand will attempt to abort the upload. If the upload cannot be
          aborted, then a partial upload may remain on the server. To check
          for partial uploads run the <emphasis role="bold">lsup</emphasis>
          subcommand. If you see an upload that failed in the output of lsup,
          then run the <emphasis role="bold">rmup</emphasis> subcommand to
          remove it.</para>
        </note>
      </section>

      <section>
        <title>get</title>

        <para><emphasis role="bold">pegasus-s3 get [options] URL
        [FILE]</emphasis></para>

        <para>The <emphasis role="bold">get</emphasis> subcommand retrives an
        object from the storage service identified by <emphasis>URL</emphasis>
        and stores it in the file specified by <emphasis>FILE</emphasis>. If
        FILE is not specified, then the key is used as the file name (Note: if
        the key has slashes, then the file name will be a relative
        subdirectory, but pegasus-s3 will not create the subdirectory if it
        does not exist).</para>

        <para>If a transient failure occurs, then the download will be retried
        several times before pegasus-s3 gives up and fails.</para>

        <para>The get subcommand can do both chunked and parallel downloads if
        the service supports ranged downloads (see ranged_downloads in the
        configuration). Currently only Amazon S3 has good support for ranged
        downloads. Eucalyptus Walrus supports ranged downloads, but the
        current release, 1.6, is inconsistent with the Amazon interface and
        has a bug that causes ranged downloads to hang in some cases. It is
        recommended that ranged downloads not be used with Eucalyptus until
        these issues are resolved.</para>

        <para>Chunked downloads can be used to reduce the probability of a
        download failing. When a download is chunked, pegasus-s3 issues
        separate GET requests for each chunk of the file. Specifying smaller
        chunks (uisng --chunksize) will reduce the chances that a download
        will fail to do a transient error. Chunk sizes can range from 1 MB to
        1 GB. By default, a download will be split into 10 MB chunks if the
        site supports ranged downloads. Chunked downloads can be disabled by
        specifying a chunksize of 0. If a download is chunked, then each chunk
        is retried independently under transient failures. If any chunk fails
        permanently, then the download is aborted.</para>

        <para>Parallel downloads can increase performance for services that
        support ranged downloads. In a parallel download, the file to be
        retrieved is split into N chunks and each chunk is downloaded
        concurrently by one of M threads in a first-come, first-served
        fashion. If the chunksize is 0, then parallel downloads are disabled.
        If M &gt; N, then the actual number of threads used will be reduced to
        N. The number of threads can be specified using the --parallel
        argument. If --parallel is 0 or 1, then only a single thread is used.
        The default value is 0. There is no maximum number of threads, but it
        is likely that the link will be saturated by ~4 threads. Very
        high-bandwidth, long-delay links may get better results with up to ~8
        threads.</para>
      </section>

      <section>
        <title>lsup</title>

        <para><emphasis role="bold">pegasus-s3 lsup [options]
        URL</emphasis></para>

        <para>The <emphasis role="bold">lsup</emphasis> subcommand lists
        active uploads. The URL specified should point to a bucket. This
        command is only valid if the site supports multipart uploads. The
        output of this command is a list of keys and upload IDs.</para>

        <para>This subcommand is used with <emphasis
        role="bold">rmup</emphasis> to help recover from failures of multipart
        uploads.</para>
      </section>

      <section>
        <title>rmup</title>

        <para><emphasis role="bold">pegasus-s3 rmup [options] URL
        UPLOAD</emphasis></para>

        <para>The <emphasis role="bold">rmup</emphasis> subcommand cancels and
        active upload. The <emphasis>URL</emphasis> specified should point to
        a bucket, and <emphasis>UPLOAD</emphasis> is the long, complicated
        upload ID shown by the <emphasis role="bold">lsup</emphasis>
        subcommand.</para>

        <para>This subcommand is used with <emphasis
        role="bold">lsup</emphasis> to recover from failures of multipart
        uploads.</para>
      </section>
    </section>

    <section>
      <title>pegasus-s3 Configuration</title>

      <para>Each user should specify a configuration file that pegasus-s3 will
      use to look up connection parameters and authentication tokens.</para>

      <section>
        <title>Configuration file search path</title>

        <para>This client will look in the following locations, in order, to
        locate the user's configuration file:</para>

        <orderedlist>
          <listitem>
             The -C/--conf argument 
          </listitem>

          <listitem>
             The S3CFG environment variable 
          </listitem>

          <listitem>
             ~/.s3cfg 
          </listitem>
        </orderedlist>

        <para>If it does not find the configuration file in one of these
        locations it will fail with an error.</para>
      </section>

      <section>
        <title>Configuration file format</title>

        <para>The configuration file is in INI format and contains two types
        of entries.</para>

        <para>The first type of entry is a <emphasis role="bold">site
        entry</emphasis>, which specifies the configuration for a storage
        service. This entry specifies the service endpoint that pegasus-s3
        should connect to for the site, and some optional features that the
        site may support. Here is an example of a site entry for Amazon
        S3:</para>

        <programlisting>[amazon]
endpoint = http://s3.amazonaws.com/</programlisting>

        <para>The other type of entry is an <emphasis role="bold">identity
        entry</emphasis>, which specifies the authentication information for a
        user at a particular site. Here is an example of an identity
        entry:</para>

        <programlisting>[pegasus@amazon]
access_key = 90c4143642cb097c88fe2ec66ce4ad4e
secret_key = a0e3840e5baee6abb08be68e81674dca</programlisting>

        <para>It is important to note that user names and site names used are
        only logical--they do not correspond to actual hostnames or usernames,
        but are simply used as a convenient way to refer to the services and
        identities used by the client.</para>

        <para>The configuration file should be saved with limited permissions.
        Only the owner of the file should be able to read from it and write to
        it (i.e. it should have permissions of 0600 or 0400). If the file has
        more liberal permissions, then pegasus-s3 will fail with an error
        message. The purpose of this is to prevent the authentication tokens
        stored in the configuration file from being accessed by other
        users.</para>
      </section>

      <section>
        <title>Configuration variables</title>

        <table>
          <tgroup cols="3">
            <thead>
              <row>
                <entry>Variable</entry>

                <entry>Scope</entry>

                <entry>Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry>endpoint</entry>

                <entry>site</entry>

                <entry>The URL of the web service endpoint. If the URL begins
                with 'https', then SSL will be used.</entry>
              </row>

              <row>
                <entry>max_object_size</entry>

                <entry>site</entry>

                <entry>The maximum size of an object in GB (default:
                5GB)</entry>
              </row>

              <row>
                <entry>multipart_uploads</entry>

                <entry>site</entry>

                <entry>Does the service support multipart uploads (True/False,
                default: False)</entry>
              </row>

              <row>
                <entry>ranged_downloads</entry>

                <entry>site</entry>

                <entry>Does the service support ranged downloads? (True/False,
                default: False)</entry>
              </row>

              <row>
                <entry>access_key</entry>

                <entry>identity</entry>

                <entry>The access key for the identity</entry>
              </row>

              <row>
                <entry>secret_key</entry>

                <entry>identity</entry>

                <entry>The secret key for the identity</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>

      <section>
        <title>Example configuration</title>

        <para>This is an example configuration that specifies a single site
        (amazon) and a single identity (pegasus@amazon). For this site the
        maximum object size is 5TB, and the site supports both multipart
        uploads and ranged downloads, so both uploads and downloads can be
        done in parallel.</para>

        <programlisting>[amazon]
endpoint = https://s3.amazonaws.com/
max_object_size = 5120
multipart_uploads = True
ranged_downloads = True

[pegasus@amazon]
access_key = 90c4143642cb097c88fe2ec66ce4ad4e
secret_key = a0e3840e5baee6abb08be68e81674dca

[magellan]
# NERSC Magellan is a Eucalyptus site. It doesn't support multipart uploads,
# or ranged downloads (the defaults), and the maximum object size is 5GB
# (also the default)
endpoint = https://128.55.69.235:8773/services/Walrus

[juve@magellan]
access_key = quwefahsdpfwlkewqjsdoijldsdf
secret_key = asdfa9wejalsdjfljasldjfasdfa

[voeckler@magellan]
# Each site can have multiple associated identities
access_key = asdkfaweasdfbaeiwhkjfbaqwhei
secret_key = asdhfuinakwjelfuhalsdflahsdl</programlisting>
      </section>
    </section>
  </section>

  <section>
    <title>pegasus-exitcode</title>

    <para>pegasus-exitcode is a utility that examines the STDOUT of a job to
    determine if the job failed, and renames the STDOUT and STDERR files of a
    job to preserve them in case the job is retried.</para>

    <para>Pegasus uses pegasus-exitcode as the DAGMan postscript for all jobs
    submitted via Globus GRAM. This tool exists as a workaround to a known
    problem with Globus where the exitcodes of GRAM jobs are not returned.
    This is a problem because Pegasus uses the exitcode of a job to determine
    if the job failed or not.</para>

    <para>In order to get around the exitcode problem, Pegasus wraps all GRAM
    jobs with Kickstart, which records the exitcode of the job in an XML
    invocation record, which it writes to the job's STDOUT. The STDOUT is
    transferred from the execution host back to the submit host when the job
    terminates. After the job terminates, DAGMan runs the job's postscript,
    which Pegasus sets to be pegasus-exitcode. pegasus-exitcode looks at the
    invocation record generated by kickstart to see if the job succeeded or
    failed. If the invocation record indicates a failure, then
    pegasus-exitcode returns a non-zero result, which indicates to DAGMan that
    the job has failed. If the invocation record indicates that the job
    succeeded, then pegasus-exitcode returns 0, which tells DAGMan that the
    job succeeeded.</para>

    <para>pegasus-exitcode performs several checks to determine whether a job
    failed or not. These checks include:</para>

    <orderedlist>
      <listitem>
        <para>Is STDOUT empty? If it is empty, then the job failed.</para>
      </listitem>

      <listitem>
        <para>Are there any &lt;status&gt; tags with a non-zero value? If
        there are, then the job failed. Note that, if this is a clustered job,
        there could be multiple &lt;status&gt; tags, one for each task. If any
        of them are non-zero, then the job failed.</para>
      </listitem>

      <listitem>
        <para>Is there at least one &lt;status&gt; tag with a zero value?
        There must be at least one successful invocation or the job has
        failed.</para>
      </listitem>
    </orderedlist>

    <para>In addition, pegasus-exitcode allows the caller to specify the
    exitcode returned by Condor using the --return argument. This can be
    passed to pegasus-exitcode in a DAGMan post script by using the $RETURN
    variable. If this value is non-zero, then pegasus-exitcode returns a
    non-zero result before performing any other checks. For GRAM jobs, the
    value of $RETURN will always be 0 regardless of whether the job failed or
    not.</para>

    <para>Also, pegasus-exitcode allows the caller to specify the number of
    successful tasks it should see using the --tasks argument. If
    pegasus-exitcode does not see N successful tasks, where N is set by
    --tasks, then it will return a non-zero result. The default value is 1.
    This can be used to detect failures in clustered jobs where, for any
    number of reasons, invocation records do not get generated for all the
    tasks in the clustered job.</para>

    <para>In addition to checking the success/failure of a job,
    pegasus-exitcode also renames the STDOUT and STDERR files of the job so
    that if the job is retried, the STDOUT and STDERR of the previous run are
    not lost. It does this by appending a sequence number to the end of the
    files. For example, if the STDOUT file is called "job.out", then the first
    time the job is run pegasus-exitcode will rename the file "job.out.000".
    If the job is run again, then pegasus-exitcode sees that "job.out.000"
    already exists and renames the file "job.out.001". It will continue to
    rename the file by incrementing the sequence number every time the job is
    executed.</para>
  </section>

  <section>
    <title>Kickstart</title>

    <para>Kickstart is a job wrapper that collects data about a job's
    execution environment, performance, and output.</para>
  </section>
</section>
