<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="running_workflows">
  <title>Running Workflows</title>

  <section id="executable_workflows">
    <title>Executable Workflows (DAG)</title>

    <para>The DAG is an executable (concrete) workflow that can be executed
    over a variety of resources. When the workflow tasks are mapped to
    multiple resources that do not share a file system, explicit nodes are
    added to the workflow for orchestrating data. transfer between the
    tasks.</para>

    <para>When you take the DAX workflow created in <link
    linkend="creating_workflows">Creating Workflows</link>, and plan it for a
    single remote grid execution, here a site with handle <emphasis
    role="bold">hpcc</emphasis>, and plan the workflow without clean-up nodes,
    the following concrete workflow is built:</para>

    <para><figure id="concepts-fig-dag">
        <title>Black Diamond DAG</title>

        <mediaobject>
          <imageobject>
            <imagedata align="center"
                       fileref="images/concepts-diamond-dag.png"
                       valign="middle"/>
          </imageobject>
        </mediaobject>
      </figure></para>

    <para>Planning augments the original abstract workflow with ancillary
    tasks to facility the proper execution of the workflow. These tasks
    include:</para>

    <itemizedlist>
      <listitem>
        <para>the creation of remote working directories. These directories
        typically have name that seeks to avoid conflicts with other
        simultaneously running similar workflows. Such tasks use a job prefix
        of <code>create_dir</code>.</para>
      </listitem>

      <listitem>
        <para>the stage-in of input files before any task which requires these
        files. Any file consumed by a task needs to be staged to the task, if
        it does not already exist on that site. Such tasks use a job prefix of
        <code>stage_in</code>.If multiple files from various sources need to
        be transferred, multiple stage-in jobs will be created. Additional
        advanced options permit to control the size and number of these jobs,
        and whether multiple compute tasks can share stage-in jobs.</para>
      </listitem>

      <listitem>
        <para>the original DAX job is concretized into a compute task in the
        DAG. Compute jobs are a concatination of the job's <emphasis
        role="bold">name</emphasis> and <emphasis role="bold">id</emphasis>
        attribute from the DAX file.</para>
      </listitem>

      <listitem>
        <para>the stage-out of data products to a collecting site. Data
        products with their <emphasis role="bold">transfer</emphasis> flag set
        to <literal>false</literal> will not be staged to the output site.
        However, they may still be eligible for staging to other, dependent
        tasks. Stage-out tasks use a job prefix of
        <code>stage_out</code>.</para>
      </listitem>

      <listitem>
        <para>If compute jobs run at different sites, an intermediary staging
        task with prefix <code>stage_inter</code> is inserted between the
        compute jobs in the workflow, ensuring that the data products of the
        parent are available to the child job.</para>
      </listitem>

      <listitem>
        <para>the registration of data products in a replica catalog. Data
        products with their <emphasis role="bold">register</emphasis> flag set
        to <literal>false</literal> will not be registered.</para>
      </listitem>

      <listitem>
        <para>the clean-up of transient files and working directories. These
        steps can be omitted with the <command>--no-cleanup</command> option
        to the planner.</para>
      </listitem>
    </itemizedlist>

    <para>The <link linkend="data_management">Data Management</link> chapter
    details more about when and how staging nodes are inserted into the
    workflow.</para>

    <para>The DAG will be found in file <filename>diamond-0.dag</filename>,
    constructed from the <emphasis role="bold">name</emphasis> and <emphasis
    role="bold">index</emphasis> attributes found in the root element of the
    DAX file.</para>

    <programlisting>######################################################################
# PEGASUS WMS GENERATED DAG FILE
# DAG diamond
# Index = 0, Count = 1
######################################################################

JOB create_dir_diamond_0_hpcc create_dir_diamond_0_hpcc.sub
SCRIPT POST create_dir_diamond_0_hpcc /opt/pegasus/default/bin/pegasus-exitcode create_dir_diamond_0_hpcc.out

JOB stage_in_local_hpcc_0 stage_in_local_hpcc_0.sub
SCRIPT POST stage_in_local_hpcc_0 /opt/pegasus/default/bin/pegasus-exitcode stage_in_local_hpcc_0.out

JOB preprocess_ID000001 preprocess_ID000001.sub
SCRIPT POST preprocess_ID000001 /opt/pegasus/default/bin/pegasus-exitcode preprocess_ID000001.out

JOB findrange_ID000002 findrange_ID000002.sub
SCRIPT POST findrange_ID000002 /opt/pegasus/default/bin/pegasus-exitcode findrange_ID000002.out

JOB findrange_ID000003 findrange_ID000003.sub
SCRIPT POST findrange_ID000003 /opt/pegasus/default/bin/pegasus-exitcode findrange_ID000003.out

JOB analyze_ID000004 analyze_ID000004.sub
SCRIPT POST analyze_ID000004 /opt/pegasus/default/bin/pegasus-exitcode analyze_ID000004.out

JOB stage_out_local_hpcc_2_0 stage_out_local_hpcc_2_0.sub
SCRIPT POST stage_out_local_hpcc_2_0 /opt/pegasus/default/bin/pegasus-exitcode stage_out_local_hpcc_2_0.out

PARENT findrange_ID000002 CHILD analyze_ID000004
PARENT findrange_ID000003 CHILD analyze_ID000004
PARENT preprocess_ID000001 CHILD findrange_ID000002
PARENT preprocess_ID000001 CHILD findrange_ID000003
PARENT analyze_ID000004 CHILD stage_out_local_hpcc_2_0
PARENT stage_in_local_hpcc_0 CHILD preprocess_ID000001
PARENT create_dir_diamond_0_hpcc CHILD findrange_ID000002
PARENT create_dir_diamond_0_hpcc CHILD findrange_ID000003
PARENT create_dir_diamond_0_hpcc CHILD preprocess_ID000001
PARENT create_dir_diamond_0_hpcc CHILD analyze_ID000004
PARENT create_dir_diamond_0_hpcc CHILD stage_in_local_hpcc_0
######################################################################
# End of DAG
######################################################################
</programlisting>

    <para>The DAG file declares all jobs and links them to a Condor submit
    file that describes the planned, concrete job. In the same directory as
    the DAG file are all Condor submit files for the jobs from the picture
    plus a number of additional helper files.</para>

    <para>The various instructions that can be put into a DAG file are
    described in <ulink
    url="http://www.cs.wisc.edu/condor/manual/v7.5/2_10DAGMan_Applications.html">Condor's
    DAGMAN documentation</ulink>.The constituents of the submit directory are
    described in the<link linkend="submit_directory"> "Submit Directory
    Details"</link>chapter</para>
  </section>

  <section id="mapping_refinement_steps">
    <title>Mapping Refinement Steps</title>

    <para>During the mapping process, the abstract workflow undergoes a series
    of refinement steps that converts it to an executable form.</para>

    <section id="planning_data_reuse">
      <title>Data Reuse</title>

      <para>The abstract workflow after parsing is optionally handed over to
      the Data Reuse Module. The Data Reuse Algorithm in Pegasus attempts to
      prune all the nodes in the abstract workflow for which the output files
      exist in the Replica Catalog. It also attempts to cascade the deletion
      to the parents of the deleted node for e.g if the output files for the
      leaf nodes are specified, Pegasus will prune out all the workflow as the
      output files in which a user is interested in already exist in the
      Replica Catalog.</para>

      <para>The Data Reuse Algorithm works in two passes</para>

      <para><emphasis role="bold">First Pass</emphasis> - Determine all the
      jobs whose output files exist in the Replica Catalog. An output file
      with the transfer flag set to false is treated equivalent to the file
      existing in the Replica Catalog , if the output file is not an input to
      any of the children of the job X.</para>

      <para><emphasis role="bold">Second Pass</emphasis> - The algorithm
      removes the job whose output files exist in the Replica Catalog and
      tries to cascade the deletion upwards to the parent jobs. We start the
      breadth first traversal of the workflow bottom up.</para>

      <programlisting>( It is already marked for deletion in Pass 1
     OR
      ( ALL of it's children have been marked for deletion
        AND
        ( Node's output files have transfer flags set to false 
          OR
          Node's output files with transfer flag as true have locations recorded in the Replica Catalog
        )
       )
 )</programlisting>

      <tip>
        <para>The Data Reuse Algorithm can be disabled by passing the
        <emphasis role="bold">--force</emphasis> option to
        pegasus-plan.</para>
      </tip>

      <figure>
        <title>Workflow Data Reuse</title>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentdepth="100%"
                       fileref="./images/refinement-data-reuse.png"
                       scalefit="1" width="100%"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center"
                       fileref="./images/refinement-data-reuse.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section id="planning_site_selection">
      <title>Site Selection</title>

      <para>The abstract workflow is then handed over to the Site Selector
      module where the abstract jobs in the pruned workflow are mapped to the
      various sites passed by a user. The target sites for planning are
      specified on the command line using the<emphasis role="bold">
      --sites</emphasis> option to pegasus-plan. If not specified, then
      Pegasus picks up all the sites in the Site Catalog as candidate sites.
      Pegasus will map a compute job to a site only if Pegasus can</para>

      <itemizedlist>
        <listitem>
          <para>find an INSTALLED executable on the site</para>
        </listitem>

        <listitem>
          <para>OR find a STAGEABLE executable that can be staged to the site
          as part of the workflow execution.</para>

          <para>Pegasus supports variety of site selectors with Random being
          the default</para>

          <itemizedlist>
            <listitem>
              <para><emphasis role="bold">Random</emphasis></para>

              <para>The jobs will be randomly distributed among the sites that
              can execute them.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">RoundRobin</emphasis></para>

              <para>The jobs will be assigned in a round robin manner amongst
              the sites that can execute them. Since each site cannot execute
              every type of job, the round robin scheduling is done per level
              on a sorted list. The sorting is on the basis of the number of
              jobs a particular site has been assigned in that level so far.
              If a job cannot be run on the first site in the queue (due to no
              matching entry in the transformation catalog for the
              transformation referred to by the job), it goes to the next one
              and so on. This implementation defaults to classic round robin
              in the case where all the jobs in the workflow can run on all
              the sites.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">Group</emphasis></para>

              <para>Group of jobs will be assigned to the same site that can
              execute them. The use of the<emphasis role="bold"> PEGASUS
              profile key group</emphasis> in the DAX, associates a job with a
              particular group. The jobs that do not have the profile key
              associated with them, will be put in the default group. The jobs
              in the default group are handed over to the "Random" Site
              Selector for scheduling.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">Heft</emphasis></para>

              <para>A version of the HEFT processor scheduling algorithm is
              used to schedule jobs in the workflow to multiple grid sites.
              The implementation assumes default data communication costs when
              jobs are not scheduled on to the same site. Later on this may be
              made more configurable.</para>

              <para>The runtime for the jobs is specified in the
              transformation catalog by associating the <emphasis
              role="bold">pegasus profile key runtime</emphasis> with the
              entries.</para>

              <para>The number of processors in a site is picked up from the
              attribute <emphasis role="bold">idle-nodes</emphasis> associated
              with the vanilla jobmanager of the site in the site
              catalog.</para>
            </listitem>

            <listitem>
              <para><emphasis role="bold">NonJavaCallout</emphasis></para>

              <para>Pegasus will callout to an external site selector.In this
              mode a temporary file is prepared containing the job information
              that is passed to the site selector as an argument while
              invoking it. The path to the site selector is specified by
              setting the property pegasus.site.selector.path. The environment
              variables that need to be set to run the site selector can be
              specified using the properties with a pegasus.site.selector.env.
              prefix. The temporary file contains information about the job
              that needs to be scheduled. It contains key value pairs with
              each key value pair being on a new line and separated by a
              =.</para>

              <para>The following pairs are currently generated for the site
              selector temporary file that is generated in the
              NonJavaCallout.</para>

              <table>
                <title>Key Value Pairs that are currently generated
                for the site selector temporary file that is generated in the
                NonJavaCallout.</title>

                <tgroup cols="2">
                  <tbody>
                    <row>
                      <entry><emphasis role="bold">Key</emphasis></entry>

                      <entry><emphasis role="bold">Value</emphasis></entry>
                    </row>

                    <row>
                      <entry>version</entry>

                      <entry>is the version of the site selector api,currently
                      2.0.</entry>
                    </row>

                    <row>
                      <entry>transformation</entry>

                      <entry>is the fully-qualified definition identifier for
                      the transformation (TR) namespace::name:version.</entry>
                    </row>

                    <row>
                      <entry>derivation</entry>

                      <entry>is the fully qualified definition identifier for
                      the derivation (DV), namespace::name:version.</entry>
                    </row>

                    <row>
                      <entry>job.level</entry>

                      <entry>is the job's depth in the tree of the workflow
                      DAG.</entry>
                    </row>

                    <row>
                      <entry>job.id</entry>

                      <entry>is the job's ID, as used in the DAX file.</entry>
                    </row>

                    <row>
                      <entry>resource.id</entry>

                      <entry>is a pool handle, followed by whitespace,
                      followed by a gridftp server. Typically, each gridftp
                      server is enumerated once, so you may have multiple
                      occurances of the same site. There can be multiple
                      occurances of this key.</entry>
                    </row>

                    <row>
                      <entry>input.lfn</entry>

                      <entry>is an input LFN, optionally followed by a
                      whitespace and file size. There can be multiple
                      occurances of this key,one for each input LFN required
                      by the job.</entry>
                    </row>

                    <row>
                      <entry>wf.name</entry>

                      <entry>label of the dax, as found in the DAX's root
                      element. wf.index is the DAX index, that is incremented
                      for each partition in case of deferred planning.</entry>
                    </row>

                    <row>
                      <entry>wf.time</entry>

                      <entry>is the mtime of the workflow.</entry>
                    </row>

                    <row>
                      <entry>wf.manager</entry>

                      <entry>is the name of the workflow manager being used
                      .e.g condor</entry>
                    </row>

                    <row>
                      <entry>vo.name</entry>

                      <entry>is the name of the virtual organization that is
                      running this workflow. It is currently set to
                      NONE</entry>
                    </row>

                    <row>
                      <entry>vo.group</entry>

                      <entry>unused at present and is set to NONE.</entry>
                    </row>
                  </tbody>
                </tgroup>
              </table>
            </listitem>
          </itemizedlist>
        </listitem>
      </itemizedlist>

      <tip>
        <para>The site selector to use for site selection can be specified by
        setting the property <emphasis
        role="bold">pegasus.selector.site</emphasis></para>
      </tip>

      <figure>
        <title>Workflow Site Selection</title>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentdepth="100%"
                       fileref="./images/refinement-site-selection.png"
                       scalefit="1" width="100%"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center"
                       fileref="./images/refinement-site-selection.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>

    <section id="mapping_job_clustering">
      <title>Job Clustering</title>

      <para>After site selection, the workflow is optionally handed for to the
      job clustering module, which clusters jobs that are scheduled to the
      same site. Clustering is usually done on short running jobs in order to
      reduce the remote execution overheads associated with a job. Clustering
      is described in detail in the <link
      linkend="job_clustering">optimization</link> chapter.</para>

      <tip>
        <para>The job clustering is turned on by passing the <emphasis
        role="bold">--cluster</emphasis> option to pegasus-plan.</para>
      </tip>
    </section>

    <section>
      <title id="planning_add_txfer_nodes">Addition of Data Transfer and
      Registration Nodes</title>

      <para>After job clustering, the workflow is handed to the Data Transfer
      module that adds data stage-in , inter site and stage-out nodes to the
      workflow. Data Stage-in Nodes transfer input data required by the
      workflow from the locations specified in the Replica Catalog to a
      directory on the staging site associated with the job. The staging site
      for a job is the execution site if running in a sharedfs mode, else it
      is the one specified by <emphasis role="bold">--staging-site</emphasis>
      option to the planner. In case, multiple locations are specified for the
      same input file, the location from where to stage the data is selected
      using a <emphasis role="bold">Replica Selector</emphasis> . Replica
      Selection is described in detail in the <link
      linkend="replica_selection">Replica Selection</link> section of the
      <link linkend="data_management">Data Management</link> chapter. More
      details about staging site can be found in the <link
      linkend="data_staging_configuration">data staging configuration</link>
      chapter.</para>

      <para>The process of adding the data stage-in and data stage-out nodes
      is handled by Transfer Refiners. All data transfer jobs in Pegasus are
      executed using <emphasis role="bold">pegasus-transfer</emphasis> . The
      pegasus-transfer client is a python based wrapper around various
      transfer clients like globus-url-copy, s3cmd, irods-transfer, scp, wget,
      cp, ln . It looks at source and destination url and figures out
      automatically which underlying client to use. pegasus-transfer is
      distributed with the PEGASUS and can be found in the bin subdirectory .
      Pegasus Transfer Refiners are are described in the detail in the
      Transfers section of the <link linkend="data_management">Data
      Management</link> chapter. The default transfer refiner that is used in
      Pegasus is the <emphasis role="bold">BalancedCluster</emphasis> Transfer
      Refiner, that clusters data stage-in nodes and data stage-out nodes per
      level of the workflow, on the basis of certain pegasus profile keys
      associated with the workflow.</para>

      <figure>
        <title>Addition of Data Transfer Nodes to the Workflow</title>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentdepth="100%"
                       fileref="./images/refinement-transfer-jobs.png"
                       scalefit="1" width="100%"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center"
                       fileref="./images/refinement-transfer-jobs.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>Data Registration Nodes may also be added to the final executable
      workflow to register the location of the output files on the final
      output site back in the Replica Catalog . An output file is registered
      in the Replica Catalog if the register flag for the file is set to true
      in the DAX.</para>

      <figure>
        <title>Addition of Data Registration Nodes to the Workflow</title>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentdepth="100%"
                       fileref="./images/refinement-registration-jobs.png"
                       scalefit="1" width="100%"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center"
                       fileref="./images/refinement-registration-jobs.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The data staged-in and staged-out from a directory that is created
      on the head node by a create dir job in the workflow. In the vanilla
      case, the directory is visible to all the worker nodes and compute jobs
      are launched in this directory on the shared filesystem. In the case
      where there is no shared filesystem, users can turn on worker node
      execution, where the data is staged from the head node directory to a
      directory on the worker node filesystem. This feature will be refined
      further for Pegasus 3.1. To use it with Pegasus 3.0 send email to
      <emphasis role="bold">pegasus-support at isi.edu</emphasis>.</para>

      <tip>
        <para>The replica selector to use for replica selection can be
        specified by setting the property <emphasis
        role="bold">pegasus.selector.replica</emphasis></para>
      </tip>
    </section>

    <section id="planning_createdir_cleanup">
      <title>Addition of Create Dir and Cleanup Jobs</title>

      <para>After the data transfer nodes have been added to the workflow,
      Pegasus adds a create dir jobs to the workflow. Pegasus usually ,
      creates one workflow specific directory per compute site , that is on
      the staging site associated with the job. In the case of shared shared
      filesystem setup, it is a directory on the shared filesystem of the
      compute site. In case of shared filesystem setup, this directory is
      visible to all the worker nodes and that is where the data is staged-in
      by the data stage-in jobs.</para>

      <para>The staging site for a job is the execution site if running in a
      sharedfs mode, else it is the one specified by <emphasis
      role="bold">--staging-site</emphasis> option to the planner. More
      details about staging site can be found in the <link
      linkend="data_staging_configuration">data staging configuration</link>
      chapter.</para>

      <para>After addition of the create dir jobs, the workflow is optionally
      handed to the cleanup module. The cleanup module adds cleanup nodes to
      the workflow that remove data from the directory on the shared
      filesystem when it is no longer required by the workflow. This is useful
      in reducing the peak storage requirements of the workflow.</para>

      <tip>
        <para>The addition of the cleanup nodes to the workflow can be
        disabled by passing the <emphasis role="bold">--nocleanup</emphasis>
        option to pegasus-plan.</para>
      </tip>

      <figure>
        <title>Addition of Directory Creation and File Removal Jobs</title>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentdepth="100%"
                       fileref="./images/refinement-creadir-rm-jobs.png"
                       scalefit="1" width="100%"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center"
                       fileref="./images/refinement-creadir-rm-jobs.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <tip>
        <para>Users can specify the maximum number of cleanup jobs added per
        level by specifying the property <emphasis
        role="bold">pegasus.file.cleanup.clusters.num</emphasis> in the
        properties.</para>
      </tip>
    </section>

    <section>
      <title id="planning_code_generation">Code Generation</title>

      <para>The last step of refinement process, is the code generation where
      Pegasus writes out the executable workflow in a form understandable by
      the underlying workflow executor. At present Pegasus supports the
      following code generators</para>

      <orderedlist>
        <listitem>
          <para><emphasis role="bold">Condor</emphasis></para>

          <para>This is the default code generator for Pegasus . This
          generator generates the executable workflow as a Condor DAG file and
          associated job submit files. The Condor DAG file is passed as input
          to Condor DAGMan for job execution.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Shell</emphasis></para>

          <para>This Code Generator generates the executable workflow as a
          shell script that can be executed on the submit host. While using
          this code generator, all the jobs should be mapped to site local i.e
          specify <emphasis role="bold">--sites local </emphasis> to
          pegasus-plan.</para>

          <tip>
            <para>To use the Shell code Generator set the property <emphasis
            role="bold">pegasus.code.generator</emphasis> Shell</para>
          </tip>
        </listitem>

        <listitem>
          <para><emphasis role="bold">PMC</emphasis></para>

          <para>This Code Generator generates the executable workflow as a PMC
          task workflow. This is useful to run on platforms where it not
          feasible to run Condor such as the new XSEDE machines such as Blue
          Waters. In this mode, Pegasus will generate the executable workflow
          as a PMC task workflow and a sample PBS submit script that submits
          this workflow. Note that the generated PBS file needs to be manually
          updated before it can be submitted.</para>

          <tip>
            <para>To use the Shell code Generator set the property <emphasis
            role="bold">pegasus.code.generator</emphasis> PMC</para>
          </tip>
        </listitem>
      </orderedlist>

      <figure>
        <title>Final Executable Workflow</title>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentdepth="100%"
                       fileref="./images/refinement-final-executable-wf.png"
                       scalefit="1" width="100%"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center"
                       fileref="./images/refinement-final-executable-wf.png"/>
          </imageobject>
        </mediaobject>
      </figure>
    </section>
  </section>

  <section id="data_staging_configuration">
    <title>Data Staging Configuration</title>

    <para>Pegasus can be broadly setup to run workflows in the following
    configurations</para>

    <itemizedlist>
      <listitem>
        <para><emphasis role="bold">Shared File System</emphasis></para>

        <para>This setup applies to where the head node and the worker nodes
        of a cluster share a filesystem. Compute jobs in the workflow run in a
        directory on the shared filesystem.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">NonShared FileSystem</emphasis></para>

        <para>This setup applies to where the head node and the worker nodes
        of a cluster don't share a filesystem. Compute jobs in the workflow
        run in a local directory on the worker node</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Condor Pool Without a shared
        filesystem</emphasis></para>

        <para>This setup applies to a condor pool where the worker nodes
        making up a condor pool don't share a filesystem. All data IO is
        achieved using Condor File IO. This is a special case of the non
        shared filesystem setup, where instead of using pegasus-transfer to
        transfer input and output data, Condor File IO is used.</para>
      </listitem>
    </itemizedlist>

    <para>For the purposes of data configuration various sites, and
    directories are defined below.</para>

    <orderedlist>
      <listitem>
        <para><emphasis role="bold">Submit Host</emphasis></para>

        <para>The host from where the workflows are submitted . This is where
        Pegasus and Condor DAGMan are installed. This is referred to as the
        <emphasis role="bold">"local"</emphasis> site in the site catalog
        .</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Compute Site</emphasis></para>

        <para>The site where the jobs mentioned in the DAX are executed. There
        needs to be an entry in the Site Catalog for every compute site. The
        compute site is passed to pegasus-plan using <emphasis
        role="bold">--sites</emphasis> option</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Staging Site</emphasis></para>

        <para>A site to which the separate transfer jobs in the executable
        workflow ( jobs with stage_in , stage_out and stage_inter prefixes
        that Pegasus adds using the transfer refiners) stage the input data to
        and the output data from to transfer to the final output site.
        Currently, the staging site is always the compute site where the jobs
        execute.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Output Site</emphasis></para>

        <para>The output site is the final storage site where the users want
        the output data from jobs to go to. The output site is passed to
        pegasus-plan using the <emphasis role="bold">--output</emphasis>
        option. The stageout jobs in the workflow stage the data from the
        staging site to the final storage site.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Input Site</emphasis></para>

        <para>The site where the input data is stored. The locations of the
        input data are catalogued in the Replica Catalog, and the pool
        attribute of the locations gives us the site handle for the input
        site.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Workflow Execution
        Directory</emphasis></para>

        <para>This is the directory created by the create dir jobs in the
        executable workflow on the Staging Site. This is a directory per
        workflow per staging site. Currently, the Staging site is always the
        Compute Site.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">Worker Node Directory</emphasis></para>

        <para>This is the directory created on the worker nodes per job
        usually by the job wrapper that launches the job.</para>
      </listitem>
    </orderedlist>

    <para>You can specifiy the data configuration to use either in</para>

    <orderedlist>
      <listitem>
        <para>properties - Specify the global property <link
        linkend="data_conf_props">pegasus.data.configuration</link> .</para>
      </listitem>

      <listitem>
        <para>site catalog - Starting 4.5.0 release, you can specify pegasus
        profile key named data.configuration and associate that with your
        compute sites in the site catalog.</para>
      </listitem>
    </orderedlist>

    <section>
      <title>Shared File System</title>

      <para>By default Pegasus is setup to run workflows in the shared file
      system setup, where the worker nodes and the head node of a cluster
      share a filesystem.</para>

      <figure>
        <title>Shared File System Setup</title>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentdepth="100%"
                       fileref="images/data-configuration-sharedfs.png"
                       scalefit="1" width="100%"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center"
                       fileref="images/data-configuration-sharedfs.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The data flow is as follows in this case</para>

      <orderedlist>
        <listitem>
          <para>Stagein Job executes ( either on Submit Host or Head Node ) to
          stage in input data from Input Sites ( 1---n) to a workflow specific
          execution directory on the shared filesystem.</para>
        </listitem>

        <listitem>
          <para>Compute Job starts on a worker node in the workflow execution
          directory. Accesses the input data using Posix IO</para>
        </listitem>

        <listitem>
          <para>Compute Job executes on the worker node and writes out output
          data to workflow execution directory using Posix IO</para>
        </listitem>

        <listitem>
          <para>Stageout Job executes ( either on Submit Host or Head Node )
          to stage out output data from the workflow specific execution
          directory to a directory on the final output site.</para>
        </listitem>
      </orderedlist>

      <tip>
        <para>Set <emphasis role="bold">pegasus.data.configuration</emphasis>
        to <emphasis role="bold">sharedfs</emphasis> to run in this
        configuration.</para>
      </tip>
    </section>

    <section id="non_shared_fs">
      <title>Non Shared Filesystem</title>

      <para>In this setup , Pegasus runs workflows on local file-systems of
      worker nodes with the the worker nodes not sharing a filesystem. The
      data transfers happen between the worker node and a staging / data
      coordination site. The staging site server can be a file server on the
      head node of a cluster or can be on a separate machine.</para>

      <para><emphasis role="bold">Setup</emphasis> <itemizedlist>
          <listitem>
            <para>compute and staging site are the different</para>
          </listitem>

          <listitem>
            <para>head node and worker nodes of compute site don't share a
            filesystem</para>
          </listitem>

          <listitem>
            <para>Input Data is staged from remote sites.</para>
          </listitem>

          <listitem>
            <para>Remote Output Site i.e site other than compute site. Can be
            submit host.</para>
          </listitem>
        </itemizedlist></para>

      <figure>
        <title>Non Shared Filesystem Setup</title>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentdepth="100%"
                       fileref="images/data-configuration-nonsharedfs.png"
                       scalefit="1" width="100%"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center"
                       fileref="images/data-configuration-nonsharedfs.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The data flow is as follows in this case</para>

      <orderedlist>
        <listitem>
          <para>Stagein Job executes ( either on Submit Host or on staging
          site ) to stage in input data from Input Sites ( 1---n) to a
          workflow specific execution directory on the staging site.</para>
        </listitem>

        <listitem>
          <para>Compute Job starts on a worker node in a local execution
          directory. Accesses the input data using pegasus transfer to
          transfer the data from the staging site to a local directory on the
          worker node</para>
        </listitem>

        <listitem>
          <para>The compute job executes in the worker node, and executes on
          the worker node.</para>
        </listitem>

        <listitem>
          <para>The compute Job writes out output data to the local directory
          on the worker node using Posix IO</para>
        </listitem>

        <listitem>
          <para>Output Data is pushed out to the staging site from the worker
          node using pegasus-transfer.</para>
        </listitem>

        <listitem>
          <para>Stageout Job executes ( either on Submit Host or staging site
          ) to stage out output data from the workflow specific execution
          directory to a directory on the final output site.</para>
        </listitem>
      </orderedlist>

      <para>In this case, the compute jobs are wrapped as <link
      linkend="pegasuslite">PegasusLite</link> instances.</para>

      <para>This mode is especially useful for running in the cloud
      environments where you don't want to setup a shared filesystem between
      the worker nodes. Running in that mode is explained in detail <link
      linkend="amazon_aws">here.</link></para>

      <tip>
        <para>Set p<emphasis role="bold">egasus.data.configuration</emphasis>
        to <emphasis role="bold">nonsharedfs</emphasis> to run in this
        configuration. The staging site can be specified using the <emphasis
        role="bold">--staging-site</emphasis> option to pegasus-plan.</para>
      </tip>
    </section>

    <section>
      <title>Condor Pool Without a Shared Filesystem</title>

      <para>This setup applies to a condor pool where the worker nodes making
      up a condor pool don't share a filesystem. All data IO is achieved using
      Condor File IO. This is a special case of the non shared filesystem
      setup, where instead of using pegasus-transfer to transfer input and
      output data, Condor File IO is used.</para>

      <para><emphasis role="bold">Setup</emphasis> <itemizedlist>
          <listitem>
            <para>Submit Host and staging site are same</para>
          </listitem>

          <listitem>
            <para>head node and worker nodes of compute site don't share a
            filesystem</para>
          </listitem>

          <listitem>
            <para>Input Data is staged from remote sites.</para>
          </listitem>

          <listitem>
            <para>Remote Output Site i.e site other than compute site. Can be
            submit host.</para>
          </listitem>
        </itemizedlist></para>

      <figure>
        <title>Condor Pool Without a Shared Filesystem</title>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentdepth="100%"
                       fileref="images/data-configuration-condorio.png"
                       scalefit="1" width="100%"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center"
                       fileref="images/data-configuration-condorio.png"/>
          </imageobject>
        </mediaobject>
      </figure>

      <para>The data flow is as follows in this case</para>

      <orderedlist>
        <listitem>
          <para>Stagein Job executes on the submit host to stage in input data
          from Input Sites ( 1---n) to a workflow specific execution directory
          on the submit host</para>
        </listitem>

        <listitem>
          <para>Compute Job starts on a worker node in a local execution
          directory. Before the compute job starts, Condor transfers the input
          data for the job from the workflow execution directory on the submit
          host to the local execution directory on the worker node.</para>
        </listitem>

        <listitem>
          <para>The compute job executes in the worker node, and executes on
          the worker node.</para>
        </listitem>

        <listitem>
          <para>The compute Job writes out output data to the local directory
          on the worker node using Posix IO</para>
        </listitem>

        <listitem>
          <para>When the compute job finishes, Condor transfers the output
          data for the job from the local execution directory on the worker
          node to the workflow execution directory on the submit host.</para>
        </listitem>

        <listitem>
          <para>Stageout Job executes ( either on Submit Host or staging site
          ) to stage out output data from the workflow specific execution
          directory to a directory on the final output site.</para>
        </listitem>
      </orderedlist>

      <para>In this case, the compute jobs are wrapped as <link
      linkend="pegasuslite">PegasusLite</link> instances.</para>

      <para>This mode is especially useful for running in the cloud
      environments where you don't want to setup a shared filesystem between
      the worker nodes. Running in that mode is explained in detail <link
      linkend="amazon_aws">here.</link></para>

      <tip>
        <para>Set p<emphasis role="bold">egasus.data.configuration</emphasis>
        to <emphasis role="bold">condorio</emphasis> to run in this
        configuration. In this mode, the staging site is automatically set to
        site <emphasis role="bold">local</emphasis></para>
      </tip>
    </section>
  </section>

  <section id="pegasuslite">
    <title>PegasusLite</title>

    <para>Starting Pegasus 4.0 , all compute jobs ( single or clustered jobs)
    that are executed in a non shared filesystem setup, are executed using
    lightweight job wrapper called PegasusLite.</para>

    <figure>
      <title>Workflow Running in NonShared Filesystem Setup with PegasusLite
      launching compute jobs</title>

      <mediaobject>
        <imageobject role="html">
          <imagedata align="center" contentdepth="100%"
                     fileref="images/data-configuration-pegasuslite.png"
                     scalefit="1" width="100%"/>
        </imageobject>

        <imageobject role="fo">
          <imagedata align="center" contentdepth="4in"
                     fileref="images/data-configuration-pegasuslite.png"/>
        </imageobject>
      </mediaobject>
    </figure>

    <para>When PegasusLite starts on a remote worker node to run a compute job
    , it performs the following actions:</para>

    <orderedlist>
      <listitem>
        <para>Discovers the best run-time directory based on space
        requirements and create the directory on the local filesystem of the
        worker node to execute the job.</para>
      </listitem>

      <listitem>
        <para>Prepare the node for executing the unit of work. This involves
        discovering whether the pegasus worker tools are already installed on
        the node or need to be brought in.</para>
      </listitem>

      <listitem>
        <para>Use pegasus-transfer to stage in the input data to the runtime
        directory (created in step 1) on the remote worker node.</para>
      </listitem>

      <listitem>
        <para>Launch the compute job.</para>
      </listitem>

      <listitem>
        <para>Use pegasus-transfer to stage out the output data to the data
        coordination site.</para>
      </listitem>

      <listitem>
        <para>Remove the directory created in Step 1.</para>
      </listitem>
    </orderedlist>
  </section>

  <section id="pegasus-plan">
    <title>Pegasus-Plan</title>

    <para>pegasus-plan is the main executable that takes in the abstract
    workflow ( DAX ) and generates an executable workflow ( usually a Condor
    DAG ) by querying various catalogs and performing several refinement
    steps. Before users can run pegasus plan the following needs to be
    done:</para>

    <orderedlist>
      <listitem>
        <para>Populate the various catalogs</para>

        <orderedlist>
          <listitem>
            <para><emphasis role="bold">Replica Catalog</emphasis></para>

            <para>The Replica Catalog needs to be catalogued with the
            locations of the input files required by the workflows. This can
            be done by using pegasus-rc-client (See the Replica section of
            <link linkend="creating_workflows">Creating
            Workflows</link>).</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Transformation
            Catalog</emphasis></para>

            <para>The Transformation Catalog needs to be catalogued with the
            locations of the executables that the workflows will use. This can
            be done by using pegasus-tc-client (See the Transformation section
            of <link linkend="creating_workflows">Creating
            Workflows</link>).</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Site Catalog</emphasis></para>

            <para>The Site Catalog needs to be catalogued with the site layout
            of the various sites that the workflows can execute on. A site
            catalog can be generated for OSG by using the client
            pegasus-sc-client (See the Site section of the <link
            linkend="creating_workflows">Creating Workflows</link>).</para>
          </listitem>
        </orderedlist>
      </listitem>

      <listitem>
        <para>Configure Properties</para>

        <para>After the catalogs have been configured, the user properties
        file need to be updated with the types and locations of the catalogs
        to use. These properties are described in the <emphasis
        role="bold">basic.properties</emphasis> files in the <emphasis
        role="bold">etc</emphasis> sub directory (see the Properties section
        of the<link linkend="configuration"> Configuration</link>
        chapter.</para>

        <para>The basic properties that need to be set usually are listed
        below:</para>

        <table>
          <title>Basic Properties that need to be set</title>

          <tgroup cols="1">
            <tbody>
              <row>
                <entry>pegasus.catalog.replica</entry>
              </row>

              <row>
                <entry>pegasus.catalog.replica.file |
                pegasus.catalog.replica.url</entry>
              </row>

              <row>
                <entry>pegasus.catalog.transformation</entry>
              </row>

              <row>
                <entry>pegasus.catalog.transformation.file</entry>
              </row>

              <row>
                <entry>pegasus.catalog.site.file</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </listitem>
    </orderedlist>

    <para>To execute pegasus-plan user usually requires to specify the
    following options:</para>

    <orderedlist>
      <listitem>
        <para><emphasis role="bold">--dax </emphasis> the path to the DAX file
        that needs to be mapped.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">--dir </emphasis> the base directory where
        the executable workflow is generated</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">--sites </emphasis> comma separated list
        of execution sites.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">--output</emphasis> the output site where
        to transfer the materialized output files.</para>
      </listitem>

      <listitem>
        <para><emphasis role="bold">--submit </emphasis> boolean value whether
        to submit the planned workflow for execution after planning is
        done.</para>
      </listitem>
    </orderedlist>
  </section>

  <xi:include href="basic_properties.xml"
              xmlns:xi="http://www.w3.org/2001/XInclude"/>
</chapter>
