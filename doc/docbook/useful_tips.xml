<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="useful_tips" lang="">
  <title>Useful Tips</title>

  <section id="migrating_from_2x">
    <title>Migrating From Pegasus 3.1 to Pegasus 4.X</title>

    <para>With Pegasus 4.0 effort has been made to move the Pegasus
    installation to be FHS compliant, make workflows run better in Cloud
    environments. This chapter is for existing users of Pegasus who use
    Pegasus 3.1 to run their workflows and walks through the steps to move to
    using Pegasus 4.0</para>

    <section>
      <title>Move to FHS layout [Mats]</title>

      <para>Talk a bit about FHS layout, and include references if any. Also
      talk about pegasus-config and how they can be used in a DAX
      generator.</para>
    </section>

    <section>
      <title>Stampede Schema Upgrade Tool [Gaurang]</title>

      <para>Include a reference to the new stampede schema. Talk about how to
      use the upgrade tool. Where it is. Include sample steps to highlight how
      to use it.</para>
    </section>

    <section>
      <title>Existing users running in a condor pool with a non shared
      filesystem setup</title>

      <para>Existing users that are running workflows in a cloud environment
      with a non shared filesystem setup have to do some trickery in the site
      catalog to include placeholders for local/submit host paths for
      execution sites when using CondorIO. In Pegasus 4.0, this has been
      rectified.</para>

      <para>For example, for a 3.1 user, to run on a local-condor pool without
      a shared filesystem and use Condor file IO for file transfers, the site
      entry looks something like this</para>

      <programlisting> &lt;site  handle="local-condor" arch="x86" os="LINUX"&gt;
        &lt;grid  type="gt2" contact="localhost/jobmanager-fork" scheduler="Fork" jobtype="auxillary"/&gt;
        &lt;grid  type="gt2" contact="localhost/jobmanager-condor" scheduler="unknown" jobtype="compute"/&gt;
        &lt;head-fs&gt;

          <emphasis role="bold">&lt;!-- the paths for scratch filesystem are the paths on local site as we execute create dir job
               on local site. Improvements planned for 4.0 release.--&gt;</emphasis>
            &lt;scratch&gt;
                &lt;shared&gt;
                    &lt;file-server protocol="file" url="file:///" mount-point="/submit-host/scratch"/&gt;
                    &lt;internal-mount-point mount-point="/submit-host/scratch"/&gt;
                &lt;/shared&gt;
            &lt;/scratch&gt;
            &lt;storage&gt;
                &lt;shared&gt;
                    &lt;file-server protocol="file" url="file:///" mount-point="/glusterfs/scratch"/&gt;
                    &lt;internal-mount-point mount-point="/glusterfs/scratch"/&gt;
                &lt;/shared&gt;
            &lt;/storage&gt;
        &lt;/head-fs&gt;
        &lt;replica-catalog  type="LRC" url="rlsn://dummyValue.url.edu" /&gt;
        &lt;profile namespace="env" key="PEGASUS_HOME" &gt;/cluster-software/pegasus/2.4.1&lt;/profile&gt;
        &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/cluster-software/globus/5.0.1&lt;/profile&gt;

        <emphasis role="bold">&lt;!-- profies for site to be treated as condor pool --&gt;</emphasis>
        &lt;profile namespace="pegasus" key="style" &gt;condor&lt;/profile&gt;
        &lt;profile namespace="condor" key="universe" &gt;vanilla&lt;/profile&gt;

        
        <emphasis role="bold">&lt;!-- to enable kickstart staging from local site--&gt;</emphasis>
        &lt;profile namespace="condor" key="transfer_executable"&gt;true&lt;/profile&gt;


    &lt;/site&gt;
</programlisting>

      <para>With Pegasus 4.0 the site entry for a local-condor pool can be as
      concise as the following</para>

      <programlisting> &lt;site  handle="condorpool" arch="x86" os="LINUX"&gt;
        &lt;head-fs&gt;
            &lt;scratch /&gt;
            &lt;storage /&gt;
        &lt;/head-fs&gt;
        &lt;profile namespace="pegasus" key="style" &gt;condor&lt;/profile&gt;
        &lt;profile namespace="condor" key="universe" &gt;vanilla&lt;/profile&gt;
    &lt;/site&gt;
</programlisting>

      <para>The planner in 4.0 correctly picks up the paths from the local
      site entry to determine the staging location for the condor io on the
      submit host.</para>

      <para>Users should read pegasus data staging configuration <link
      linkend="data_staging_configuration">chapter</link> and also look in the
      examples directory ( share/pegasus/examples).</para>
    </section>

    <section>
      <title>New Clients for directory creation and file cleanup</title>

      <para>Pegasus 4.0 has new clients for directory creation and cleanup.
      </para>

      <itemizedlist>
        <listitem>
          <para>pegasus-create-dir</para>
        </listitem>

        <listitem>
          <para>pegasus-cleanup</para>
        </listitem>
      </itemizedlist>

      <para>Both these clients are python based wrapper scripts around various
      protocol specific clients that are used to determine what client to pick
      up. </para>

      <table>
        <title> Clients interfaced to by pegasus-create-dir</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry> Client</entry>

              <entry>Used For</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>globus-url-copy</entry>

              <entry>to create directories against a gridftp/ftp
              server</entry>
            </row>

            <row>
              <entry>srm-mkdir</entry>

              <entry>to create directories against a SRM server.</entry>
            </row>

            <row>
              <entry>mkdir</entry>

              <entry>to create a directory on the local filesystem</entry>
            </row>

            <row>
              <entry>pegasus-s3</entry>

              <entry>to create a s3 bucket in the amazon cloud</entry>
            </row>

            <row>
              <entry>scp</entry>

              <entry>staging files using scp</entry>
            </row>

            <row>
              <entry>imkdir</entry>

              <entry>to create a directory against an IRODS server</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <table>
        <title>Clients interfaced to by pegasus-cleanup</title>

        <tgroup cols="2">
          <thead>
            <row>
              <entry> Client</entry>

              <entry>Used For</entry>
            </row>
          </thead>

          <tbody>
            <row>
              <entry>globus-url-copy</entry>

              <entry>to remove a file against a gridftp/ftp server. In this
              case a zero byte file is created</entry>
            </row>

            <row>
              <entry>srm-rm</entry>

              <entry>to remove files against a SRM server.</entry>
            </row>

            <row>
              <entry>rm</entry>

              <entry>to remove a file on the local filesystem</entry>
            </row>

            <row>
              <entry>pegasus-s3</entry>

              <entry>to remove a file from the s3 bucket.</entry>
            </row>

            <row>
              <entry>scp</entry>

              <entry>to remove a file against a scp server. In this case a
              zero byte file is created.</entry>
            </row>

            <row>
              <entry>irm</entry>

              <entry>to remove a file against an IRODS server</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <para>With Pegasus 4.0, the planner will prefer to run the create dir
      and cleanup jobs locally on the submit host. The only case, where these
      jobs are scheduled to run remotely is when for the staging site, a file
      server is specified.</para>
    </section>
  </section>

  <section id="migrating_from_2x">
    <title>Migrating From Pegasus 2.X to Pegasus 3.X</title>

    <para>With Pegasus 3.0 effort has been made to simplify configuration.
    This chapter is for existing users of Pegasus who use Pegasus 2.x to run
    their workflows and walks through the steps to move to using Pegasus
    3.0</para>

    <section>
      <title>PEGASUS_HOME and Setup Scripts</title>

      <para>Earlier versions of Pegasus required users to have the environment
      variable PEGASUS_HOME set and to source a setup file
      $PEGASUS_HOME/setup.sh | $PEGASUS_HOME/setup.csh before running Pegasus
      to setup CLASSPATH and other variables.</para>

      <para>Starting with Pegasus 3.0 this is no longer required. The above
      paths are automaticallly determined by the Pegasus tools when they are
      invoked.</para>

      <para>All the users need to do is to set the PATH variable to pick up
      the pegasus executables from the bin directory.</para>

      <programlisting>$ <emphasis role="bold">export PATH=/some/install/pegasus-3.0.0/bin:$PATH</emphasis></programlisting>
    </section>

    <section>
      <title>Changes to Schemas and Catalog Formats</title>

      <section>
        <title>DAX Schema</title>

        <para>Pegasus 3.0 by default now parses DAX documents conforming to
        the DAX Schema 3.2 available <ulink role=""
        url="http://pegasus.isi.edu/wms/docs/schemas/dax-3.2/dax-3.2.xsd"
        userlevel="">here</ulink> and is explained in detail in the chapter on
        API references.</para>

        <para>Starting Pegasus 3.0 , DAX generation API's are provided in
        Java/Python and Perl for users to use in their DAX Generators. The use
        of API's is highly encouraged. Support for the old DAX schema's has
        been deprecated and will be removed in a future version.</para>

        <para>For users, who still want to run using the old DAX formats i.e
        3.0 or earlier, can for the time being set the following property in
        the properties and point it to dax-3.0 xsd of the installation.</para>

        <programlisting><emphasis role="bold">pegasus.schema.dax  /some/install/pegasus-3.0/etc/dax-3.0.xsd</emphasis></programlisting>
      </section>

      <section>
        <title>Site Catalog Format</title>

        <para>Pegasus 3.0 by default now parses Site Catalog format conforming
        to the SC schema 3.0 ( XML3 ) available <ulink role=""
        url="http://pegasus.isi.edu/wms/docs/schemas/dax-3.2/dax-3.2.xsd"
        userlevel="">here</ulink> and is explained in detail in the chapter on
        Catalogs.</para>

        <para>Pegasus 3.0 comes with a pegasus-sc-converter that will convert
        users old site catalog ( XML ) to the XML3 format. Sample usage is
        given below.</para>

        <programlisting><emphasis role="bold">$ pegasus-sc-converter -i sample.sites.xml -I XML -o sample.sites.xml3 -O XML3
</emphasis>
2010.11.22 12:55:14.169 PST:   Written out the converted file to sample.sites.xml3 
</programlisting>

        <para>To use the converted site catalog, in the properties do the
        following</para>

        <orderedlist>
          <listitem>
            <para>unset pegasus.catalog.site or set pegasus.catalog.site to
            XML3</para>
          </listitem>

          <listitem>
            <para>point pegasus.catalog.site.file to the converted site
            catalog</para>
          </listitem>
        </orderedlist>
      </section>

      <section>
        <title>Transformation Catalog Format</title>

        <para>Pegasus 3.0 by default now parses a file based multiline textual
        format of a Transformation Catalog. The new Text format is explained
        in detail in the chapter on Catalogs.</para>

        <para>Pegasus 3.0 comes with a pegasus-tc-converter that will convert
        users old transformation catalog ( File ) to the Text format. Sample
        usage is given below.</para>

        <programlisting><emphasis role="bold">$ pegasus-tc-converter -i sample.tc.data -I File -o sample.tc.text -O Text
</emphasis>
2010.11.22 12:53:16.661 PST:   Successfully converted Transformation Catalog from File to Text 
2010.11.22 12:53:16.666 PST:   The output transfomation catalog is in file  /lfs1/software/install/pegasus/pegasus-3.0.0cvs/etc/sample.tc.text 
</programlisting>

        <para>To use the converted transformation catalog, in the properties
        do the following</para>

        <orderedlist>
          <listitem>
            <para>unset pegasus.catalog.transformation or set
            pegasus.catalog.transformation to Text</para>
          </listitem>

          <listitem>
            <para>point pegasus.catalog.transformation.file to the converted
            transformation catalog</para>
          </listitem>
        </orderedlist>
      </section>
    </section>

    <section>
      <title>Properties and Profiles Simplification</title>

      <para>Starting with Pegasus 3.0 all profiles can be specified in the
      properties file. Profiles specified in the properties file have the
      lowest priority. Profiles are explained in the detail in the<link
      linkend="profiles"> Profiles</link>chapter. As a result of this a lot of
      existing Pegasus Properties were replaced by profiles. The table below
      lists the properties removed and the new profile based names.</para>

      <table>
        <title>Table 1: Property Keys removed and their Profile based
        replacement</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Old Property Key</emphasis></entry>

              <entry><emphasis role="bold">New Property Key</emphasis></entry>
            </row>

            <row>
              <entry>pegasus.local.env</entry>

              <entry>no replacement. Specify env profiles for local site in
              the site catalog</entry>
            </row>

            <row>
              <entry>pegasus.condor.release</entry>

              <entry>condor.periodic_release</entry>
            </row>

            <row>
              <entry>pegasus.condor.remove</entry>

              <entry>condor.periodic_remove</entry>
            </row>

            <row>
              <entry>pegasus.job.priority</entry>

              <entry>condor.priority</entry>
            </row>

            <row>
              <entry>pegasus.condor.output.stream</entry>

              <entry>pegasus.condor.output.stream</entry>
            </row>

            <row>
              <entry>pegasus.condor.error.stream</entry>

              <entry>condor.stream_error</entry>
            </row>

            <row>
              <entry>pegasus.dagman.retry</entry>

              <entry>dagman.retry</entry>
            </row>

            <row>
              <entry>pegasus.exitcode.impl</entry>

              <entry>dagman.post</entry>
            </row>

            <row>
              <entry>pegasus.exitcode.scope</entry>

              <entry>dagman.post.scope</entry>
            </row>

            <row>
              <entry>pegasus.exitcode.arguments</entry>

              <entry>dagman.post.arguments</entry>
            </row>

            <row>
              <entry>pegasus.exitcode.path.*</entry>

              <entry>dagman.post.path.*</entry>
            </row>

            <row>
              <entry>pegasus.dagman.maxpre</entry>

              <entry>dagman.maxpre</entry>
            </row>

            <row>
              <entry>pegasus.dagman.maxpost</entry>

              <entry>dagman.maxpost</entry>
            </row>

            <row>
              <entry>pegasus.dagman.maxidle</entry>

              <entry>dagman.maxidle</entry>
            </row>

            <row>
              <entry>pegasus.dagman.maxjobs</entry>

              <entry>dagman.maxjobs</entry>
            </row>

            <row>
              <entry>pegasus.remote.scheduler.min.maxwalltime</entry>

              <entry>globus.maxwalltime</entry>
            </row>

            <row>
              <entry>pegasus.remote.scheduler.min.maxtime</entry>

              <entry>globus.maxtime</entry>
            </row>

            <row>
              <entry>pegasus.remote.scheduler.min.maxcputime</entry>

              <entry>globus.maxcputime</entry>
            </row>

            <row>
              <entry>pegasus.remote.scheduler.queues</entry>

              <entry>globus.queue</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <section>
        <title>Profile Keys for Clustering</title>

        <para>The pegasus profile keys for job clustering were <emphasis
        role="bold">renamed</emphasis>. The following table lists the old and
        the new names for the profile keys.</para>

        <table>
          <title>Table 2: Old and New Names For Job Clustering Profile
          Keys</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry><emphasis role="bold">Old Pegasus Profile
                Key</emphasis></entry>

                <entry><emphasis role="bold">New Pegasus Profile
                Key</emphasis></entry>
              </row>

              <row>
                <entry>collapse</entry>

                <entry>clusters.size</entry>
              </row>

              <row>
                <entry>bundle</entry>

                <entry>clusters.num</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>
    </section>

    <section>
      <title>Transfers Simplification</title>

      <para>Pegasus 3.0 has a new default transfer client pegasus-transfer
      that is invoked by default for first level and second level staging. The
      pegasus-transfer client is a python based wrapper around various
      transfer clients like globus-url-copy, lcg-copy, wget, cp, ln .
      pegasus-transfer looks at source and destination url and figures out
      automatically which underlying client to use. pegasus-transfer is
      distributed with the PEGASUS and can be found in the bin subdirectory
      .</para>

      <para>Also, the Bundle Transfer refiner has been made the default for
      pegasus 3.0. Most of the users no longer need to set any transfer
      related properties. The names of the profiles keys that control the
      Bundle Transfers have been changed . The following table lists the old
      and the new names for the Pegasus Profile Keys and are explained in
      details in the Profiles Chapter.</para>

      <table>
        <title>Table 3: Old and New Names For Transfer Bundling Profile
        Keys</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Old Pegasus Profile
              Key</emphasis></entry>

              <entry><emphasis role="bold">New Pegasus Profile
              Keys</emphasis></entry>
            </row>

            <row>
              <entry>bundle.stagein</entry>

              <entry>stagein.clusters | stagein.local.clusters |
              stagein.remote.clusters</entry>
            </row>

            <row>
              <entry>bundle.stageout</entry>

              <entry>stageout.clusters | stageout.local.clusters |
              stageout.remote.clusters</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <section>
        <title>Worker Package Staging</title>

        <para>Starting Pegasus 3.0 there is a separate boolean property
        <emphasis role="bold">pegasus.transfer.worker.package</emphasis> to
        enable worker package staging to the remote compute sites. Earlier it
        was bundled with user executables staging i.e if <emphasis
        role="bold">pegasus.catalog.transformation.mapper</emphasis> property
        was set to Staged .</para>
      </section>
    </section>

    <section>
      <title>Clients in bin directory</title>

      <para>Starting with Pegasus 3.0 the pegasus clients in the bin directory
      have a pegasus prefix. The table below lists the old client names and
      new names for the clients that replaced them</para>

      <table>
        <title>Table 1: Old Client Names and their New Names</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Old Client</emphasis></entry>

              <entry><emphasis role="bold">New Client</emphasis></entry>
            </row>

            <row>
              <entry>rc-client</entry>

              <entry>pegasus-rc-client</entry>
            </row>

            <row>
              <entry>tc-client</entry>

              <entry>pegasus-tc-client</entry>
            </row>

            <row>
              <entry>pegasus-get-sites</entry>

              <entry>pegasus-sc-client</entry>
            </row>

            <row>
              <entry>sc-client</entry>

              <entry>pegasus-sc-converter</entry>
            </row>

            <row>
              <entry>tailstatd</entry>

              <entry>pegasus-monitord</entry>
            </row>

            <row>
              <entry>genstats and genstats-breakdown</entry>

              <entry>pegasus-statistics</entry>
            </row>

            <row>
              <entry>show-job</entry>

              <entry>pegasus-plots</entry>
            </row>

            <row>
              <entry>cleanup</entry>

              <entry>pegasus-cleanup</entry>
            </row>

            <row>
              <entry>dirmanager</entry>

              <entry>pegasus-dirmanager</entry>
            </row>

            <row>
              <entry>exitcode</entry>

              <entry>pegasus-exitcode</entry>
            </row>

            <row>
              <entry>rank-dax</entry>

              <entry>pegasus-rank-dax</entry>
            </row>

            <row>
              <entry>transfer</entry>

              <entry>pegasus-transfer</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
  </section>

  <section id="portable_code">
    <title>Best Practices For Developing Portable Code</title>

    <para>This document lists out issues for the algorithm developers to keep
    in mind while developing the respective codes. Keeping these in mind will
    alleviate a lot of problems while trying to run the codes on the Grid
    through workflows.</para>

    <section>
      <title>Supported Platforms</title>

      <para>Most of the hosts making a Grid run variants of Linux or in some
      case Solaris. The Grid middleware mostly supports UNIX and it's
      variants.</para>

      <section>
        <title>Running on Windows</title>

        <para>The majority of the machines making up the various Grid sites
        run Linux. In fact, there is no widespread deployment of a
        Windows-based Grid. Currently, the server side software of Globus does
        not run on Windows. Only the client tools can run on Windows. The
        algorithm developers should not code exclusively for the Windows
        platforms. They must make sure that their codes run on Linux or
        Solaris platforms. If the code is written in a portable language like
        Java, then porting should not be an issue.</para>

        <para>If for some reason the code can only be executed on windows
        platform, please contact the pegasus team at pegasus aT isi dot edu .
        In certain cases it is possible to stand up a linux headnode in front
        of a windows cluster running Condor as it's scheduler.</para>
      </section>
    </section>

    <section>
      <title>Packaging of Software</title>

      <para>As far as possible, binary packages (preferably statically linked)
      of the codes should be provided. If for some reason the codes, need to
      be built from the source then they should have an associated makefile (
      for C/C++ based tools) or an ant file ( for Java tools). The building
      process should refer to the standard libraries that are part of a normal
      Linux installation. If the codes require non-standard libraries, clear
      documentation needs to be provided, as to how to install those
      libraries, and make the build process refer to those libraries.</para>

      <para>Further, installing software as root is not a possibility. Hence,
      all the external libraries that need to be installed can only be
      installed as non-root in non-standard locations.</para>
    </section>

    <section>
      <title>MPI Codes</title>

      <para>If any of the algorithm codes are MPI based, they should contact
      the Grid group. MPI can be run on the Grid but the codes need to be
      compiled against the installed MPI libraries on the various Grid sites.
      The pegasus group has some experience running MPI code through
      PBS.</para>
    </section>

    <section>
      <title>Maximum Running Time of Codes</title>

      <para>Each of the Grid sites has a policy on the maximum time for which
      they will allow a job to run. The algorithms catalog should have the
      maximum time (in minutes) that the job can run for. This information is
      passed to the Grid sites while submitting a job, so that Grid site does
      not kill a job before that published time expires. It is OK, if the job
      runs only a fraction of the max time.</para>
    </section>

    <section>
      <title>Codes cannot specify the directory in which they should be
      run</title>

      <para>Codes are installed in some standard location on the Grid Sites or
      staged on demand. However, they are not invoked from directories where
      they are installed. The codes should be able to be invoked from any
      directory, as long as one can access the directory where the codes are
      installed.</para>

      <para>This is especially relevant, while writing scripts around the
      algorithm codes. At that point specifying the relative paths do not
      work. This is because the relative path is constructed from the
      directory where the script is being invoked. A suggested workaround is
      to pick up the base directory where the software is installed from the
      environment or by using the <command>dirname</command> cmd or api. The
      workflow system can set appropriate environment variables while
      launching jobs on the Grid.</para>
    </section>

    <section>
      <title>No hard-coded paths</title>

      <para>The algorithms should not hard-code any directory paths in the
      code. All directories paths should be picked up explicitly either from
      the environment (specifying environment variables) or from command line
      options passed to the algorithm code.</para>
    </section>

    <section>
      <title>Wrapping legacy codes with a shell wrapper</title>

      <para>When wrapping a legacy code in a script (or another program), it
      is necessary that the wrapper knows where the executable lives. This is
      accomplished using an environmental variable. Be sure to include this
      detail in the component description when submitting a component for use
      on the Grid -- include a brief descriptive name like GDA_BIN.</para>
    </section>

    <section>
      <title>Propogating back the right exitcode</title>

      <para>A job in the workflow is only released for execution if its
      parents have executed successfully. Hence, it is very important that the
      algorithm codes exit with the correct error code in case of success and
      failure. The algorithms should exit with a status of 0 in case of
      success, and a non zero status in case of error. Failure to do so will
      result in erroneous workflow execution where jobs might be released for
      execution even though their parents had exited with an error.</para>

      <para>The algorithm codes should catch all errors and exit with a non
      zero exitcode. The successful execution of the algorithm code can only
      be determined by an exitcode of 0. The algorithm code should not rely
      upon something being written to the stdout to designate success for e.g.
      if the algorithm code writes out to the stdout SUCCESS and exits with a
      non zero status the job would be marked as failed.</para>

      <para>In *nix, a quick way to see if a code is exiting with the correct
      code is to execute the code and then execute echo $?.</para>

      <programlisting>$ component-x input-file.lisp
... some output ...
$ echo $?
0</programlisting>

      <para>If the code is not exiting correctly, it is necessary to wrap the
      code in a script that tests some final condition (such as the presence
      or format of a result file) and uses exit to return correctly.</para>
    </section>

    <section>
      <title>Static vs. Dynamically Linked Libraries</title>

      <para>Since there is no way to know the profile of the machine that will
      be executing the code, it is important that dynamically linked libraries
      are avoided or that reliance on them is kept to a minimum. For example,
      a component that requires libc 2.5 may or may not run on a machine that
      uses libc 2.3. On *nix, you can use the <command>ldd</command> command
      to see what libraries a binary depends on.</para>

      <para>If for some reason you install an algorithm specific library in a
      non standard location make sure to set the
      <envar>LD_LIBRARY_PATH</envar> for the algorithm in the transformation
      catalog for each site.</para>
    </section>

    <section>
      <title>Temporary Files</title>

      <para>If the algorithm codes create temporary files during execution,
      they should be cleared by the codes in case of errors and success
      terminations. The algorithm codes will run on scratch file systems that
      will also be used by others. The scratch directories get filled up very
      easily, and jobs will fail in case of directories running out of free
      space. The temporary files are the files that are not being tracked
      explicitly through the workflow generation process.</para>
    </section>

    <section>
      <title>Handling of stdio</title>

      <para>When writing a new application, it often appears feasible to use
      <emphasis>stdin</emphasis> for a single file data, and
      <emphasis>stdout</emphasis> for a single file output data. The
      <emphasis>stderr</emphasis> descriptor should be used for logging and
      debugging purposes only, never to put data on it. In the *nix world,
      this will work well, but may hiccup in the Windows world.</para>

      <para>We are suggesting that you avoid using stdio for data files,
      because there is the implied expectation that stdio data gets magically
      handled. There is no magic! If you produce data on
      <emphasis>stdout</emphasis>, you need to declare to Pegasus that your
      <emphasis>stdout</emphasis> has your data, and what LFN Pegasus can
      track it by. After the application is done, the data product will be a
      remote file just like all other data products. If you have an input file
      on <emphasis>stdin</emphasis>, you must track it in a similar manner. If
      you produce logs on <emphasis>stderr</emphasis> that you care about, you
      must track it in a similar manner. Think about it this way: Whenever you
      are redirecting stdio in a *nix shell, you will also have to specify a
      file name.</para>

      <para>Most execution environments permit to connect
      <emphasis>stdin</emphasis>, <emphasis>stdout</emphasis> or
      <emphasis>stderr</emphasis> to any file, and Pegasus supports this case.
      However, there are certain very specific corner cases where this is not
      possible. For this reason, we recommend that in new code, you avoid
      using stdio for data, and provide alternative means on the commandline,
      i.e. via <command>--input <replaceable>fn</replaceable></command> and
      <command>--output <replaceable>fn</replaceable></command> commandline
      arguments instead relying on <emphasis>stdin</emphasis> and
      <emphasis>stdout</emphasis>.</para>
    </section>

    <section>
      <title>Configuration Files</title>

      <para>If your code requires a configuration file to run and the
      configuration changes from one run to another, then this file needs to
      be tracked explicitly via the Pegasus WMS. The configuration file should
      not contain any absolute paths to any data or libraries used by the
      code. If any libraries, scripts etc need to be referenced they should
      refer to relative paths starting with a <filename>./xyz</filename> where
      <filename>xyz</filename> is a tracked file (defined in the workflow) or
      as $ENV-VAR/xyz where <envar>$ENV-VAR</envar> is set during execution
      time and evaluated by your application code internally.</para>
    </section>

    <section>
      <title>Code Invocation and input data staging by Pegasus</title>

      <para>Pegasus will create one temporary directory per workflow on each
      site where the workflow is planned. Pegasus will stage all the files
      required for the execution of the workflow in these temporary
      directories. This directory is shared by all the workflow components
      that executed on the site. You will have no control over where this
      directory is placed and as such you should have no expectations about
      where the code will be run. The directories are created per workflow and
      not per job/alogrithm/task. Suppose there is a component component-x
      that takes one argument: input-file.lisp (a file containing the data to
      be operated on). The staging step will bring input-file.lisp to the
      temporary directory. In *nix the call would look like this:</para>

      <programlisting>$ /nfs/software/component-x input-file.lisp</programlisting>

      <para>Note that Pegasus will call the component using the full path to
      the component. If inside your code/script you invoke some other code you
      cannot assume a path for this code to be relative or absolute. You have
      to resovle it either using a dirname $0 trick in shell assuming the
      child code is in the same directory as the parent or construct the path
      by expecting an enviornment variable to be set by the workflow system.
      These env variables need to be explicitly published so that they can be
      stored in the transformation catalog.</para>

      <para>Now suppose that internally, component-x writes its results to
      /tmp/component-x-results.lisp. This is not good. Components should not
      expect that a /tmp directory exists or that it will have permission to
      write there. Instead, component-x should do one of two things: 1. write
      component-x-results.lisp to the directory where it is run from or 2.
      component-x should take a second argument output-file.lisp that
      specifies the name and path of where the results should be
      written.</para>
    </section>

    <section>
      <title>Logical File naming in DAX</title>

      <para>The logical file names used by your code can be of two
      types.</para>

      <itemizedlist>
        <listitem>
          <para>Without a directory path e.g. <filename>f.a</filename>,
          <filename>f.b</filename> etc</para>
        </listitem>

        <listitem>
          <para>With a directory path e.g. <filename>a/1/f.a</filename>,
          <filename>b/2/f.b</filename></para>
        </listitem>
      </itemizedlist>

      <para>Both types of files are supported. We will create any directory
      structure mentioned in your logical files on the remote execution site
      when we stage in data as well as when we store the output data to a
      permanent location. An example invocation of a code that consumes and
      produces files will be</para>

      <programlisting>$/bin/test --input f.a --output f.b</programlisting>

      <para>OR</para>

      <programlisting>$/bin/test --input a/1/f.a --output b/1/f.b</programlisting>

      <note>
        <para>A logical file name should never be an absolute file path, e.g.
        /a/1/f.a In other words, there should not be a starting slash (/) in a
        logical filename.</para>
      </note>
    </section>
  </section>
</chapter>