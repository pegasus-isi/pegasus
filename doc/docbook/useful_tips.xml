<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="useful_tips" lang="">
  <title>Useful Tips</title>

    <section id="migrating_from_lt47">
      <title>Migrating From Pegasus 4.5.X to Pegasus current version</title>

      <para>Since Pegasus 4.5 all databases are managed by a single tool: <emphasis
      role="bold">pegasus-db-admin</emphasis>. Databases will be automatically
      updated when <emphasis role="bold">pegasus-plan</emphasis> is invoked, but
      WORKFLOW databases from past runs may not be updated accordingly. Since Pegasus
      4.6.0, the <emphasis role="bold">pegasus-db-admin</emphasis> tool provides an
      option to automatically update all databases from completed workflows in the
      MASTER database. To enable this option, run the following command:</para>

      <programlisting>
$ pegasus-db-admin update -a
Your database has been updated.
Your database is compatible with Pegasus version: 4.7.0

Verifying and updating workflow databases:
21/21

Summary:
Verified/Updated: 21/21
Failed: 0/21
Unable to connect: 0/21
Unable to update (active workflows): 0/21

Log files:
20161006T134415-dbadmin.out (Succeeded operations)
20161006T134415-dbadmin.err (Failed operations)
      </programlisting>

      <para>This option generates a log file for succeeded operations, and a log
      file for failed operations. Each file contains the list of URLs of the
      succeeded/failed databases.</para>

      <para>Note that, if no URL is provided, the tool will create/use a SQLite
      database in the user's home directory:
      <emphasis>${HOME}/.pegasus/workflow.db</emphasis>.</para>

      <para>For complete description of pegasus-db-admin, see the <link
      linkend="cli-pegasus-db-admin">man page</link>.</para>

    </section>

    <section id="migrating_from_leq44">
    <title>Migrating From Pegasus &lt;4.5 to Pegasus 4.5.X</title>

    <para>Since Pegasus 4.5 all databases are managed by a single tool: <emphasis
    role="bold">pegasus-db-admin</emphasis>. Databases will be automatically
    updated when <emphasis role="bold">pegasus-plan</emphasis> is invoked, but
    it may require manually invocation of the <emphasis role="bold">
    pegasus-db-admin</emphasis> for other Pegasus tools.</para>

    <para>The <emphasis role="bold">check</emphasis> command verifies if the
    database is compatible with the Pegasus' latest version. If the database is
    not compatible, it will print the following message:</para>

    <programlisting>
$ pegasus-db-admin check
Your database is NOT compatible with version 4.5.0
    </programlisting>

    <para>If you are running the <emphasis role="bold">check</emphasis> command
    for the first time, the tool will prompt the following message:</para>

    <programlisting>
Missing database tables or tables are not updated:
    dbversion
Run 'pegasus-db-admin update &lt;path_to_database&gt;' to create/update your database.
    </programlisting>

    <para>To update the database, run the following command:</para>

    <programlisting>
$ pegasus-db-admin update
Your database has been updated.
Your database is compatible with Pegasus version: 4.5.0
    </programlisting>

    <para>The <emphasis role="bold">pegasus-db-admin</emphasis> tool can operate
    directly over a database URL, or can read configuration parameters from the
    properties file or a submit directory. In the later case, a database type
    should be provided to indicate which properties should be used to connect to
    the database. For example, the tool will seek for <emphasis>pegasus.catalog.replica.db.*</emphasis>
    properties to connect to the JDBCRC database; or seek for <emphasis>pegasus.catalog.master.url</emphasis>
    (or <emphasis>pegasus.dashboard.output</emphasis>, which is deprecated) property
    to connect to the MASTER database; or seek for the <emphasis>pegasus.catalog.workflow.url</emphasis>
    (or <emphasis>pegasus.monitord.output</emphasis>, which is deprecated) property
    to connect to the WORKFLOW database. If none of these properties are found, the
    tool will connect to the default database in the user's home directory
    (sqlite:///${HOME}/.pegasus/workflow.db).</para>

    <para>Example: connection by providing the URL to the database:</para>

    <programlisting>
$ pegasus-db-admin create sqlite:///${HOME}/.pegasus/workflow.db
$ pegasus-db-admin update sqlite:///${HOME}/.pegasus/workflow.db
    </programlisting>

    <para>Example: connection by providing a properties file that contains the
    information to connect to the database. Note that a database type (MASTER,
    WORKFLOW, or JDBCRC) should be provided:</para>

    <programlisting>
$ pegasus-db-admin update -c pegasus.properties -t MASTER
$ pegasus-db-admin update -c pegasus.properties -t JDBCRC
$ pegasus-db-admin update -c pegasus.properties -t WORKFLOW
    </programlisting>

    <para>Example: connection by providing the path to the submit directory
    containning the <emphasis>braindump.txt</emphasis> file, where information
    to connect to the database can be extracted. Note that a database type
    (MASTER, WORKFLOW, or JDBCRC) should also be provided:</para>

    <programlisting>
$ pegasus-db-admin update -s /path/to/submitdir -t WORKFLOW
$ pegasus-db-admin update -s /path/to/submitdir -t MASTER
$ pegasus-db-admin update -s /path/to/submitdir -t JDBCRC
    </programlisting>

    <para>Note that, if no URL is provided, the tool will create/use a SQLite
    database in the user's home directory:
    <emphasis>${HOME}/.pegasus/workflow.db</emphasis>.</para>

    <para>For complete description of pegasus-db-admin, see the <link
    linkend="cli-pegasus-db-admin">man page</link>.</para>

  </section>

  <section id="migrating_from_3x">
    <title>Migrating From Pegasus 3.1 to Pegasus 4.X</title>

    <para>With Pegasus 4.0 effort has been made to move the Pegasus
    installation to be FHS compliant, and to make workflows run better in
    Cloud environments and distributed grid environments. This chapter is for
    existing users of Pegasus who use Pegasus 3.1 to run their workflows and
    walks through the steps to move to using Pegasus 4.0</para>

    <section>
      <title>Move to FHS layout</title>

      <para>Pegasus 4.0 is the first release of Pegasus which is <ulink
      url="http://www.pathname.com/fhs/">Filesystem Hierarchy Standard
      (FHS)</ulink> compliant. The native packages no longer installs under
      /opt. Instead, pegasus-* binaries are in /usr/bin/ and example workflows
      can be found under /usr/share/pegasus/examples/.</para>

      <para>To find Pegasus system components, a pegasus-config tool is
      provided. pegasus-config supports setting up the environment for</para>

      <itemizedlist>
        <listitem>
          <para>Python</para>
        </listitem>

        <listitem>
          <para>Perl</para>
        </listitem>

        <listitem>
          <para>Java</para>
        </listitem>

        <listitem>
          <para>Shell</para>
        </listitem>
      </itemizedlist>

      <para>For example, to find the PYTHONPATH for the DAX API, run:</para>

      <programlisting>export PYTHONPATH=`pegasus-config --python`</programlisting>

      <para>For complete description of pegasus-config, see the <link
      linkend="cli-pegasus-config">man page</link>.</para>
    </section>

    <section>
      <title>Stampede Schema Upgrade Tool</title>

      <para>Starting Pegasus 4.x the monitoring and statistics database schema
      has changed. If you want to use the pegasus-statistics, pegasus-analyzer
      and pegasus-plots against a 3.x database you will need to upgrade the
      schema first using the schema upgrade tool
      /usr/share/pegasus/sql/schema_tool.py or
      /path/to/pegasus-4.x/share/pegasus/sql/schema_tool.py</para>

      <para>Upgrading the schema is required for people using the MySQL
      database for storing their monitoring information if it was setup with
      3.x monitoring tools.</para>

      <para>If your setup uses the default SQLite database then the new
      databases run with Pegasus 4.x are automatically created with the
      correct schema. In this case you only need to upgrade the SQLite
      database from older runs if you wish to query them with the newer
      clients.</para>

      <para>To upgrade the database</para>

      <programlisting>For SQLite Database

<emphasis role="bold">cd /to/the/workflow/directory/with/3.x.monitord.db</emphasis>

Check the db version<emphasis role="bold">

/usr/share/pegasus/sql/schema_tool.py -c connString=sqlite:////to/the/workflow/directory/with/workflow.stampede.db</emphasis>
2012-02-29T01:29:43.330476Z INFO   netlogger.analysis.schema.schema_check.SchemaCheck.init |
2012-02-29T01:29:43.330708Z INFO   netlogger.analysis.schema.schema_check.SchemaCheck.check_schema.start |
2012-02-29T01:29:43.348995Z INFO   netlogger.analysis.schema.schema_check.SchemaCheck.check_schema
                                   | Current version set to: 3.1.
2012-02-29T01:29:43.349133Z ERROR  netlogger.analysis.schema.schema_check.SchemaCheck.check_schema
                                   | Schema version 3.1 found - expecting 4.0 - database admin will need to run upgrade tool.


Convert the Database to be version 4.x compliant<emphasis role="bold">

/usr/share/pegasus/sql/schema_tool.py -u connString=sqlite:////to/the/workflow/directory/with/workflow.stampede.db
</emphasis>2012-02-29T01:35:35.046317Z INFO   netlogger.analysis.schema.schema_check.SchemaCheck.init |
2012-02-29T01:35:35.046554Z INFO   netlogger.analysis.schema.schema_check.SchemaCheck.check_schema.start |
2012-02-29T01:35:35.064762Z INFO   netlogger.analysis.schema.schema_check.SchemaCheck.check_schema
                                  | Current version set to: 3.1.
2012-02-29T01:35:35.064902Z ERROR  netlogger.analysis.schema.schema_check.SchemaCheck.check_schema
                                  | Schema version 3.1 found - expecting 4.0 - database admin will need to run upgrade tool.
2012-02-29T01:35:35.065001Z INFO   netlogger.analysis.schema.schema_check.SchemaCheck.upgrade_to_4_0
                                  | Upgrading to schema version 4.0.

Verify if the database has been converted to Version 4.x<emphasis role="bold">

/usr/share/pegasus/sql/schema_tool.py -c connString=sqlite:////to/the/workflow/directory/with/workflow.stampede.db</emphasis>
2012-02-29T01:39:17.218902Z INFO   netlogger.analysis.schema.schema_check.SchemaCheck.init |
2012-02-29T01:39:17.219141Z INFO   netlogger.analysis.schema.schema_check.SchemaCheck.check_schema.start |
2012-02-29T01:39:17.237492Z INFO   netlogger.analysis.schema.schema_check.SchemaCheck.check_schema | Current version set to: 4.0.
2012-02-29T01:39:17.237624Z INFO   netlogger.analysis.schema.schema_check.SchemaCheck.check_schema | Schema up to date.

For upgrading a MySQL database the steps remain the same. The only thing that changes is the connection String to the database
E.g.<emphasis role="bold">

/usr/share/pegasus/sql/schema_tool.py -u connString=mysql://username:password@server:port/dbname

</emphasis></programlisting>

      <para>After the database has been upgraded you can use either 3.x or 4.x
      clients to query the database with <emphasis
      role="bold">pegasus-statistics</emphasis>, as well as <emphasis
      role="bold">pegasus-plots </emphasis>and <emphasis
      role="bold">pegasus-analyzer.</emphasis></para>
    </section>

    <section>
      <title>Existing users running in a condor pool with a non shared
      filesystem setup</title>

      <para>Existing users that are running workflows in a cloud environment
      with a non shared filesystem setup have to do some trickery in the site
      catalog to include placeholders for local/submit host paths for
      execution sites when using CondorIO. In Pegasus 4.0, this has been
      rectified.</para>

      <para>For example, for a 3.1 user, to run on a local-condor pool without
      a shared filesystem and use Condor file IO for file transfers, the site
      entry looks something like this</para>

      <programlisting> &lt;site  handle="local-condor" arch="x86" os="LINUX"&gt;
        &lt;grid  type="gt2" contact="localhost/jobmanager-fork" scheduler="Fork" jobtype="auxillary"/&gt;
        &lt;grid  type="gt2" contact="localhost/jobmanager-condor" scheduler="unknown" jobtype="compute"/&gt;
        &lt;head-fs&gt;

          <emphasis role="bold">&lt;!-- the paths for scratch filesystem are the paths on local site as we execute create dir job
               on local site. Improvements planned for 4.0 release.--&gt;</emphasis>
            &lt;scratch&gt;
                &lt;shared&gt;
                    &lt;file-server protocol="file" url="file:///" mount-point="/submit-host/scratch"/&gt;
                    &lt;internal-mount-point mount-point="/submit-host/scratch"/&gt;
                &lt;/shared&gt;
            &lt;/scratch&gt;
            &lt;storage&gt;
                &lt;shared&gt;
                    &lt;file-server protocol="file" url="file:///" mount-point="/glusterfs/scratch"/&gt;
                    &lt;internal-mount-point mount-point="/glusterfs/scratch"/&gt;
                &lt;/shared&gt;
            &lt;/storage&gt;
        &lt;/head-fs&gt;
        &lt;replica-catalog  type="LRC" url="rlsn://dummyValue.url.edu" /&gt;
        &lt;profile namespace="env" key="PEGASUS_HOME" &gt;/cluster-software/pegasus/2.4.1&lt;/profile&gt;
        &lt;profile namespace="env" key="GLOBUS_LOCATION" &gt;/cluster-software/globus/5.0.1&lt;/profile&gt;

        <emphasis role="bold">&lt;!-- profies for site to be treated as condor pool --&gt;</emphasis>
        &lt;profile namespace="pegasus" key="style" &gt;condor&lt;/profile&gt;
        &lt;profile namespace="condor" key="universe" &gt;vanilla&lt;/profile&gt;


        <emphasis role="bold">&lt;!-- to enable kickstart staging from local site--&gt;</emphasis>
        &lt;profile namespace="condor" key="transfer_executable"&gt;true&lt;/profile&gt;


    &lt;/site&gt;
</programlisting>

      <para>With Pegasus 4.0 the site entry for a local-condor pool can be as
      concise as the following</para>

      <programlisting> &lt;site  handle="condorpool" arch="x86" os="LINUX"&gt;
        &lt;head-fs&gt;
            &lt;scratch /&gt;
            &lt;storage /&gt;
        &lt;/head-fs&gt;
        &lt;profile namespace="pegasus" key="style" &gt;condor&lt;/profile&gt;
        &lt;profile namespace="condor" key="universe" &gt;vanilla&lt;/profile&gt;
    &lt;/site&gt;
</programlisting>

      <para>The planner in 4.0 correctly picks up the paths from the local
      site entry to determine the staging location for the condor io on the
      submit host.</para>

      <para>Users should read pegasus data staging configuration <link
      linkend="data_staging_configuration">chapter</link> and also look in the
      examples directory ( share/pegasus/examples).</para>
    </section>

  </section>

  <section id="migrating_from_2x">
    <title>Migrating From Pegasus 2.X to Pegasus 3.X</title>

    <para>With Pegasus 3.0 effort has been made to simplify configuration.
    This chapter is for existing users of Pegasus who use Pegasus 2.x to run
    their workflows and walks through the steps to move to using Pegasus
    3.0</para>

    <section>
      <title>PEGASUS_HOME and Setup Scripts</title>

      <para>Earlier versions of Pegasus required users to have the environment
      variable PEGASUS_HOME set and to source a setup file
      $PEGASUS_HOME/setup.sh | $PEGASUS_HOME/setup.csh before running Pegasus
      to setup CLASSPATH and other variables.</para>

      <para>Starting with Pegasus 3.0 this is no longer required. The above
      paths are automaticallly determined by the Pegasus tools when they are
      invoked.</para>

      <para>All the users need to do is to set the PATH variable to pick up
      the pegasus executables from the bin directory.</para>

      <programlisting>$ <emphasis role="bold">export PATH=/some/install/pegasus-3.0.0/bin:$PATH</emphasis></programlisting>
    </section>

    <section>
      <title>Changes to Schemas and Catalog Formats</title>

      <section>
        <title>DAX Schema</title>

        <para>Pegasus 3.0 by default now parses DAX documents conforming to
        the DAX Schema 3.2 available <ulink role=""
        url="schemas/dax-3.2/dax-3.2.xsd"
        userlevel="">here</ulink> and is explained in detail in the chapter on
        API references.</para>

        <para>Starting Pegasus 3.0 , DAX generation API's are provided in
        Java/Python and Perl for users to use in their DAX Generators. The use
        of API's is highly encouraged. Support for the old DAX schema's has
        been deprecated and will be removed in a future version.</para>

        <para>For users, who still want to run using the old DAX formats i.e
        3.0 or earlier, can for the time being set the following property in
        the properties and point it to dax-3.0 xsd of the installation.</para>

        <programlisting><emphasis role="bold">pegasus.schema.dax  /some/install/pegasus-3.0/etc/dax-3.0.xsd</emphasis></programlisting>
      </section>

      <section>
        <title>Site Catalog Format</title>

        <para>Pegasus 3.0 by default now parses Site Catalog format conforming
        to the SC schema 3.0 ( XML3 ) available <ulink role=""
        url="schemas/dax-3.2/dax-3.2.xsd"
        userlevel="">here</ulink> and is explained in detail in the chapter on
        Catalogs.</para>

        <para>Pegasus 3.0 comes with a pegasus-sc-converter that will convert
        users old site catalog ( XML ) to the XML3 format. Sample usage is
        given below.</para>

        <programlisting><emphasis role="bold">$ pegasus-sc-converter -i sample.sites.xml -I XML -o sample.sites.xml3 -O XML3
</emphasis>
2010.11.22 12:55:14.169 PST:   Written out the converted file to sample.sites.xml3
</programlisting>

        <para>To use the converted site catalog, in the properties do the
        following</para>

        <orderedlist>
          <listitem>
            <para>unset pegasus.catalog.site or set pegasus.catalog.site to
            XML3</para>
          </listitem>

          <listitem>
            <para>point pegasus.catalog.site.file to the converted site
            catalog</para>
          </listitem>
        </orderedlist>
      </section>

      <section>
        <title>Transformation Catalog Format</title>

        <para>Pegasus 3.0 by default now parses a file based multiline textual
        format of a Transformation Catalog. The new Text format is explained
        in detail in the chapter on Catalogs.</para>

        <para>Pegasus 3.0 comes with a pegasus-tc-converter that will convert
        users old transformation catalog ( File ) to the Text format. Sample
        usage is given below.</para>

        <programlisting><emphasis role="bold">$ pegasus-tc-converter -i sample.tc.data -I File -o sample.tc.text -O Text
</emphasis>
2010.11.22 12:53:16.661 PST:   Successfully converted Transformation Catalog from File to Text
2010.11.22 12:53:16.666 PST:   The output transfomation catalog is in file  /lfs1/software/install/pegasus/pegasus-3.0.0cvs/etc/sample.tc.text
</programlisting>

        <para>To use the converted transformation catalog, in the properties
        do the following</para>

        <orderedlist>
          <listitem>
            <para>unset pegasus.catalog.transformation or set
            pegasus.catalog.transformation to Text</para>
          </listitem>

          <listitem>
            <para>point pegasus.catalog.transformation.file to the converted
            transformation catalog</para>
          </listitem>
        </orderedlist>
      </section>
    </section>

    <section>
      <title>Properties and Profiles Simplification</title>

      <para>Starting with Pegasus 3.0 all profiles can be specified in the
      properties file. Profiles specified in the properties file have the
      lowest priority. Profiles are explained in the detail in the<link
      linkend="configuration"> configuration</link> chapter. As a result of
      this a lot of existing Pegasus Properties were replaced by profiles. The
      table below lists the properties removed and the new profile based
      names.</para>

      <table>
        <title>Table 1: Property Keys removed and their Profile based
        replacement</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Old Property Key</emphasis></entry>

              <entry><emphasis role="bold">New Property Key</emphasis></entry>
            </row>

            <row>
              <entry>pegasus.local.env</entry>

              <entry>no replacement. Specify env profiles for local site in
              the site catalog</entry>
            </row>

            <row>
              <entry>pegasus.condor.release</entry>

              <entry>condor.periodic_release</entry>
            </row>

            <row>
              <entry>pegasus.condor.remove</entry>

              <entry>condor.periodic_remove</entry>
            </row>

            <row>
              <entry>pegasus.job.priority</entry>

              <entry>condor.priority</entry>
            </row>

            <row>
              <entry>pegasus.condor.output.stream</entry>

              <entry>pegasus.condor.output.stream</entry>
            </row>

            <row>
              <entry>pegasus.condor.error.stream</entry>

              <entry>condor.stream_error</entry>
            </row>

            <row>
              <entry>pegasus.dagman.retry</entry>

              <entry>dagman.retry</entry>
            </row>

            <row>
              <entry>pegasus.exitcode.impl</entry>

              <entry>dagman.post</entry>
            </row>

            <row>
              <entry>pegasus.exitcode.scope</entry>

              <entry>dagman.post.scope</entry>
            </row>

            <row>
              <entry>pegasus.exitcode.arguments</entry>

              <entry>dagman.post.arguments</entry>
            </row>

            <row>
              <entry>pegasus.exitcode.path.*</entry>

              <entry>dagman.post.path.*</entry>
            </row>

            <row>
              <entry>pegasus.dagman.maxpre</entry>

              <entry>dagman.maxpre</entry>
            </row>

            <row>
              <entry>pegasus.dagman.maxpost</entry>

              <entry>dagman.maxpost</entry>
            </row>

            <row>
              <entry>pegasus.dagman.maxidle</entry>

              <entry>dagman.maxidle</entry>
            </row>

            <row>
              <entry>pegasus.dagman.maxjobs</entry>

              <entry>dagman.maxjobs</entry>
            </row>

            <row>
              <entry>pegasus.remote.scheduler.min.maxwalltime</entry>

              <entry>globus.maxwalltime</entry>
            </row>

            <row>
              <entry>pegasus.remote.scheduler.min.maxtime</entry>

              <entry>globus.maxtime</entry>
            </row>

            <row>
              <entry>pegasus.remote.scheduler.min.maxcputime</entry>

              <entry>globus.maxcputime</entry>
            </row>

            <row>
              <entry>pegasus.remote.scheduler.queues</entry>

              <entry>globus.queue</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <section>
        <title>Profile Keys for Clustering</title>

        <para>The pegasus profile keys for job clustering were <emphasis
        role="bold">renamed</emphasis>. The following table lists the old and
        the new names for the profile keys.</para>

        <table>
          <title>Table 2: Old and New Names For Job Clustering Profile
          Keys</title>

          <tgroup cols="2">
            <tbody>
              <row>
                <entry><emphasis role="bold">Old Pegasus Profile
                Key</emphasis></entry>

                <entry><emphasis role="bold">New Pegasus Profile
                Key</emphasis></entry>
              </row>

              <row>
                <entry>collapse</entry>

                <entry>clusters.size</entry>
              </row>

              <row>
                <entry>bundle</entry>

                <entry>clusters.num</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>
    </section>

    <section>
      <title>Transfers Simplification</title>

      <para>Pegasus 3.0 has a new default transfer client pegasus-transfer
      that is invoked by default for first level and second level staging. The
      pegasus-transfer client is a python based wrapper around various
      transfer clients like globus-url-copy, lcg-copy, wget, cp, ln .
      pegasus-transfer looks at source and destination url and figures out
      automatically which underlying client to use. pegasus-transfer is
      distributed with the PEGASUS and can be found in the bin subdirectory
      .</para>

      <para>Also, the Bundle Transfer refiner has been made the default for
      pegasus 3.0. Most of the users no longer need to set any transfer
      related properties. The names of the profiles keys that control the
      Bundle Transfers have been changed . The following table lists the old
      and the new names for the Pegasus Profile Keys and are explained in
      details in the Profiles Chapter.</para>

      <table>
        <title>Table 3: Old and New Names For Transfer Bundling Profile
        Keys</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Old Pegasus Profile
              Key</emphasis></entry>

              <entry><emphasis role="bold">New Pegasus Profile
              Keys</emphasis></entry>
            </row>

            <row>
              <entry>bundle.stagein</entry>

              <entry>stagein.clusters | stagein.local.clusters |
              stagein.remote.clusters</entry>
            </row>

            <row>
              <entry>bundle.stageout</entry>

              <entry>stageout.clusters | stageout.local.clusters |
              stageout.remote.clusters</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <section>
        <title>Worker Package Staging</title>

        <para>Starting Pegasus 3.0 there is a separate boolean property
        <emphasis role="bold">pegasus.transfer.worker.package</emphasis> to
        enable worker package staging to the remote compute sites. Earlier it
        was bundled with user executables staging i.e if <emphasis
        role="bold">pegasus.catalog.transformation.mapper</emphasis> property
        was set to Staged .</para>
      </section>
    </section>

    <section>
      <title>Clients in bin directory</title>

      <para>Starting with Pegasus 3.0 the pegasus clients in the bin directory
      have a pegasus prefix. The table below lists the old client names and
      new names for the clients that replaced them</para>

      <table>
        <title>Table 1: Old Client Names and their New Names</title>

        <tgroup cols="2">
          <tbody>
            <row>
              <entry><emphasis role="bold">Old Client</emphasis></entry>

              <entry><emphasis role="bold">New Client</emphasis></entry>
            </row>

            <row>
              <entry>rc-client</entry>

              <entry>pegasus-rc-client</entry>
            </row>

            <row>
              <entry>tc-client</entry>

              <entry>pegasus-tc-client</entry>
            </row>

            <row>
              <entry>pegasus-get-sites</entry>

              <entry>pegasus-sc-client</entry>
            </row>

            <row>
              <entry>sc-client</entry>

              <entry>pegasus-sc-converter</entry>
            </row>

            <row>
              <entry>tailstatd</entry>

              <entry>pegasus-monitord</entry>
            </row>

            <row>
              <entry>genstats and genstats-breakdown</entry>

              <entry>pegasus-statistics</entry>
            </row>

            <row>
              <entry>show-job</entry>

              <entry>pegasus-plots</entry>
            </row>

            <row>
              <entry>dirmanager</entry>

              <entry>pegasus-dirmanager</entry>
            </row>

            <row>
              <entry>exitcode</entry>

              <entry>pegasus-exitcode</entry>
            </row>

            <row>
              <entry>rank-dax</entry>

              <entry>pegasus-rank-dax</entry>
            </row>

            <row>
              <entry>transfer</entry>

              <entry>pegasus-transfer</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
    </section>
  </section>

  <section id="portable_code">
    <title>Best Practices For Developing Portable Code</title>

    <para>This document lists out issues for the algorithm developers to keep
    in mind while developing the respective codes. Keeping these in mind will
    alleviate a lot of problems while trying to run the codes on the Grid
    through workflows.</para>

    <section>
      <title>Supported Platforms</title>

      <para>Most of the hosts making a Grid run variants of Linux or in some
      case Solaris. The Grid middleware mostly supports UNIX and it's
      variants.</para>

      <section>
        <title>Running on Windows</title>

        <para>The majority of the machines making up the various Grid sites
        run Linux. In fact, there is no widespread deployment of a
        Windows-based Grid. Currently, the server side software of Globus does
        not run on Windows. Only the client tools can run on Windows. The
        algorithm developers should not code exclusively for the Windows
        platforms. They must make sure that their codes run on Linux or
        Solaris platforms. If the code is written in a portable language like
        Java, then porting should not be an issue.</para>

        <para>If for some reason the code can only be executed on windows
        platform, please contact the pegasus team at pegasus aT isi dot edu .
        In certain cases it is possible to stand up a linux headnode in front
        of a windows cluster running Condor as it's scheduler.</para>
      </section>
    </section>

    <section>
      <title>Packaging of Software</title>

      <para>As far as possible, binary packages (preferably statically linked)
      of the codes should be provided. If for some reason the codes, need to
      be built from the source then they should have an associated makefile (
      for C/C++ based tools) or an ant file ( for Java tools). The building
      process should refer to the standard libraries that are part of a normal
      Linux installation. If the codes require non-standard libraries, clear
      documentation needs to be provided, as to how to install those
      libraries, and make the build process refer to those libraries.</para>

      <para>Further, installing software as root is not a possibility. Hence,
      all the external libraries that need to be installed can only be
      installed as non-root in non-standard locations.</para>
    </section>

    <section>
      <title>MPI Codes</title>

      <para>If any of the algorithm codes are MPI based, they should contact
      the Grid group. MPI can be run on the Grid but the codes need to be
      compiled against the installed MPI libraries on the various Grid sites.
      The pegasus group has some experience running MPI code through
      PBS.</para>
    </section>

    <section>
      <title>Maximum Running Time of Codes</title>

      <para>Each of the Grid sites has a policy on the maximum time for which
      they will allow a job to run. The algorithms catalog should have the
      maximum time (in minutes) that the job can run for. This information is
      passed to the Grid sites while submitting a job, so that Grid site does
      not kill a job before that published time expires. It is OK, if the job
      runs only a fraction of the max time.</para>
    </section>

    <section>
      <title>Codes cannot specify the directory in which they should be
      run</title>

      <para>Codes are installed in some standard location on the Grid Sites or
      staged on demand. However, they are not invoked from directories where
      they are installed. The codes should be able to be invoked from any
      directory, as long as one can access the directory where the codes are
      installed.</para>

      <para>This is especially relevant, while writing scripts around the
      algorithm codes. At that point specifying the relative paths do not
      work. This is because the relative path is constructed from the
      directory where the script is being invoked. A suggested workaround is
      to pick up the base directory where the software is installed from the
      environment or by using the <command>dirname</command> cmd or api. The
      workflow system can set appropriate environment variables while
      launching jobs on the Grid.</para>
    </section>

    <section>
      <title>No hard-coded paths</title>

      <para>The algorithms should not hard-code any directory paths in the
      code. All directories paths should be picked up explicitly either from
      the environment (specifying environment variables) or from command line
      options passed to the algorithm code.</para>
    </section>

    <section>
      <title>Wrapping legacy codes with a shell wrapper</title>

      <para>When wrapping a legacy code in a script (or another program), it
      is necessary that the wrapper knows where the executable lives. This is
      accomplished using an environmental variable. Be sure to include this
      detail in the component description when submitting a component for use
      on the Grid -- include a brief descriptive name like GDA_BIN.</para>
    </section>

    <section>
      <title>Propogating back the right exitcode</title>

      <para>A job in the workflow is only released for execution if its
      parents have executed successfully. Hence, it is very important that the
      algorithm codes exit with the correct error code in case of success and
      failure. The algorithms should exit with a status of 0 in case of
      success, and a non zero status in case of error. Failure to do so will
      result in erroneous workflow execution where jobs might be released for
      execution even though their parents had exited with an error.</para>

      <para>The algorithm codes should catch all errors and exit with a non
      zero exitcode. The successful execution of the algorithm code can only
      be determined by an exitcode of 0. The algorithm code should not rely
      upon something being written to the stdout to designate success for e.g.
      if the algorithm code writes out to the stdout SUCCESS and exits with a
      non zero status the job would be marked as failed.</para>

      <para>In *nix, a quick way to see if a code is exiting with the correct
      code is to execute the code and then execute echo $?.</para>

      <programlisting>$ component-x input-file.lisp
... some output ...
$ echo $?
0</programlisting>

      <para>If the code is not exiting correctly, it is necessary to wrap the
      code in a script that tests some final condition (such as the presence
      or format of a result file) and uses exit to return correctly.</para>
    </section>

    <section>
      <title>Static vs. Dynamically Linked Libraries</title>

      <para>Since there is no way to know the profile of the machine that will
      be executing the code, it is important that dynamically linked libraries
      are avoided or that reliance on them is kept to a minimum. For example,
      a component that requires libc 2.5 may or may not run on a machine that
      uses libc 2.3. On *nix, you can use the <command>ldd</command> command
      to see what libraries a binary depends on.</para>

      <para>If for some reason you install an algorithm specific library in a
      non standard location make sure to set the
      <envar>LD_LIBRARY_PATH</envar> for the algorithm in the transformation
      catalog for each site.</para>
    </section>

    <section>
      <title>Temporary Files</title>

      <para>If the algorithm codes create temporary files during execution,
      they should be cleared by the codes in case of errors and success
      terminations. The algorithm codes will run on scratch file systems that
      will also be used by others. The scratch directories get filled up very
      easily, and jobs will fail in case of directories running out of free
      space. The temporary files are the files that are not being tracked
      explicitly through the workflow generation process.</para>
    </section>

    <section>
      <title>Handling of stdio</title>

      <para>When writing a new application, it often appears feasible to use
      <emphasis>stdin</emphasis> for a single file data, and
      <emphasis>stdout</emphasis> for a single file output data. The
      <emphasis>stderr</emphasis> descriptor should be used for logging and
      debugging purposes only, never to put data on it. In the *nix world,
      this will work well, but may hiccup in the Windows world.</para>

      <para>We are suggesting that you avoid using stdio for data files,
      because there is the implied expectation that stdio data gets magically
      handled. There is no magic! If you produce data on
      <emphasis>stdout</emphasis>, you need to declare to Pegasus that your
      <emphasis>stdout</emphasis> has your data, and what LFN Pegasus can
      track it by. After the application is done, the data product will be a
      remote file just like all other data products. If you have an input file
      on <emphasis>stdin</emphasis>, you must track it in a similar manner. If
      you produce logs on <emphasis>stderr</emphasis> that you care about, you
      must track it in a similar manner. Think about it this way: Whenever you
      are redirecting stdio in a *nix shell, you will also have to specify a
      file name.</para>

      <para>Most execution environments permit to connect
      <emphasis>stdin</emphasis>, <emphasis>stdout</emphasis> or
      <emphasis>stderr</emphasis> to any file, and Pegasus supports this case.
      However, there are certain very specific corner cases where this is not
      possible. For this reason, we recommend that in new code, you avoid
      using stdio for data, and provide alternative means on the commandline,
      i.e. via <command>--input <replaceable>fn</replaceable></command> and
      <command>--output <replaceable>fn</replaceable></command> commandline
      arguments instead relying on <emphasis>stdin</emphasis> and
      <emphasis>stdout</emphasis>.</para>
    </section>

    <section>
      <title>Configuration Files</title>

      <para>If your code requires a configuration file to run and the
      configuration changes from one run to another, then this file needs to
      be tracked explicitly via the Pegasus WMS. The configuration file should
      not contain any absolute paths to any data or libraries used by the
      code. If any libraries, scripts etc need to be referenced they should
      refer to relative paths starting with a <filename>./xyz</filename> where
      <filename>xyz</filename> is a tracked file (defined in the workflow) or
      as $ENV-VAR/xyz where <envar>$ENV-VAR</envar> is set during execution
      time and evaluated by your application code internally.</para>
    </section>

    <section>
      <title>Code Invocation and input data staging by Pegasus</title>

      <para>Pegasus will create one temporary directory per workflow on each
      site where the workflow is planned. Pegasus will stage all the files
      required for the execution of the workflow in these temporary
      directories. This directory is shared by all the workflow components
      that executed on the site. You will have no control over where this
      directory is placed and as such you should have no expectations about
      where the code will be run. The directories are created per workflow and
      not per job/alogrithm/task. Suppose there is a component component-x
      that takes one argument: input-file.lisp (a file containing the data to
      be operated on). The staging step will bring input-file.lisp to the
      temporary directory. In *nix the call would look like this:</para>

      <programlisting>$ /nfs/software/component-x input-file.lisp</programlisting>

      <para>Note that Pegasus will call the component using the full path to
      the component. If inside your code/script you invoke some other code you
      cannot assume a path for this code to be relative or absolute. You have
      to resovle it either using a dirname $0 trick in shell assuming the
      child code is in the same directory as the parent or construct the path
      by expecting an enviornment variable to be set by the workflow system.
      These env variables need to be explicitly published so that they can be
      stored in the transformation catalog.</para>

      <para>Now suppose that internally, component-x writes its results to
      /tmp/component-x-results.lisp. This is not good. Components should not
      expect that a /tmp directory exists or that it will have permission to
      write there. Instead, component-x should do one of two things: 1. write
      component-x-results.lisp to the directory where it is run from or 2.
      component-x should take a second argument output-file.lisp that
      specifies the name and path of where the results should be
      written.</para>
    </section>

    <section>
      <title>Logical File naming in DAX</title>

      <para>The logical file names used by your code can be of two
      types.</para>

      <itemizedlist>
        <listitem>
          <para>Without a directory path e.g. <filename>f.a</filename>,
          <filename>f.b</filename> etc</para>
        </listitem>

        <listitem>
          <para>With a directory path e.g. <filename>a/1/f.a</filename>,
          <filename>b/2/f.b</filename></para>
        </listitem>
      </itemizedlist>

      <para>Both types of files are supported. We will create any directory
      structure mentioned in your logical files on the remote execution site
      when we stage in data as well as when we store the output data to a
      permanent location. An example invocation of a code that consumes and
      produces files will be</para>

      <programlisting>$/bin/test --input f.a --output f.b</programlisting>

      <para>OR</para>

      <programlisting>$/bin/test --input a/1/f.a --output b/1/f.b</programlisting>

      <note>
        <para>A logical file name should never be an absolute file path, e.g.
        /a/1/f.a In other words, there should not be a starting slash (/) in a
        logical filename.</para>
      </note>
    </section>
  </section>

  <section id="cpu_affinity_condor">
    <title>Slot Partitioning and CPU Affinity in Condor</title>

    <para>By default, Condor will evenly divide the resources in a machine (such as
        RAM, swap space and disk space) among all the CPUs, and advertise each CPU
        as its own slot with an even share of the system resources. If you want to
        have your custom configuration, you can use the following setting to define
        the maximum number of different slot types:</para>

    <programlisting>
MAX_SLOT_TYPES = 2
    </programlisting>

    <para>For each slot type, you can divide system resources unevenly among your
        CPUs. The <emphasis role="bold">N</emphasis> in the name of the macro listed
        below must be an integer from 1 to <emphasis role="bold">MAX_SLOT_TYPES</emphasis>
        (defined above).</para>

    <programlisting>
SLOT_TYPE_1 = cpus=2, ram=50%, swap=1/4, disk=1/4
SLOT_TYPE_N = cpus=1, ram=20%, swap=1/4, disk=1/8
    </programlisting>


    <para>Slots can also be partitioned to accommodate actual needs by accepted jobs.
        A partitionable slot is always unclaimed and dynamically splitted when jobs
        are started. Slot partitioning can be enable as follows:</para>

    <programlisting>
SLOT_TYPE_1_PARTITIONABLE = True
SLOT_TYPE_N_PARTITIONABLE = True
    </programlisting>

    <para>Condor can also bind cores to each slot through CPU affinity:</para>

    <programlisting>
ENFORCE_CPU_AFFINITY = True
SLOT1_CPU_AFFINITY=0,2
SLOT2_CPU_AFFINITY=1,3
    </programlisting>

    <para>Note that CPU numbers may vary from machines. Thus you need to verify what is
        the association for your machine. One way to accomplish this is by using the
        <emphasis role="bold">lscpu</emphasis> command line tool. For instance, the
        output provided from this tool may look like:</para>

    <programlisting>
NUMA node0 CPU(s):     0,2,4,6,8,10
NUMA node1 CPU(s):     1,3,5,7,9,11
    </programlisting>

    <para>The following example assumes a machine with 2 sockets and 6 cores per
        socket, where even cores belong to socket 1 and odd cores to socket
        2:</para>

    <programlisting>
NUM_SLOTS_TYPE_1 = 1
NUM_SLOTS_TYPE_2 = 1
SLOT_TYPE_1_PARTITIONABLE = True
SLOT_TYPE_2_PARTITIONABLE = True

SLOT_TYPE_1 = cpus=6
SLOT_TYPE_2 = cpus=6

ENFORCE_CPU_AFFINITY = True

SLOT1_CPU_AFFINITY=0,2,4,6,8,10
SLOT2_CPU_AFFINITY=1,3,5,7,9,11
    </programlisting>

    <para>Please read the section on "Configuring The Startd for SMP Machines" in
        the Condor Administrator's Manual for full details.</para>
  </section>
</chapter>
