<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="example_workflows">
  <title>Example Workflows</title>

  <para>These examples are included in the Pegasus distribution and can be
  found under <filename>share/pegasus/examples</filename> in your Pegasus
  install (<filename>/usr/share/pegasus/examples</filename> for native
  packages)</para>

  <note>
    <para>These examples are intended to be a starting point for when you want
    to create your own workflows and want to see how other workflows are set
    up. The example workflows will probably not work in your environment
    without modifications. Site and transformation catalogs contain site and
    user specifics such as paths to scratch directories and installed
    software, and at least minor modificiations are required to get the
    workflows to plan and run.</para>
  </note>

  <section id="grid_examples">
    <title>Grid Examples</title>

    <para>These examples assumes you have access to a cluster with Globus
    installed. A pre-ws gatekeeper and gridftp server is required. You also
    need Globus and Pegasus installed, both on the machine you are submitting
    from, and the cluster.</para>

    <section id="example_black_diamond" xreflabel="Black Diamond">
      <title>Black Diamond</title>

      <para>Pegasus is shipped with 3 different Black Diamond examples for the
      grid. This is to highlight the available DAX APIs which are Java, Perl
      and Python. The examples can be found under:</para>

      <programlisting>share/pegasus/examples/grid-blackdiamond-java/
share/pegasus/examples/grid-blackdiamond-perl/
share/pegasus/examples/grid-blackdiamond-python/</programlisting>

      <para>The workflow has 4 nodes, layed out in a diamond shape, with files
      being passed between them (f.*):</para>

      <mediaobject>
        <imageobject>
          <imagedata align="center" fileref="images/examples-diamond.jpg"
                     valign="middle"/>
        </imageobject>
      </mediaobject>

      <para>The binary for the nodes is a simple "mock application" name
      <command>keg</command> ("canonical example for the grid") which reads
      input files designated by arguments, writes them back onto output files,
      and produces on STDOUT a summary of where and when it was run. Keg ships
      with Pegasus in the bin directory.</para>

      <para>This example ships with a "submit" script which will build the
      replica catalog, the transformation catalog, and the site catalog. When
      you create your own workflows, such a submit script is not needed if you
      want to maintain those catalogs manually.</para>

      <note>
        <para>The use of <filename>./submit</filename> scripts in these
        examples are just to make it more easy to run the examples out of the
        box. For a production site, the catalogs (transformation, replica,
        site) may or may not be static or generated by other tooling.</para>
      </note>

      <para>To test the examples, edit the <command>submit</command> script
      and change the cluster config to the setup and install locations for
      your cluster. Then run:</para>

      <para><programlisting>$ <emphasis role="bold">./submit</emphasis></programlisting></para>

      <para>The workflow should now be submitted and in the output you should
      see a work dir location for the instance. With that directory you can
      monitor the workflow with:</para>

      <para><programlisting>$ <emphasis role="bold">pegasus-status [workdir]</emphasis></programlisting></para>

      <para>Once the workflow is done, you can make sure it was sucessful
      with:</para>

      <para><programlisting>$ <emphasis role="bold">pegasus-analyzer -d [workdir]</emphasis></programlisting></para>
    </section>

    <section>
      <title>NASA/IPAC Montage</title>

      <para>This example can be found under</para>

      <programlisting><filename>share/pegasus/examples/grid-montage/</filename></programlisting>

      <para>The NASA IPAC Montage (<ulink
      url="http://montage.ipac.caltech.edu/">http://montage.ipac.caltech.edu/</ulink>)
      workflow projects/montages a set of input images from telescopes like
      Hubble and end up with images like <ulink
      url="http://montage.ipac.caltech.edu/images/m104.jpg">http://montage.ipac.caltech.edu/images/m104.jpg</ulink>
      . The test workflow is for a 1 by 1 degrees tile. It has about 45 input
      images which all have to be projected, background modeled and adjusted
      to come out as one seamless image.</para>

      <para>Just like the <xref linkend="example_black_diamond">
           black diamond example 
        </xref> above, this example uses a <filename>./submit</filename>
      script.</para>

      <para>The Montage DAX is generated with a tool called
      <filename>mDAG</filename> shipped with Montage which generates the
      workflow.</para>
    </section>

    <section>
      <title>Rosetta</title>

      <para>This example can be found under</para>

      <programlisting><filename>share/pegasus/examples/grid-rosetta/</filename></programlisting>

      <para>Rosetta (<ulink
      url="http://www.rosettacommons.org/">http://www.rosettacommons.org/</ulink>)
      is a high resolution protein prediction and design software. Highlights
      in this example are:</para>

      <itemizedlist>
        <listitem>
          <para>Using the Pegasus Java API to generate the DAX</para>
        </listitem>

        <listitem>
          <para>The DAX generator loops over the input PDBs and creates a job
          for each input</para>
        </listitem>

        <listitem>
          <para>The jobs all have a dependency on a flatfile database. For
          simplicity, each job depends on all the files in the database
          directory.</para>
        </listitem>

        <listitem>
          <para>Job clustering is turned on to make each grid job run longer
          and better utilize the compute cluster</para>
        </listitem>
      </itemizedlist>

      <para>Just like the <xref linkend="example_black_diamond">
           black diamond example 
        </xref> above, this example uses a <filename>./submit</filename>
      script.</para>
    </section>
  </section>

  <section id="condor_examples">
    <title>Condor Examples</title>

    <section>
      <title>Black Diamond - condorio</title>

      <para>There are a set of Condor examples available, highlighting
      different <link linkend="data_staging_configuration">data staging
      configurations</link>.The most basic one is condorio, and the example
      can be found under:</para>

      <programlisting><filename>share/pegasus/examples/condor-blackdiamond-condorio/</filename></programlisting>

      <para>This example is using the same abstract workflow as the <xref
          linkend="example_black_diamond">
           Black Diamond 
        </xref> grid example above, and can be executed either on the submit
      machine (universe="local") or on a local Condor pool
      (universe="vanilla").</para>

      <para>You can run this example with the <filename>./submit</filename>
      script. Example:</para>

      <programlisting>$ <emphasis role="bold">./submit</emphasis></programlisting>
    </section>
  </section>

  <section id="container_examples">
    <title>Container Examples</title>

    <section id="example_containers_montage"
             xreflabel="Montage Using Containers">
       

      <title>Montage Using Containers</title>

       

      <para>This section contains an example of a real workflow running inside
      Singularity containers. The application is <ulink
      url="http://montage.ipac.caltech.edu/">Montage</ulink> using the <ulink
      url="https://github.com/pegasus-isi/montage-workflow-v2">montage-v2
      workflow</ulink>. Be aware that this workflow can be fairly data
      intensive, and when running with containers in
      <emphasis>condorio</emphasis> or <emphasis>nonsharedfs</emphasis> data
      management modes, the data staging of the application data and the
      container image to each job can result in a non-trivial amount of
      network traffic.</para>

       

      <para>The software dependencies consists of the
      <emphasis>Montage</emphasis> software stack, and
      <emphasis>AstroPy</emphasis>. These are installed into the image (see
      the <emphasis>Singularity</emphasis> file in the GitHub repository). The
      image has been made available in <ulink
      url="https://singularity-hub.org/">Singularity Hub</ulink>.</para>

       

      <para>Now that we have an image, the next step is to check out the
      workflow from GitHub, and use it to create an abstract workflow
      description, a transformation catalog and a replica catalog. The
      <emphasis>montage-workflow.py</emphasis> command create all this for us,
      but the command itself requires Montage to look up input data for the
      specified location in the sky. The provide the environment, run this
      command inside the same Singularity image. For example:</para>

       

      <programlisting>
singularity exec \
            --bind $PWD:/srv --workdir /srv \
            shub://singularity-hub.org/pegasus-isi/montage-workflow-v2 \
            /srv/montage-workflow.py \
                --tc-target container \
                --center "56.7 24.00" \
                --degrees 2.0 \
                --band dss:DSS2B:blue \
                --band dss:DSS2R:green \
                --band dss:DSS2IR:red
    </programlisting>

       

      <para>The command executes a data find for the 3 specified bands, 2.0
      degrees around the location 56.7 24.00, and generates a workflow to
      combine the images into a single image. One extra flag is provided to
      let the command know we want to execute the workflow inside containers:
      <emphasis>--tc-target container</emphasis>. The result is a
      transformation catalog in <emphasis>data/tc.txt</emphasis></para>

      , with starts with: 

      <programlisting>
cont montage {
   type "singularity"
   image "shub://singularity-hub.org/pegasus-isi/montage-workflow-v2"
   profile env "MONTAGE_HOME" "/opt/Montage"
}

tr mDiffFit {
  site condor_pool {
    type "INSTALLED"
    container "montage"
    pfn "file:///opt/Montage/bin/mDiffFit"
    profile pegasus "clusters.size" "5"
  }
}
... 
    </programlisting>

       

      <para>The first entry describes the container, where the image can be
      found (Singularity Hub in this example), and a special environment
      variable we want to be set for the jobs.</para>

       

      <para>The second entry, of which there are many more similar ones in the
      file, describes the application. Note how it refers back to the
      <emphasis>"montage"</emphasis> container, specifying that we want the
      job to be wrapped in the container.</para>

       

      <para>In the <emphasis>data/</emphasis> directory. we can also find the
      abstract workflow (<emphasis>montage.dax</emphasis>), and replica
      catalog (<emphasis>rc.dax</emphasis>). Note that this are the same as if
      the workflow was running in a non-container environment. To plan the
      workflow:</para>

       

      <programlisting>
pegasus-plan \
        --dir work \
        --relative-dir `date +'%s'` \
        --dax data/montage.dax \
        --sites condor_pool \
        --output-site local \
        --cluster horizontal 
    </programlisting>

       
    </section>
  </section>

  <section id="local_shell_examples">
    <title>Local Shell Examples</title>

    <section>
      <title>Black Diamond</title>

      <para>To aid in workflow development and debugging, Pegasus can now map
      a workflow to a local shell script. One advantage is that you do not
      need a remote compute resource.</para>

      <para>This example is using the same abstract workflow as the <xref
          linkend="example_black_diamond">
           Black Diamond 
        </xref> grid example above. The difference is that a property is set
      in pegasusrc to force shell execution:</para>

      <programlisting># tell pegasus to generate shell version of
# the workflow
pegasus.code.generator = Shell</programlisting>

      <para>You can run this example with the <filename>./submit</filename>
      script.</para>
    </section>
  </section>

  <section id="notifications_example">
    <title>Notifications Example</title>

    <para>A new feature in Pegasus 3.1. is notifications. While the workflow
    is running, a monitoring tool is running side by side to the workflow, and
    issues user defined notifications when certain events takes place, such as
    job completion or failure. See <link linkend="notifications">notifications
    section</link> for detailed information. A workflow example with
    notifications can be found under examples/notifications. This workflow is
    based on the Black Diamond, with the changes being notifications added to
    the DAX generator. For example, notifications are added at the workflow
    level:</para>

    <programlisting># Create a abstract dag
diamond = ADAG("diamond")
# dax level notifications
diamond.invoke('all', os.getcwd() + "/my-notify.sh")</programlisting>

    <para>The DAX generator also contains job level notifications:</para>

    <programlisting># job level notifications - in this case for at_end events
frr.invoke('at_end', os.getcwd() + "/my-notify.sh")</programlisting>

    <para>These invoke lines specify that the <command>my-notify.sh</command>
    script will be invoked for events generated (<emphasis
    role="bold">all</emphasis> in the first case, <emphasis
    role="bold">at_end</emphasis> in the second). The
    <command>my-notify.sh</command> script contains callouts sample
    notification tools shipped with Pegasus, one for email and for
    Jabber/GTalk (commented out by default):</para>

    <programlisting>#!/bin/bash

# Pegasus ships with a couple of basic notification tools. Below
# we show how to notify via email and gtalk.

# all notifications will be sent to email
# change $USER to your full email addess
$PEGASUS_HOME/libexec/notification/email -t $USER

# this sends notifications about failed jobs to gtalk.
# note that you can also set which events to trigger on in your DAX.
# set jabberid to your gmail address, and put in yout
# password
# uncomment to enable
if [ "x$PEGASUS_STATUS" != "x" -a "$PEGASUS_STATUS" != "0" ]; then
    $PEGASUS_HOME/libexec/notification/jabber --jabberid FIXME@gmail.com \
                                              --password FIXME \
                                              --host talk.google.com
fi
</programlisting>
  </section>

  <section id="workflow_of_workflows">
    <title>Workflow of Workflows</title>

    <section>
      <title>Galactic Plane</title>

      <para>The <ulink
      url="http://en.wikipedia.org/wiki/Galactic_plane">Galactic Plane</ulink>
      workflow is a workflow of many Montage workflows. The output is a set of
      tiles which can be used in software which takes the tiles and produces a
      seamless image which can be scrolled and zoomed into. As this is more of
      a production workflow than an example one, it can be a little bit harder
      to get running in your environment.</para>

      <para>Highlights of the example are:</para>

      <itemizedlist>
        <listitem>
          <para>The subworkflow DAXes are generated as jobs in the parent
          workflow - this is an example on how to make more dynamic workflows.
          For example, if you need a job in your workflow to determine the
          number of jobs in the next level, you can have the first job create
          a subworkflow with the right number of jobs.</para>
        </listitem>

        <listitem>
          <para>DAGMan job categories are used to limit the number of
          concurrant jobs in certain places. This is used to limit the number
          of concurrant connections to the data find service, as well limit
          the number of concurrant subworkflows to manage disk usage on the
          compute cluster.</para>
        </listitem>

        <listitem>
          <para>Job priorities are used to make sure we overlap staging and
          computation. Pegasus sets default priorities, which for most jobs
          are fine, but the priority of the data find job is set explicitly to
          a higher priority.</para>
        </listitem>

        <listitem>
          <para>A specific output site is defined the the site catalog and
          specified with the --output option of subworkflows.</para>
        </listitem>
      </itemizedlist>

      <para>The DAX API has support for sub workflows:</para>

      <programlisting>    remote_tile_setup = Job(namespace="gp", name="remote_tile_setup", version="1.0")
    remote_tile_setup.addArguments("%05d" % (tile_id))
    remote_tile_setup.addProfile(Profile("dagman", "CATEGORY", "remote_tile_setup"))
    remote_tile_setup.uses(params, link=Link.INPUT, register=False)
    remote_tile_setup.uses(mdagtar, link=Link.OUTPUT, register=False, transfer=True)
    uberdax.addJob(remote_tile_setup)
...
    subwf = DAX("%05d.dax" % (tile_id), "ID%05d" % (tile_id))
    subwf.addArguments("-Dpegasus.schema.dax=%s/etc/dax-2.1.xsd" %(os.environ["PEGASUS_HOME"]),
                       "-Dpegasus.catalog.replica.file=%s/rc.data" % (tile_work_dir),
                       "-Dpegasus.catalog.site.file=%s/sites.xml" % (work_dir),
                       "-Dpegasus.transfer.links=true",
                       "--sites", cluster_name,
                       "--cluster", "horizontal",
                       "--basename", "tile-%05d" % (tile_id),
                       "--force",
                       "--output", output_name)
    subwf.addProfile(Profile("dagman", "CATEGORY", "subworkflow"))
    subwf.uses(subdax_file, link=Link.INPUT, register=False)
    uberdax.addDAX(subwf)

</programlisting>

      <para/>
    </section>
  </section>
</chapter>
