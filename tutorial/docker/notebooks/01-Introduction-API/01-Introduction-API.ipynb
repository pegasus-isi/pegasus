{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pegasus Tutorial\n",
    "\n",
    "Welcome to the Pegasus tutorial notebook, which is intended for new users who want to get a quick overview of Pegasus concepts and usage. This tutorial covers:\n",
    "\n",
    " - Using the Pegasus API to generate an abstract workflow\n",
    " - Using the API to plan the abstract workflow into an executable workflow\n",
    " - Pegasus catalogs for sites, transformations, and data\n",
    " - Debug and recover from failures (02-Debugging notebook)\n",
    " - Command line tools (03-Command-Line-Tools notebook)\n",
    " \n",
    "For a quick overview of Pegasus, please see this short YouTube video:\n",
    "\n",
    "[![A 5 Minute Introduction](../images/youtube-pegasus-intro.png)](https://www.youtube.com/watch?v=MNN80OHMQUQ \"A 5 Minute Introduction\")\n",
    "\n",
    "## Docker Container Notes\n",
    "\n",
    "This Docker image is based on CentOS 8, and includes HTCondor, Docker and Jupyter.\n",
    "\n",
    "Jupyter is a great environment for learning how Pegasus works, but please note that production use is commonly done on the command line. There are no Jupyter dependencies in Pegasus.\n",
    "\n",
    "To simplify the use of this container, everything is contained within the image. Thus there is no permanent storage - if you make changes to these tutorial notebooks and restart the container, the changes will be lost. If you want to save notebooks or data permanently, start the container with a volume mount and copy what you want to keep there. \n",
    "\n",
    "## Diamond Workflow\n",
    "\n",
    "This notebook will generate the **diamond workflow** illustrated below, then plan and execute the workflow on the local condorpool. Rectangles represent input/output files, and ovals represent compute jobs. The arrows represent file dependencies between each compute job. \n",
    "\n",
    "![Diamond Workflow](../images/diamond.svg)\n",
    "\n",
    "The abstract workflow description that you specify to Pegasus is portable, and usually does not contain any locations to physical input files, executables or cluster end points where jobs are executed. Pegasus uses three information catalogs during the planning process. A picture of this process is:\n",
    "\n",
    "![Catalogs](../images/catalogs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Python API\n",
    "\n",
    "Pegasus 5.0 introduces a new Python API, which is fully documented in the [Pegasus reference guide](https://pegasus.isi.edu/documentation/reference-guide/api-reference.html). A high level overview of the components:\n",
    "<br>\n",
    "```\n",
    "from Pegasus.api.mixins import EventType, Namespace\n",
    "from Pegasus.api.properties import Properties\n",
    "from Pegasus.api.replica_catalog import File, ReplicaCatalog\n",
    "from Pegasus.api.site_catalog import (\n",
    "    OS,\n",
    "    Arch,\n",
    "    Directory,\n",
    "    FileServer,\n",
    "    Grid,\n",
    "    Operation,\n",
    "    Scheduler,\n",
    "    Site,\n",
    "    SiteCatalog,\n",
    ")\n",
    "from Pegasus.api.transformation_catalog import (\n",
    "    Container,\n",
    "    Transformation,\n",
    "    TransformationCatalog,\n",
    "    TransformationSite,\n",
    ")\n",
    "from Pegasus.api.workflow import Job, SubWorkflow, Workflow\n",
    "from Pegasus.client._client import PegasusClientError\n",
    "```\n",
    "\n",
    "While you can import just parts of the API, the most convenient way is to just import it all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pegasus.api import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Logging\n",
    "\n",
    "Configure logging. While this is **not required**, it is useful for seeing output from tools such as `pegasus-plan`, `pegasus-analyzer`, etc. when using these python wrappers. Here we also include a few other imports we might need further down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Pegasus Properties\n",
    "\n",
    "The `pegasus.properties` file can now be generated using the `Properties()` object as shown below. To see a list of the most commonly used properties, you can use `Properties.ls(prefix)`. By default, `pegasus-plan` will look in `cwd` for a `pegasus.properties` file if one is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Properties ---------------------------------------------------------------\n",
    "props = Properties()\n",
    "props[\"pegasus.monitord.encoding\"] = \"json\"                                                                    \n",
    "props[\"pegasus.catalog.workflow.amqp.url\"] = \"amqp://friend:donatedata@msgs.pegasus.isi.edu:5672/prod/workflows\"\n",
    "props[\"pegasus.mode\"] = \"tutorial\" # speeds up tutorial workflows - remove for production ones\n",
    "props.write() # written to ./pegasus.properties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Properties.ls(\"condor.request\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a Replica Catalog (Specify Initial Input Files)\n",
    "\n",
    "Any initial input files given to the workflow should be specified in the `ReplicaCatalog`. This object tells Pegasus where each input file is physically located. First, we create a file that will be used as input to this workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"f.a\", \"w\") as f:\n",
    "    f.write(\"This is the contents of the input file for the diamond workflow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `./f.a` will be used in this workflow, and so we create a corresponding `File` object. Metadata may also be added to the file as shown below.\n",
    "\n",
    "Next, a `ReplicaCatalog` object is created so that the physical locations of each input file can be cataloged. This is done using the `ReplicaCatalog.add_replica(site, file, path)` function. As the file `f.a` resides here on the submit machine, we use the reserved keyword `local` for the site parameter. Second, the `File` object is passed in for the `file` parameter. Finally, the absolute path to the file is given. `pathlib.Path` may be used as long as an absolute path is given. \n",
    "\n",
    "By default, `pegasus-plan` will look in `cwd` for a `replicas.yml` file if one is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Replicas -----------------------------------------------------------------\n",
    "fa = File(\"f.a\").add_metadata(creator=\"ryan\")\n",
    "rc = ReplicaCatalog()\\\n",
    "    .add_replica(\"local\", fa, Path(\".\").resolve() / \"f.a\")\\\n",
    "    .write() # written to ./replicas.yml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat replicas.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a Transformation Catalog (Specify Executables Used)\n",
    "\n",
    "Any executable (referred to as ***transformations***) used by the workflow needs to be specified in the `TransformationCatalog`. This is done by creating `Transformation` objects, which represent executables. Once created, these must be added to the `TransformationCatalog` object. \n",
    "\n",
    "By default, `pegasus-plan` will look in `cwd` for a `transformations.yml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Transformations ----------------------------------------------------------\n",
    "preprocess = Transformation(\n",
    "                \"preprocess\",\n",
    "                site=\"condorpool\",\n",
    "                pfn=\"/usr/bin/pegasus-keg\",\n",
    "                is_stageable=False,\n",
    "                arch=Arch.X86_64,\n",
    "                os_type=OS.LINUX\n",
    "            )\n",
    "\n",
    "findrange = Transformation(\n",
    "                \"findrange\",\n",
    "                site=\"condorpool\",\n",
    "                pfn=\"/usr/bin/pegasus-keg\",\n",
    "                is_stageable=False,\n",
    "                arch=Arch.X86_64,\n",
    "                os_type=OS.LINUX\n",
    "            )\n",
    "\n",
    "analyze = Transformation(\n",
    "                \"analyze\",\n",
    "                site=\"condorpool\",\n",
    "                pfn=\"/usr/bin/pegasus-keg\",\n",
    "                is_stageable=False,\n",
    "                arch=Arch.X86_64,\n",
    "                os_type=OS.LINUX\n",
    "            )\n",
    "\n",
    "tc = TransformationCatalog()\\\n",
    "    .add_transformations(preprocess, findrange, analyze)\\\n",
    "    .write() # ./written to ./transformations.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat transformations.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create the Workflow\n",
    "\n",
    "The `Workflow` object is used to store jobs and dependencies between each job. Typical job creation is as follows:\n",
    "\n",
    "```\n",
    "# Define job Input/Output files\n",
    "input_file = File(\"input.txt\")\n",
    "output_file1 = File(\"output1.txt\")\n",
    "output_file2 = File(\"output2.txt\")\n",
    "\n",
    "# Define job, passing in the transformation (executable) it will use\n",
    "j = Job(transformation_obj)\n",
    "\n",
    "# Specify command line arguments (if any) which will be passed to the transformation when run\n",
    "j.add_args(\"arg1\", \"arg2\", input_file, \"arg3\", output_file)\n",
    "\n",
    "# Specify input files (if any)\n",
    "j.add_inputs(input_file)\n",
    "\n",
    "# Specify output files (if any)\n",
    "j.add_outputs(output_file1, output_file2)\n",
    "\n",
    "# Add profiles to the job\n",
    "j.add_env(FOO=\"bar\")\n",
    "j.add_profiles(Namespace.PEGASUS, key=\"checkpoint.time\", value=1)\n",
    "\n",
    "# Add the job to the workflow object\n",
    "wf.add_jobs(j)\n",
    "```\n",
    "\n",
    "By default, depedencies between jobs are inferred based on input and output files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Workflow -----------------------------------------------------------------\n",
    "wf = Workflow(\"blackdiamond\")\n",
    "\n",
    "fb1 = File(\"f.b1\")\n",
    "fb2 = File(\"f.b2\")\n",
    "job_preprocess = Job(preprocess)\\\n",
    "                    .add_args(\"-a\", \"preprocess\", \"-T\", \"3\", \"-i\", fa, \"-o\", fb1, fb2)\\\n",
    "                    .add_inputs(fa)\\\n",
    "                    .add_outputs(fb1, fb2)\n",
    "\n",
    "fc1 = File(\"f.c1\")\n",
    "job_findrange_1 = Job(findrange)\\\n",
    "                    .add_args(\"-a\", \"findrange\", \"-T\", \"3\", \"-i\", fb1, \"-o\", fc1)\\\n",
    "                    .add_inputs(fb1)\\\n",
    "                    .add_outputs(fc1)\n",
    "\n",
    "fc2 = File(\"f.c2\")\n",
    "job_findrange_2 = Job(findrange)\\\n",
    "                    .add_args(\"-a\", \"findrange\", \"-T\", \"3\", \"-i\", fb2, \"-o\", fc2)\\\n",
    "                    .add_inputs(fb2)\\\n",
    "                    .add_outputs(fc2)\n",
    "\n",
    "fd = File(\"f.d\")\n",
    "job_analyze = Job(analyze)\\\n",
    "                .add_args(\"-a\", \"analyze\", \"-T\", \"3\", \"-i\", fc1, fc2, \"-o\", fd)\\\n",
    "                .add_inputs(fc1, fc2)\\\n",
    "                .add_outputs(fd)\n",
    "\n",
    "wf.add_jobs(job_preprocess, job_findrange_1, job_findrange_2, job_analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing the Workflow\n",
    "\n",
    "Once you have defined your abstract workflow, you can use `pegasus-graphviz` to visualize it. `Workflow.graph()` will invoke `pegasus-graphviz` internally and render your workflow using one of the available formats such as `png`. **Note that Workflow.write() must be invoked before calling Workflow.graph().**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.write()\n",
    "    wf.graph(include_files=True, label=\"xform-id\", output=\"graph.png\")\n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view rendered workflow\n",
    "from IPython.display import Image\n",
    "Image(filename='graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run the Workflow\n",
    "\n",
    "When working in Python, we can just use the reference do the `Workflow` object, you can plan, run, and monitor the workflow directly. These are wrappers around Pegasus CLI tools, and as such, the same arguments may be passed to them. \n",
    "\n",
    "**Note that the Pegasus binaries must be added to your PATH for this to work.**\n",
    "\n",
    "Please wait for the progress bar to indicate that the workflow has finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.plan(submit=True)\\\n",
    "        .wait()\n",
    "except PegasusClientError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the line in the output that starts with pegasus-status, contains the command you can use to monitor the status of the workflow. We will cover this command line tool in the next couple of notbooks. The path it contains is the path to the submit directory where all of the files required to submit and monitor the workflow are stored. For now we will just continue to use the Python `Workflow` object.\n",
    "\n",
    "## 9. Statistics\n",
    "\n",
    "Depending on if the workflow finished successfully or not, you have options on what to do next. If the workflow failed you can use `wf.analyze()` do get help finding out what went wrong. If the workflow finished successfully, we can pull out some statistcs from the provenance database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    wf.statistics()\n",
    "except PegasusClientError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. What's Next?\n",
    "\n",
    "To continue exploring Pegasus, and specifically learn how to debug failed workflows, please open the notebook in `02-Debugging/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
